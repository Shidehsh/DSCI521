{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 521: Methods for analysis and interpretation <br> Chapter 7: Approaching regression and classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.0 The big picture\n",
    "In the context of gradient descent we discussed optimization. There, we utilized the algorithm to determine some parameters (a slope and intercept) for a model that 'fit' the best, i.e., achieve the best possible output in the context of some measure of performance. But let's step back for a moment to discuss the difference betwen classification and regression. \n",
    "> In general, for regression models the output variables take the form of continuous values, and in classification the output variables takes class labels.\n",
    "\n",
    "While there are clear differences between these two modeling contexts, and many different methods for each, routines for their optimzation and evaluation often bear similarities. For example, it is common between the two for a researcher to \n",
    "\n",
    "1. identify an objective function \n",
    "- split data into training and test sets \n",
    "- propose models for evaluation \n",
    "- optimize models and select the most desirable\n",
    "\n",
    "So, since we've already discussed the concept of evaluation and objective functions (we'll review a new on in our discussion of classification), the next topic for us to hit involves splitting data for an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0.1 Data splits\n",
    "Commonly, an optimized model will be _over-fit_ to the particular data on which it is trained. So, to get an idea of how well an algorithm performs on _other_ data from the real world it's customary to have separate datasets&mdash;one for training, and the other for evaluation (testing). We'll get into more complex validation procedures below, and additionally, use them to optimize models. However, at present we're interested in meaningful evaluations out of our trained models. To this end, the first thing we might do is separate some data for training from evaluation (testing). This is referred to as a data _split_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.0.1.1 Separating training from evaluation data\n",
    "While we've been building up some of our methods from nuts and bolts, much of the modeling work in data science has well established, open-source software in Python that can _at least_ support prototyping. While the contents of this Chpter's discussion focuses on just a few statistical methods, this is a good time to intrtoduce the main machine learning (ML) library for Python, called scikit-learn, i.e., `sklearn`.\n",
    "\n",
    "`sklearn.model_selection` has a nice way of addressing this topic (much nicer than splitting our data from scratch). But no matter how we split our data up, there's one thing we are positively _required_ to have: a reproducible&mdash;even if randomized&mdash;procedure to replicate our experiment. The utility we're interested in here is `train_test_split(x, y, test_size, random_state)`.\n",
    "\n",
    "Here, `x`  refers to the predictor, and `y` refers to the target data for prediction. `y` should generally be the 'ground truth', against which an evaluation will take place at the experiment's close. Beyond this, the `test_size` parameter (a number ranging over $(0,1)$) defines how much data gets put into the evaluation set, and the `random_state` parameter make this process reproducible! Let's try this out to start with some baseball player heights, predicting weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['yearID', 'playerID', 'teamID', 'G', 'AB', 'R', 'H', '2B', '3B', 'HR',\n",
      "       'RBI', 'SO', 'HBP', 'G_F', 'PO', 'A', 'E', 'nameGiven', 'nameLast',\n",
      "       'height', 'weight', 'G_P', 'W', 'L', 'SO_P', 'ER', 'HR_P', 'H_P',\n",
      "       'HBP_P', 'salary', 'name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseball_data = pd.read_csv('./data/2008_merged_baseball_data.csv')\n",
    "\n",
    "print(baseball_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2072, 31)\n",
      "(1388, 1) (684, 1) (1388, 1) (684, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(baseball_data.shape)\n",
    "\n",
    "height_train, height_test, weight_train, weight_test = train_test_split(\n",
    "    baseball_data[[\"height\"]], baseball_data[[\"weight\"]], \n",
    "    test_size=0.33, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(height_train.shape, height_test.shape, weight_train.shape, weight_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1  Regression\n",
    "In __Chapter 5__, we talked at length about different mathematical functions as models and worked to determine their parameters. Since we were considering continuous data, that optimization experiment was indeed regression. As is common in this scenario, we strove for the prediction of continuous data values: $y_1, \\cdots,y_m$ from some other numeric data: $x_1,\\cdots, x_m$. Regardless of the prediction method, it's common to write the _regressed_ estimations of the $y$-values as $\\hat{y}_1,\\cdots,\\hat{y}_n$. \n",
    "\n",
    "### 7.1.1 SSE (review)\n",
    "To get back into regression, let's review our objective functon&mdash;the sum of squared errors ($SSE$):\n",
    "\n",
    "$$SSE = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "$SSE$ is a way to summarize the total prediction quality of a model. As in __Chapter 5__, the general strategy is to build a model that produces predictions&mdash;$\\hat{y}_i$ values&mdash;that _minimize_ the $SSE$. $SSE$ is used as an objective function for modeling in many regression contexts, and for simpliciy we'll return to the linear model we studied in the context of gradient descent, too. This model&mdash;linear regression&mdash;is probably among the most commonly studied in these contexts for convenience and some very nice properties.\n",
    "\n",
    "### 7.1.2 Linear Regression\n",
    "Here, we're hoping to define a slope, $m$, and $y$-intercept, $b$, to produce prediction values, $\\hat{y}_i = mx_i + b$ that minimize $SSE$:\n",
    "\n",
    "$$\n",
    "SSE = \\sum_{i = 1}^n (\\hat{y}_i - y_i)^2 \n",
    "    = \\sum_{i = 1}^n (mx_i + b - y_i)^2\n",
    "$$\n",
    "\n",
    "The really exciting thing about linear regression is that there is actually a guarenteed best line, i.e., $(m, b)$ pair, and they can be found with some pretty basic calculus:\n",
    "\n",
    "1. Set the components of the gradient each equal to zero: $$\\begin{align}\n",
    "SSE'_b &= 2\\sum_{i = 1}^n (mx_i + b - y_i) = 0\\\\\n",
    "SSE'_m &= 2\\sum_{i = 1}^n x_i(mx_i + b - y_i) = 0\\\\\\end{align}$$\n",
    "2. Solve the two equations for the two variables; the substitution method works, and is easiest solving for $b$ first:$$\\begin{align}\n",
    "0 &= SSE'_b \\\\\n",
    "0 &= 2m\\sum_{i = 1}^n x_i + 2\\sum_{i = 1}^n b - 2\\sum_{i = 1}^n y_i\\\\\n",
    "0 &= 2mn\\overline{x} + 2nb - 2n\\overline{y}\\\\\n",
    "0 &= m\\overline{x} + b - \\overline{y}\\\\\n",
    "  \\Longrightarrow \n",
    "b &= \\overline{y} - m\\overline{x}\\end{align}$$\n",
    "3. Now, substituting into the second equation:$$\\begin{align}\n",
    "0 &= SSE'_m \\\\\n",
    "  &= 2\\sum_{i = 1}^n x_i(mx_i + b - y_i)\\\\\n",
    "  &= \\sum_{i = 1}^n mx_i^2 + (\\overline{y} - m\\overline{x})x_i - x_iy_i\\\\\n",
    "  &= \\sum_{i = 1}^n mx_i^2 - mx_i\\overline{x} + x_i\\overline{y} - x_iy_i\\\\\n",
    "  &= \\sum_{i = 1}^n mx_i(x_i - \\overline{x}) + x_i(\\overline{y} - y_i)\\\\\n",
    "  &= \\sum_{i = 1}^n mx_i(x_i - \\overline{x}) \n",
    "   - m\\overline{x}\\underbrace{(x_i - \\overline{x})}_{0 = \\sum (x_i - \\overline{x})}\n",
    "                   + x_i(\\overline{y} - y_i) \n",
    "   - \\overline{x}\\underbrace{(\\overline{y} - y_i)}_{0 = \\sum (\\overline{y} - y_i)}\\\\\n",
    "  &= \\sum_{i = 1}^n m(x_i - \\overline{x})^2 + (x_i - \\overline{x})(\\overline{y} - y_i)\\\\\n",
    "  \\Longrightarrow\n",
    "  m &=  \\frac{\\sum_{i = 1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i = 1}^n (x_i - \\overline{x})^2}\\\\  \n",
    "\\end{align}$$\n",
    "\n",
    "So, even though we went to great lengths in __Chapter 5__ to determine these coefficients through the gradient descent algorithm, it turns out one can simply solve for the two:\n",
    "\n",
    "$$\n",
    "\\hat{m} = \\frac{\\sum_{i=1}^n (x_i - \\overline{x}) (y_i - \\overline{y})}{\\sum_{i=1}^n (x_i - \\overline{x})^2}; \\hspace{25pt}\n",
    "\\hat{b} = \\overline{y} - \\hat{m}\\overline{x},\n",
    "$$\n",
    "\n",
    "where the 'hats' on our $\\hat{m}$ and $\\hat{b}$ now indicate they are the 'best', _regressed_ parameters. But remember, most regression contexts don't come with this guarentee! There are some more (higher-order derivative) details on why these $(\\hat{m}, \\hat{b})$ are the best for linear regression, and the sad part is that this solvability of the linear model is really not something we can expect for all other regression contexts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.1 Example: Exploring linearity in heights and weights\n",
    "Let's start with linear regression by reviewing our data to see what we might expect for a relationship. Since we can, let's plot the baseball players' heights and weights again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAGPCAYAAACQ4537AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzs3Xt8VPWZ+PHPA0TlJhOugluNYGukl59dsFC8otTivVqtpcZddddL1W2V1ha3xSq2a6tWrVqvrbWKrltZb8VeLF4qiKBEqrtSLgbBJQkkkExIQu55fn+cMzAZkuHkzDeZk8nzfr3mFc5lnvnOJOTJ93u+z/mKqmKMMca4NCDbDTDGGJN7LLkYY4xxzpKLMcYY5yy5GGOMcc6SizHGGOcsuRhjjHHOkovpQERuEhFNepSJyH+LyKSkcx4TkVXZbGdQ/nu4xkGcAj/WGd183on+8z6zj/O+JiIXZ9TIHCAiF/uf17BuPu8mEdke4LzviciJoRtoArPkYjpTA3zRf3wXOAp4RUSGZrVV2VWO93ks66H4XwMu7qHYfclLeJ/zrh6K/z3gxB6KbZIMynYDTCS1quoK/98rRORjYClwGvBM9prVOREZrKoNPfkaqtoErNjniSYjqloJVGa7HSZz1nMxQRT7Xws6Oygi40XkURHZKCINIrJeRH4sIvslnfO2iDzWyXMfE5HVSdsjReRhEdkmIo0islxEpqU8R0VkrojcLSKVwP/so/0DReQ/RKRSRCpE5Jcisn9KzENE5GkRqRKRXSLyZxE5Iun4XsNiIrK/iDwgInER2SEit4vItSLS2W0vRovIMyJS539OVyV/BsBXgROShiNv8o8dKyJLRWSn//ibiJzf1RsVkY9E5PZO9j8jIsv8f+eJyB0i8rGINPlDn88lf7/2RURuFpH1SdtDRaRFRN5N2jdaRNpF5EtJ+44Tkb/6n/EOEXlERIYnHd9rWMz/3vzR/9n6yD9nkYi83km7Pi8iK/z4q0XkuKRjm4BRwI+SPucT/WP/IiJr/NfY7rfx00E/D7M3Sy4miAL/69Yujo8GqoC5wGzgduAS4N6kc34NnJfyS2MYcB7wqL+9P7AEmAVcD3wF76/YJSJyUMprXg+MBy4CvrWP9n8HmAAU+W27Avh2UjtG4g13HQFciTdENdR/3cFp4t6GN5R1M3AhcIj/Wp15BHgPOAd4HfiliHzBP3YL8Bqwmj3Dkb8SkQOBxcBGvORzHvAEEEvTpt8BHZKP/zmfDjzt77rBb+984EvAtXhDoQPTxE21FPikiIzzt2cArcD/89sNcBzQDrzlt+MYvO/vVv+9XIvXG/5NVy8iIgK8CBwJXIr3M/YtYFonpw8Bfgs8hPd5NQHPisgQ//g5/vv8NXs+53dF5HjgQbzP9lT/dZYDIwJ/GmZvqmoPe+x+ADcB2/GGTAcBn8L7xbcTGO+f8xiwKk2MQcA3gEZgP3/fgUA9cEnSeZfi/QIY5W//C9AMfDIlVglwe9I+Bd4N+H4UeCNl3/PAiqTtW4AdwMikffl4v4iu9rcL/Fhn+NujgAbg+qTnCPCB999q974T/ectSNqXh5c0f5q0bxHweko7p/rPHd6N79/n/edMT9o3B+8X/zh/ezHw8wx/ToYCLcB5/vYC4L+BMmC2v+9O4J2k5ywFXkuJc5Lf3s/42xf728P87dP97aOTnnOw/9qvJ+27yT/vpKR9R/n7Zift2w7clNKG7wLF2f6/l2sP67mYzozC+8/bAqwDJgIXqGp5ZyeL59rEsIL/vCeB/fH+mkdVd+L9Ar046akXAy+q6g5/exbeENxHIjJIRBLXBP+K94s22R+68X5eTtleA/xD0vYs4C/AzqTXrfXbkvq6CZ8FDsD7qxrwMwr8fl9tUNUWYENKGzpTAtQBT4nI2SKSrseSiL0aWA9ckLT7AuCvqrrN3/4bcLF4M6c+5/cOukVV64F38XonAMcDb+AlkOR9SwH83sMXgd8lPmP/c16G9/MypYuXOhrYqqrvJL12KXuGapM14/UKE9b4X/f1Of8N+LyI3CUix3dneNB0zZKL6UwN3n/qqXj/MQtU9Y9pzr8WuAN4Djgb+AJwtX/sgKTzfg0cJyITxZvafBz+kJhvNDCdPYkt8bgE+ETKa24juHjKdnNKu0bj/QJOfd2ZnbxuQmKYLvXic1cXo/fVhr2oajXesFUe3nBXpYi8JCIT0z0P+C/gfD/pH4g3VPl00vEfA78ErsIbqvs/Efn23mH2aSne93M/vGGqpUn7huP1HJb65+bjDbvdT8fPuMl/f+k+584+08721apqe2JDVZv9f+7rc16C9zN2PF5y2i7edbn+PDsyYzZbzHSmVVW7U8dyPrBIVX+Q2CEik1NPUtU3RGQDXo9F8IZQknsVVcAq4JudvEZTarhutG9fqvB6ILd0cqy2i+ckrj+N8Z9P0rYz6s3am+1f+5mFN9T0FF4S7sp/4V1PORY4DO+PyGeTYjYCNwI3isgn8a4z3S0i61T1T91o3lLgOuBkvGT5N6AN7w+NmXjJJDF1O473PbuJznudZV28xlY6/0zH4A27OqGqvwV+KyJjgHOBu/C+9/NcvUZ/Y8nFuDCYvX/5X9jFuY/i/cUM8LiqtiUdewU4BfhYVSvcNjGtV/Au4n+gwac0/w/eL7ez8S7sJy4+nxmyDWl7Mn67fi9eMeYN6QKp6gci8r94vbHDgCVJQ4+p524Qke/i9TQnA91NLoL3C/hNVW0Xkf/Buxb1HWCtelOLUdV6EVkBHKGqC7rxGu/gze76gqq+DSAiB+MNo73ZjTgJ+/qcK4GHRORcvM/DhGTJxbjwF+BbIrIS7zrBhcDhXZz7W7xhmUHsPUvocby/ol8XkTvwZkmNwhtm26qqd/VA28HrDRQBr4rIvUApMA44AVimqv+Z+gRV3SEijwA3i0gL8He8oZUDCderWgucLSJfAbbg/SX/ebxJD88DH+NdyL4CeDVAvP/CmxE3Args+YCIPId3zWI1XiI4D+/78YZ//FC87+Olqvp4Vy+gqlUisgZvOOkGf1+7iLyJdyH+kZSnfA+vGLcd7/pbLd41udOBH6jqevb2B7yhu9+JyA1+e3+ENyza3sn5+7IWOF1E/oR3PWsd3gX9kfhDYnif+wlYryUjllyMCwvwhil+7G8/izdddK+L26q61U9CpP4yUdVGEZnpx7sZ7xd8BfA2SRfOXVPV7SIyHfgJ3nBIDK8ifxnwfpqnfg/vesFNeL/onsC7rnRtiGbcj/dL7VG86xM3A/+Jl6j+AxiLd51hMfDvAeI9jTfM14SXnJItx+vVXI83ZLYG+GrSUKjgDWkFuSa7FPg0fmJK2nc6KXczUNVl/rTfm/E+q4HAZrzeUqfX0FRVReRsvOnFv/HP+wleQgxTxX893vWml/CmLs/E6x1dB3wdGO636SbgFyHiG594E1yM6R1+TUkpcI2q/jrb7XFNRJYAeap6QrbbkqtEZARer/Y+Vf1RtttjOmc9F9Mr/NlDk/GGamrx/irv0/xe1jS8Kbl5eL2Bk0kpYjSZEZEr8XqGG/B6yHPxprk/mu55JrssuZjeMgWvGHMz8E+q2lM3JuxNdXh3EbgB7yLxBuBiVV2U1Vblnkbg+8CheMOEbwOzVHVzVltl0rJhMWOMMc5ZEaUxxhjnLLkYY4xxrt9ecxk9erQWFBRkuxnGGNOnFBcXb1fVfd6Jot8ml4KCAlat6hMr9RpjTGSISKCJFDYsZowxxjlLLsYYY5yz5GKMMcY5Sy7GGGOcs+RijDHGOUsuxhhjnLPkYowxxrleTy4icp6ILBeRHSLSKCLrROSH/jrciXNERP5dRP5PRBpE5A0ROaqTWJNF5BUR2SUiZSKyQEQG9u47MsYYkyobRZSj8FbSux1vXe0v4C3McxBwjX/OPLw1wK/HWzluLrBERD6jqlsBRCQfWIK30NHZwCTg53gJ84e99F6MMcZ0oteTi6o+lLLrNRE5ELhaRP4Nb52GecCtqnofgIi8BWzCSz6JxHEl3trt56rqTuAvfpybROQ2f58xxpgsiMo1lx1AYlhsBt465L9LHFTVerwlc09Nes6pwJ9TksjTeAnHVgE0po+Jx+PMnTuXeDye7aYYB7KWXERkoIgMEZFj8dZbf0C9xWUKgTa8hZeS/d0/llCIN2S2m6p+jLeudvJ5xpiIi8fj5Ofnc9ddd5Gfn28JJgdks+dS7z+WAn/Fu74CkA/UqWpbyvnVwJCkC//5eNdsUlX7x/YiIpeLyCoRWVVZWZlp+40xjixYsCDttul7splcZgDHAd/BuyB/X0+/oKo+rKpTVXXqmDH7vGO0MaaX3HjjjWm3Td+TteSiqu+q6jJVvRNvWOybIjIJr+cxrJMpxfnALlVt9rergRGdhM73jxlj+ohYLEZ1dTXXXXcd1dXVxGKxbDfJZCgq67m86389DO86ykDgcGBd0jmp11jWknJtRUQ+AQxJOc8Y0wfEYjHuvPPObDfDOBKV2WLH+F8/ApYDO4HzEwdFZAhwJvDHpOf8EfiyiAxP2ncB0IB3DccYY0yW9HrPRUT+hFf8+AHerLBj8K67/Jeqlvjn/BSYLyLV7CmiHADcmxTqQbzhtGdF5GfARLxizDutxsUYY7IrG8Ni7wAXAwVAK7ARuAEvWST8FC+Z3IBX0b8K+JKqbkucoKrVInIy3kSA3+PNHLsLL8EYY4zJIvFKS/qfqVOn6qpVq7LdDGOM6VNEpFhVp+7rvKhcczEm56kqdXV19Nc/6Ez/YsnFmF6gqjz00ENcddVVPPTQQ5ZgTM6z5GJML6ivr2f58uUceuihLF++nPr6+mw3yZgeZcnFmF4wdOhQZsyYwebNm5kxYwZDhw7NdpOM6VF2Qd+YXqKq1NfXM3ToUEQk280xJpSgF/SjUqFvTM4TEYYNG5btZhjTK2xYzBhjjHOWXIwxxjhnycUYY4xzllyMMcY4Z8nFGGP6iXg8zty5c3tlGWlLLsYY0w/E43Hy8/O56667yM/P7/EEY8nFGGP6gQULFqTdds2KKI0xph9I9FwSwi4nbXdFNsYYs1ssFqO6uprrrrsudGLpDqvQN8aYfiIWi3HnnXf2ymtZz8UYY4xzllyMMcY4Z8nFGGOMc5ZcjOmnorbscl1dHXfccQd1dXXZbopxwJKLMf1Q1JZdrqurY/jw4Vx//fUMHz7cEkwOsORiTD8UtWWXH3zwwbTbpu+x5GJMPxS1ZZevvPLKtNum77EKfWP6qagtu1xXV8eDDz7IlVdeaSt2Rpgtc2yMSStqyy4PGzaM7373u9luhnGkV4fFROR8EXlRREpFpE5EikVkTso5B4rI3SKySUR2icjfReRaSfnTSkQOFpHnRKRWRLaLyH0iMqQ3348xxpjO9XbPZS7wEXAdsB04DXhKREar6r3+OY8BxwP/DnwIzATuBAS4C0BE8oA/A83A14GYf04MKOql92KMMaYLvZ1czlTV7Unbr4rIBLykc6/f8zgbuFZVH04659N4SeQuf995wJHA4ar6EYCItABPi8jNqrqhN96MMcaYzvXqsFhKYklYDUzw/z0Qr001KefE8XouCacC7yQSi+95vJ7MbDetNcYYE1YUpiJ/EVgPoKq1wO+A74nIUSIyXETOAL4G/DLpOYXA2uQgqtoMlPjHjDH70N7eztatW2lvb892U4Do3THAZCaryUVETga+Avw8afc/4SWO1cBO4EXgFlX9bdI5+Xi9mVTV/jFjTBrt7e0UFRVxzDHHUFRUlPUEE7U7BpjMZS25iEgB8BTwgqo+lnToLmAacAlwAvBD4CYR+RcHr3m5iKwSkVWVlZWZhjOmz6qoqGDlypWMHz+elStXUlFRkdX2RO2OASZzWUkuIjIS+COwGbgwaf/ngG8Cl6rqY6r6hqr+B3A3cIeIJNpbDYzoJHS+f6xTqvqwqk5V1aljxoxx9G6M6XvGjh3LtGnTKC8vZ9q0aYwdOzar7YnaHQNM5nq9iNKfEbYY2A84Q1V3JR1OXC/5W8rTVuNNMx4FVOINm3W4tiIi+wETAbspkTH7MGDAABYuXEhFRQVjx45lwIDsXn4VEa644gqKiooic8cAk5neLqIcBDwDfBKYraqpffHN/td/TNk/BajHq40Br9dztIgcmnTOWcD+wJ+cNtqYHDVgwAAOOuigrCeWhMQdAyyx5Ibe7rncj1c4+W1glIiMSjq2GljlPx4VkRvxCi6PBa4FfqF7rvItAn4APCsi8/GGyO4CnrIaF2OMyb7eTi6n+F9/0cmxw1R1k4icCfwYuBEYg9ebuYmkGWWq2iIis4H78KYuNwFPA9f3XNONMcYE1avJRVULApyzFfjXAOdtwZvGbIwxJmKiMdhqTAorqOt5USuiNLnFkouJHCuo63lRK6I0uceSi4kcK6jreVErojS5x5KLiRwrqOt5USuiNLnHljk2kRS1JXhzUXt7e2SKKE3fYcscmz4takvw5qJEEaUxPcH+XDHGGOOcJRdjjDHOWXIxxhjjnCUXY4wxzllyMaaXuLrrgKs4rir0o/a+TDRYcjGmF7i664CrOK4q9KP2vkx0WHIxphe4uuuAqziuKvSj9r5MdFhyMaYXuLrrgKs4rir0o/a+THRYhb4xvcTVXQdcxXFVoR+192V6llXoGxMxru464CqOqwr9qL0vEw02LGaMMcY5Sy7GGGOcs+RijDHGOUsuxvRTVvxoepIlF2P6ISt+ND3Nkosx/ZAVP5qeZsnFmH7Iih9NT7MiSmP6KSt+NGFYEaUxJi0rfjQ9yYbFjDHGONeryUVEzheRF0WkVETqRKRYROZ0ct6hIvKfIlIlIrtE5D0RmZ1yzsEi8pyI1IrIdhG5T0SG9N67McYY05XeHhabC3wEXAdsB04DnhKR0ap6L4CIfAJ4C3gPuASoB44CBieCiEge8GegGfg6EAPu9L8W9dabMcYY07neTi5nqur2pO1XRWQCXtK51993O1ACnK6qiRWMlqTEOQ84EjhcVT8CEJEW4GkRuVlVN/TYOzDGGLNPvToslpJYElYDEwBEZARwLnB/UmLpzKnAO4nE4nseryczu/OnmP4oF6vHXS1PnKvs84mGKFzQ/yKw3v/3PwJ5gIrImyLSIiJbROQG6TjHsRBYmxxEVZvxejyFvdFoE325WD3uanniXGWfT3RkNbmIyMnAV4Cf+7sSi0s8BCwFTgEeBX4MfDPpqflAvJOQ1f6xrl7vchFZJSKrKisrM2y9ibpcrB53tTxxrrLPJzqyllxEpAB4CnhBVR9L7Pa//lFV56nqa6p6I/Bb4IZMX1NVH1bVqao6dcyYMZmGMxGXi9XjrpYnzlX2+URHVir0RWQk8CZQC5yoqrv8/acCfwCuVNWHks4vAp4ARqjqThF5G/hAVS9JifsB8LqqXr2vNliFfv+Qi9XjrpYnzlX2+fSsoBX6vf7J+7Uoi4H9gDMSicX398RpqU/zvyYGUNeScm1FRPYDJpJyLcb0b4nq8VxJLLBneWL7xdk5+3yiobeLKAcBzwCfBGaraocBUVXdBHwAnJTy1JOBElWt87f/CBwtIocmnXMWsD/wpx5oujHGmG7o7TqX+/EKJ78NjBKRUUnHVqtqEzAf+G8RuR14GTgRuAj4p6RzFwE/AJ4VkfnACOAu4CmrcTHGmOzr7eRyiv/1F50cOwzYpKrPicg/4SWPbwMfA1er6pOJE1W1xb8dzH3A74Am4Gng+p5svDHGmGB6NbmoakHA8xYCC/dxzha8aczGGGMixq54GROAi0r/qN0twCrZ+5ao/fzsiyUXY/bBRaV/1O4WYJXsfUvUfn6CsORizD64qPSP2t0CrJK9b4naz08QllyM2QcXlf5Ru1uAVbL3LVH7+QkiKxX6UWAV+qY7XFT6R+1uAVbJ3rdE5ecnaIV+b09FNqZPcrFOfNTWmk9Uspu+IWo/P/tif64YY4xxzpKLMcYY5yy5GGOMcc6Si8lprgoFXcRpa2ujpKSEtra2jNrS0tLCihUraGlpySiOq/a4iuOKq2LDvla0GDWWXEzOclUo6CJOW1sb06dP56ijjmL69OmhfxG3tLQwbtw4ZsyYwbhx40InGFftcRXHFVfFhn2xaDFqLLmYnOWqUNBFnE2bNrF27VpisRhr165l06ZNodpSXFxMPB5n4MCBxONxiouLQ8Vx1R5XcVxxVWzYF4sWo8aSi8lZrgoFXcQpKCigsLCQeDxOYWEhBQUFodoyZcoUYrEYbW1txGIxpkyZEiqOq/a4iuOKq2LDvli0GDVWRGlymqtCQRdx2tra2LRpEwUFBQwcODB0W1paWiguLmbKlCnk5eWFjuOqPa7iuOKq2DAqRYtRE7SI0pKLMcaYwIImFxsWM8YY45wlF2OMMc5ZcjHGGOOcJRdjjDHOWXIxOa21tZX333+f1tbWbDeF6upqrrnmGqqrqzOKE7UKfVeV7Lbscm6x5GJyVmtrK5MmTWLKlClMmjQpqwmmurqakSNH8stf/pKRI0eGTjBRq9B3Vcluyy7nHksuJmetWbOGsrIyDjjgAMrKylizZk3W2jJ//vy020FFrULfVSW7Lbuceyy5mJw1efJkJkyYQGNjIxMmTGDy5MlZa8stt9ySdjuoqFXou6pkt2WXc48VUZqc1traypo1a5g8eTKDBmV34dXq6mrmz5/PLbfcQn5+fug4UavQd1XJbssu9w3OK/RFZAQwFTgIOACoAtar6geZNDRbLLkYY0z3OanQF5EDReRqEVkJ7AD+AjwBPAL8N/C+iFSJyK9F5IsBGnW+iLwoIqUiUicixSIyJ8353xYRFZFFnRw7WESeE5FaEdkuIveJyJB9tcEYY0zP6zK5iMgPgU3AtcBrwLnAYcBwYD9gLDAN+HdgFPCqiLwqIp9J83pzgTrgOuAsP+5TIvJvnbz+WOAmoLKTY3nAn4FDga8D3wbOBx5O92aNMcb0jnSD0NOAs1R1WRfHt/uPVcCD/rDZlcAxwP928ZwzVXV70varIjIBL+ncm3LurcBi4BOdxDkPOBI4XFU/AhCRFuBpEblZVTekeV/GGGN6WJc9F1U9M01i6ez8GlX9mao+lOac7Z3sXg1MSN4hIl8AvgbM6yLUqcA7icTiex5oBmYHbbPZI1eXdI3SkrfNzc28+uqrNDc3Z9SWxsZGFi1aRGNjY0Zx1q1bx6GHHsq6desyitPU1MRLL71EU1NTRnGiVtTpStTa01uyO33G80VgfWJDvOkm9wK3qWppF7NPCoEORQuq2iwiJf4x0w2JQrjly5czY8YMrrjiipxYv8LV+3IRp7m5mVgsRkNDA4MHDyYej7Pffvt1uy2NjY0MHTqU9vZ2BgwYQH19PQcccEC346xbt47CQu+/SmFhIWvXruWII47odpympiaGDx9OS0sLeXl51NbWsv/++3c7TqKoc+3atRQWFrJixYpQM9ii9rMctfb0psDz/UTksyLylIh8KCL1/tcnReRzYV9cRE4GvgL8PGn3JcA44I40T80H4p3sr/aPdfV6l4vIKhFZVVm516WcfitXl3SN0pK3y5Yto6GhAYCGhgaWLQs8KNDB4sWLd1evt7e3s3jx4lBxTjnllLTbQS1ZsmT3XQJaWlpYsmRJqDhRK+p0JWrt6U2BkouIfAUoBj4PLALm+1//EVjlH+8WESkAngJeUNXH/H0j8K61fE9VG7obc19U9WFVnaqqU8eMGeM6fJ+Vq0u6RmnJ22OPPZbBgwcDMHjwYI499thQbTnjjDN214AMGDCAM844I1Scl19+Oe12ULNmzdpda5OXl8esWbNCxYlaUacrUWtPr1LVfT6AdcAz+HUxSfsFL8msCxIn6Xkjgb8DbwNDkvb/DHgHiCU9lgEv+P8e6J/3NvCbTuJ+APwySBumTJmiZo/29natra3V9vb2bDfFKVfvy0WcpqYmfeWVV7SpqSmjtjQ0NOgzzzyjDQ0NGcVZu3atHnLIIbp27dqM4jQ2NurixYu1sbExozitra364Ycfamtra0ZxovazHLX2ZApYpQF+xwYqohSRXcA5qvrnTo59GXhOVQPVmPi1KEvwhr6+qKoVSceeB85O8/TjVHWZiDwOfFJVd9fWiMh+QA1eryd15tlerIjSGGO6z/Uyx6uAT3dx7DPAuwEbNQivB/RJYHZyYvH9EJiZ8ngPeMP/9//45/0ROFpEDk167lnA/sCfgrTFGGNMz+lytlhKtftcvBqSPLwpvxV4RZTnAP+KV8gYxP3AaXhFj6NEZFTSsdWquld9jIjEge2q+nrS7kXAD4BnRWQ+MAK4C3hKrcbFGGOyLt1U5DogecxM8C62/0fKPoCVQJB5g4kpKb/o5NhheHcE2CdVbRGR2cB9wO+AJuBp4PogzzfGGNOz0iWXS+mYXDKmqgUhnnNiF/u34E1jNsYYEzHpKvQfU9XfBn30ZqON6W0ulkt2Vcm+a9cuHnnkEXbt2pVRHFcV8VVVVVx++eVUVVVlFMfVMse2XHI02KIJxuyDi+WSE5XsZ5xxBsOHDw+dYHbt2sXQoUO5/PLLGTp0aOgE42qZ46qqKkaNGsUjjzzCqFGjQicYV8sc23LJ0ZHursjviMjbQR+92WhjepOL5ZJdVbI/+eSTabeDclURP2/evLTbQbla5tiWS46OLutcROQxunHNRVUvcdSmXmF1LiaoRM+lrKyMCRMmUFJS0u1VLV3dgyvRc0mor69nyJDuL2Pk6l5eiZ5Lwo4dOxg5cmS34yR6HCtXrmTatGksXLgw1GqUruKYrjlfiTLXWHIx3eFiueSmpiaWLFnCrFmzQiWWhF27dvHkk09y4YUXhkosCa6WOa6qqmLevHn89Kc/DZVYElwtc2zLJfcsSy77YMnFGGO6L+MKfX/p4k924wXzRORSEbko6HOMMcbkpnT9+3rgPREpxquIXw78rybdrdi//coUvMW7vgKUAZf1XHONMcb0BenqXL4FHIF3X6/v4FXh1/lruVSJSCuwEa8y/iC8pHKUqtrMMWOM6efSXu1S1f9T1R+o6iF4KzwW4d1c8nbgauBkIKbeksjPa3+9gGOcc1UI19LSwooVK3ZPAw7LxRLFq1evJhaLsXr16oza4qqIcuvWrZx22mls3bowOAxkAAAgAElEQVQ1ozguCkzB3ffKFe2nyxM7E+S+/Ln4sPVcoqutrU3nzJmjEydO1Dlz5mhbW1uoOM3NzZqfn68iovn5+drc3BwqTlNTkw4ePFgBHTx4cKj1WN59913Fm9qvgL777ruh2lJfX98hTn19fag45eXlHeKUl5eHitPS0qKHHHKIDho0SA855BBtaWkJFcfV98qV9vZ2feCBB/Siiy7SBx54IGfWYnGBgOu52Dw9EzmuCuGKi4uJx+MMHDiQeDxOcXFxqDguliieOXNm2u2gXBVRXnrppWm3g3JRYAruvleu9OfliV2x5GIiZ+zYsUybNo3y8nKmTZvG2LFjQ8WZMmUKsViMtrY2YrEYU6ZMCRXHxRLFr732WtrtoC688MK020E9+uijabeDmjx5MhMmTKCxsZEJEyYwefLkUHFcfa9c6dfLE7sSpHuTiw8bFou2trY2LS8vDz0kltDc3KxvvfVWxsMsLpYofvfdd3XEiBGhh8QS6uvr9eGHHw49JJZQXl6up556aughsYSWlhZ97733Qg+JJbj6XrmSa8sTu4LLZY5zkRVRGmNM9zld5lhEDvFXoezs2CAROaS7DTTGGJO7gl5z+Qj4fBfH/p9/3BhjjAGCJxdJc+wAvGWGjTHGGCDN7V9E5HPAUUm7ThORwpTTDgC+BqzvgbYZY4zpo9L1XM4BHvMfCtyYtJ14PAgcBny3pxpoeoerKmtVN1XNruK4qmavr6/n3nvvzajeoaamhnnz5lFTU5NRWzZu3MiRRx7Jxo0bM4pTVlbGSSedRFlZWUZxtm/fTlFREdu3b88ojqtll1397ERNn3tfXU0jA/KAocAwoB040d9OfuQFmZIWxYdNRd7DVZW1q6pmV3FcVbPX1dV1iFNXV9ftGPF4vEOMeDweqi0lJSUd4pSUlISKU1pa2iFOaWlpqDiVlZUd4lRWVoaK09raqlOnTtVhw4bp1KlTtbW1NVScXK2sj9L7ItMKfVVtUdV6Va1T1QGq+rq/nfyIxk2ATEZcVVm7qmp2FcdVNbuLgsNbb7017XZQp59+etrtoIqKitJuB3Xttdem3Q7K1bLLuVpZ3xffV7fqXETkU8A/4F1r6UBV/+CwXT3O6lz2cLGML3i94Iceeojly5czY8YMrrjiCkTSzQXp2TiulgSur69n2LBhu7fr6uq6XbFdU1NDLBbbvR2PxxkxYkS327Jx40YmTZq0e7ukpISJEyd2O05ZWRkHH3zw7u3S0lImTJjQ7Tjbt29nzJgxu7crKysZPXp0t+O4WnbZ1c9O1ETpfQWtcwk0hARMBt4H2vCGyFIfbUHiROlhw2IduaqydlXV7CqOq2r2uro6veeee0INiSXE43H9/ve/H3pILKGkpEQLCwtDD4kllJaW6syZM0MPiSVUVlbqhRdeGHpILKG1tVU//PDD0ENiCblaWR+V94XLCn0RWQqMBb4HrAH2uu+4qm4OmPgiwXouxhjTfU4r9PEKKL+jqi+o6gZV3Zz6CNio80XkRREpFZE6ESkWkTlJxw8UkZtF5G0RqRGRrSLynD8clxprhIj8RkSq/XOfFJFRAd+PMcaYHhQ0uZTQyXWWEOYCdcB1wFnAa8BTIvJv/vFD8Fa0/DNwHnAFMB5YKSKfSIn1O7wZbP8KXAwcDTzvoI3GGGMyFPSq7XeA20TkXVXNZHL9maqaPBn+VRGZgJd07sW7jcwkVW1InOAPyX0MXArc7O/7InAKcIKqvuHvK8VLQrNUdUkGbTSmR6gq9fX1DB06NCcuMhuTTroK/Xfw5q4nHAysFZFNQDz1fFX9wr5eLCWxJKwGvuof32t+napWichmIHkqy6nAtkRi8c97W0Q+8o9ZcjGRohGa7WNMb0g3LPZByuMPwJPAm50c+yCDNnyRNLePEZExwOEp5xQCazs5/e/+MdNNqtGqrHe1nvq2bds466yz2LZtW0ZxMq30r6+v5+WXX+aNN97g5ZdfzqhOobKykgsuuIDKysrQMQDKy8s55ZRTKC8vzyhOaWkpxx9/PKWlpRnFaW9vZ+vWrbS3t2cUx0REkCllPfUATsabynxxmnMeB3YAo5L2/QV4vpNzFwLLg7y2TUXeI2qV9a7WU9+6dWuH6vGtW7eGiuOi0n/Lli0dYmzZsiVUWyoqKjrEqaioCBWnrKysQ5yysrJQcVy9r7a2Np0zZ45OnDhR58yZk/EicabnkGmFfk8TkQLgKeAFVX2si3O+CRQB/6qqOxy85uUiskpEVmX6V18uiVplvav11C+77LK020G5qPS/6KKL0m4Hdc0116TdDuqf//mf024HNWfOnLTbQVVUVLBy5UrGjx/PypUrqaioCBXHREfQOpd097toB3YCfwOeVdW6APFG4g2v1QInqupeYw0ichbwLHCDqt6ecux3wBhVnZmy/yUAVd3nPTGszmUPdXQ9wFWclpYWxo0bRzweJxaLsW3bNvLyOl2rLq2SkhIOP/zw3dsffvhhh+r2oFxU+ruqiK+srGTs2LG7tysqKjpUyAdVXl7e4fXLysoYP358t+OUlpbyD//wD7u3t2zZ0uF9BtXe3k5RURErV65k2rRpLFy4kAEDsva3r0nDdYX+O8BWvERSjletX+5vbwXW4RVW/h/wqX3EGgIsx5vePLaLc44BdgH3dXF8AVDeyf4S4OdB3pMNi3UUtcp6F+upt7e3680336wHHXSQ3nzzzRm1yUWlv6uK+IqKCv3a174WekgsoaysTL/0pS+FHhJL2LJlix533HGhh8QS2tratLy83IbEIg7HFfqnAncBF6nqO0n7vwA8AVwP/A/wErBBVc/uIs4g4AXgC8AMVd3QyTmfBpYCrwPnqepeV/f8qcjLgeNUdZm/bypeEvySBpiKbD2X/kFt+q8xTgXtuQStc7kN+FFyYoHd039vAn6mqkeKyE+BX6SJcz9wGvBtYFRKRf1qYATwJ7xCy3uALyT9Qtipqmv8131LRF4GHheR7+L1oH4GLAuSWEz/ISIdbjppjOkdQZPL4UBDF8d2AQX+vzcD+6eJc4r/tbMEdJgfJzGA+1rK8b/iVeQnXIDXm3oUb0r1YuBbaV7b9CHW4zCmbwuaXFYDPxKRt1V1a2KniIwHfgQkpvMcCnS5rJ2qFuzjdTYBgX6TqGocuMR/mByiVnBoTJ8XdDrGlXgV8ptE5E0ReV5E3sS7XctBwDf98yYAj7hvpulp6qj40cVStS4XRqqoqODcc8/NeGrrzp07ufHGG9m5c2foGEuXLmXQoEEsXbo0o7a4Wrq5qamJl156iaampkjEccXVz7LJTODFwkRkMN79vabiJZSteBfQf6NJ9wLrK+yC/h6uegpRW/CpoqKCcePG7d7etm1bh2m8Qe3cubPDwl41NTUceOCB3YqxdOlSjj/++N3bb7zxBscdd1y32+JqAbSmpiaGDx9OS0sLeXl51NbWsv/+6Ua0ezaOK9br7Xmub7mPqjao6i9V9RJVPdX/en9fTCymI1c9BVdL1YoIV1xxBffff39GvxyuvPLKtNtB3XHHHWm3g5g5c2ba7aBcLd28ZMmS3bfWaWlpYcmScPNgXMVxpS8uB5yrurXMcS6xnsseUeu5uGI9l65Zz8WElXERJVABfN7/d6W/3eUjSFFNlB5WRNmRq+JHV0vVurJt2zY955xzdNu2bRnFqamp0fnz52tNTU3oGG+88YYOHDhQ33jjjYza4mrp5sbGRl28eLE2NjZGIo4rUVkOOFeRaRGliPwIeERVy/xalrRdHFW9OVDaiwjruRhjTPdlXESZnCxU9SZH7TLGZEitBsj0Ad26M5yI5IvIcSLyDRHJ9/cdICJ2hzljeoH61xSuuuoqHnroIZtuayIrUFIQkUEichuwBa9S/gm8inqA/8YrpDTGpKEO6i9sNpTpK4L2OH4CXAZcA0ykYxX9C8CZjttlTE5x1eMYOnQoM2bMYPPmzcyYMaPDzDFjoiRocvknYJ6q/gbvtvrJSvASjunDWltbef/992ltbc0ojou/ziE6yxMnbNy4kSOPPJKNGzeGen59fT0vvvgizz33HC+++GLoHoeI8I1vfIOCggK+8Y1vZHTNxVVlfWNjI4sWLaKxsTGjOK5+dlzFMRkKMqUM76aVs/x/D8S7C/E/+tunArVB4kTpYVOR92hpadFDDjlEBw0apIcccoi2tLSEiuNqmeMoLU+sqlpSUtIhTklJSbdjfPDBBx1ifPDBB6HasnPnzg5xdu7cGSpOY2Oj5uXlKaB5eXmhpxE3NDTogAEDFNABAwZoQ0NDqDhRW2rbdA3Hyxz/L9DpGi1+cnk3eDozUbNmzRrKyso44IADKCsrY82aNaHi1NfXs2zZMkaNGsWyZctC/3UepeWJAU4//fS020F86UtfSrsd1D333JN2OyhXlfWLFy+mvd1bcqm9vZ3FixeHihO1pbaNA0EyEF5iaQV+BXwZaMO7z9gtQBPw5SBxovSwnssernoura2tOnXqVB02bJhOnTo1dCFleXl5h7/Oy8vLQ8WJUs9lzZo1HWKsWbMmVFus59I7cUzXCNhzCfzLGPga3i3x25Me/wd8LWiMKD0suXTU0tKi7733XujEouolhYkTJ+r06dN14sSJoZNCbW2tzp49WydOnKizZ8/W2tra0G1yVc1eUlKihYWFoRJLwpo1a3TChAmhE0vCzp079cc//nHoxJLgqrK+oaFBn3nmmdCJJSFqS22bzgVNLt2+t5iIfAoYDVQB67S7ASLCKvTda29vp6ioiJUrVzJt2jQWLlzIgAHdL4FStftDGRNVQSv0093+ZTpQrKotrhsXBZZcekZ7ezsVFRWMHTs2VGJJULUqdGOiKOPbvwDLgUYRKQbe9B/LVXWHozaaHDRgwAAOOuigjOOICMOGDXPQImNMNqRLLl8GvgjMAK4AvgeoiGzATzTAm6q6tsdbaYwxpk/pctxCVf+iqgtUdTYwEvgccBWwAjgObznjD0Rku4i82CutNT3GVSGci2WOARoaGnjiiSdoaMhsLbqFCxciIixcuDCjOB9//DFHH300H3/8cegY77zzDsOGDeOdd97JqC2uCkzb29vZunXr7qnEYVVXV3PNNddQXV2dURxVN8WPrn52XP2f6LeCXPXv7AGcCCzGm5bcFjZOth42W2wPV9NJXU1F3rVrV4fptrt27QoV54knnugQ54knnggVZ/PmzR3ibN68udsx3n777Q4x3n777VBtcVVg2tbWpnPmzNGJEyfqnDlztK2tLVScqqqqDu2pqqoKFcfVFGJXPzuu/k/kIlwWUYrIUBE5SUR+KCJ/EJEq4C/AJ/B6MJdmluJMNrkqhHO1zPGiRYvSbgd10UUXpd0O6qtf/Wra7SBcLXPsqsC0oqKClStXMn78eFauXElFRUWoOPPnz0+7HZSr4kdXPzuu/k/0a11lHeAbwH141fctwHa8nsoPgZOB4UGyV1Qf1nPZw3ou6VnPpWvWc+l/yLSIEq9Ishb4JfDpIMH60sOSS0euCuFcLXO8a9cuffzxx0P/ckhIJJiwiSVh8+bNOnXq1FCJJeHtt9/WoUOHhk4sCVu3btUzzzwzdGJJaGtr0/Ly8tCJJaGqqkqvvvrq0IklwVXxo6ufHVf/J3JN0OSSrs7lZ3izxab6fwUUA2/5j+WqGq4fHRFW52KMMd0XtM4l3Wyx76vq8cCBeBfvFwGHAvcAW0WkREQWisjVIvKPARt1voi8KCKlIlInIsUiMqeT8y4TkQ0i0uifc3In5xwsIs+JSK0/Y+0+ERkSpB3GZIOq3Qre9B/p6lwAUNVW4B3/cQ94v9jx6l/+GfhF0FjAXOAj4Dq8azinAU+JyGhVvdePPQd4ELgJWAZcAiwWkaNV9X/9c/KAPwPNwNeBGHCn/7UoQDuM6VWqdksb078ESQgAiMj+wNF4SWUG3pDZGP9w0Mn/Z6rq9qTtV0VkAl7SudffdxPwW1W9xX/dvwKfB+axJ3GcBxwJHK6qH/nntQBPi8jNqroh6PsypjekzoYqKiqyOxCYnNblsJiITBCR80TkThFZAdQAbwA/BiYATwEXAJ9Q1YIgL5aSWBJW+/EQkYnAp4DfJT2nHXgGb92YhFOBdxKJxfc8Xk9mdpC2GNObbHli09+kq3PZAvwX3hLHlcDNeNdeRqjqF1T1OlV9RlVLM2zDF4H1/r8L/a+pt5T5OzBSRMYkndfhHFVtxltyuRDTba6qml1dV9iwYQOTJk1iw4bMOqEfffQRn/nMZ/joo4/2fXIapaWlHH/88ZSWhvtxFxG+/vWvM3r0aL7+9a9nNCRWVVXF5ZdfTlVVVegYkLsV+iYiuppGhlcYWRhkylnYB169TDtwsb99Id7MtFjKebP8/Z/ytzcAd3cSbxnwVJDXtqnIe7iqDXBVq7B+/foO7Vm/fn2oOBs3buwQZ+PGjaHibNmypUOcLVu2dDtGPB7vECMej4dqy44dOzrE2bFjR6g4uVrnYnoemVboq+qj2oM3pRSRAryhtRdU9bGeep2U17xcRFaJyKrKysreeMk+wVVVs6sq69mzZ6fdDurMM89Mux3UnDlz0m4Hceutt6bdDmrevHlpt4PK1Qp9EyFBMpDrB96NMP8OvA0MSdp/Gt5fQIemnH++v3+Mv/028JtO4n4A/DJIG6znsof1XNKznkvXrOfS/+B6mWNXD2AI3u36S4CxKccm+j+kX07ZPx/YkbT9OPBWyjn7AQ3AvwVphyWXjlxVNbuqsl6/fr1OnDgxdGJJcLE8saqXYI477rhQiSUhHo/r97///dCJJWHHjh162WWXhU4sCblaoW96VtDk0u1ljjMhIoOAF4AvADO0kynDIrIOWKqq/+pvDwD+BryvqkX+vjnAE8AkVd3s7zsPb5bZEZ3FTWUV+rlP1WpLjHEt4wr9HnI/3tDXLcAoEZme9NjfP+cm4BL/DswzgUeBTwI/TYqzCG+22LMicpqfbO7Du5hvNS4GsHF8Y7IpcBGlI6f4X3/RybHDgE2q+p8iMgz4Pt5w2AfAGepX5wOoaouIzMZLKL8DmoCnget7svGmb0nUliR6LlZbYkzv6dVhsSixYbH+QVWpr69n6NChNiRmjANRHRYzjqm6KTyrqalh3rx51NTUZBQnHo8zd+5c4vF4RnFcFeZt2LCBz372sxkXY7p4Xzt37uTGG29k586dGbXF1WfjqojSmM5Yz6UPc3XBuqamhlgstns7Ho8zYsSIbseJx+Pk5+fv3q6uru4QN6jq6mpGjhy5e7uqqqpD3KDWr1/PEUccsXt73bp1fOpTn+p2HBfva+fOnR0+05qaGg488MBut8XVZ9Pe3k5RURErV65k2rRpLFy4kAED7G9Ns2/Wc+kHXF2wdlXgt2DBgrTbQbkqzPvyl7+cdjsoF+/rjjvuSLsdlKvPxlURpTFdsZ5LH2Y9l/Ss59I167mYsIL2XLJSoR+FR64UUboqPHNV4LdhwwY9/PDDdcOGDRnFcVWYt27dOi0oKNB169ZlFKe6ulqvu+46ra6uDh2jpqZG58+frzU1NRm1xdVn46qI0vQvRLGIMkpyoecSNdu3b2fMmDG7tysrKxk9enQWW2SMcc2uuZhed+2116bd7sva2tooKSmhra0t200xpk+w5GKcufvuu9Nu91VtbW1Mnz6do446iunTp1uCMSYASy7GmdGjR1NZWcmFF16YU0NimzZtYu3atcRiMdauXcumTZuy3SRjIs+Si3Fq9OjRLFy4MGcSC0BBQQGFhYXE43EKCwspKCjIdpOMiTxLLgZwV62d6XLACa6WOS4rK+Okk06irKwsdIyBAwdyzz330NLSwj333MPAgQNDxamtreUnP/kJtbW1odsC7pak3rFjB5dccgk7duzIKI6ruzKYHBNkSlkuPnJlKrILrhaOcrGolqq7xcJKS0s7xCktLQ0VZ+XKlR3irFy5stsxdu7c2SHGzp07Q7XF1cJu27dv7xBn+/btoeJUV1d3iJPJVG3TN5DpMsem/3BVre1iOWBwt8xxUVFR2u2gZs6cmXY7iHvuuSftdlCulqT+7ne/m3Y7KFd3ZTA5KEgGysWH9Vz2sJ5LetZz6Zr1XPoforrMcVQellw6clWt7WI5YFV3yxyXlpbqzJkzQyeWhJUrV+qQIUNCJZYEVxX6rpak3r59u1588cWhE0uCi7sXmL4jaHKxCn1jeoGqLblscoNV6JusUHWzvkyusSWXTX9jycU4k/jr/KqrruKhhx6yBJMkseTy5s2bbcll0y/YsJhxpq6ujquuuopDDz2UzZs3c//99zNs2LCstkk1OsscR6ktxoRlw2IR5+pGiFGKM3ToUAoLC/nVr35FYWFhRn+d19fXc++992Y0fKSq3H333ZxwwgncfffdGfWkKioqOPfcczNaVKu2tpbbbrst4yLKpqYmXnrpJZqamjKK42oI04ZCTaeCXPXPxUc2Z4u1trbq1KlTddiwYTp16lRtbW3NiTjl5eUdpqWWl5eHilNXV9chTl1dXVbbs23btg5xtm3b1u0YNTU1HWKEnTHW2NioeXl5CmheXp42NjaGitPe3q4PPPCAXnTRRfrAAw+EXg/IVRzTd2BFlNHl6kaIUYtz6aWXpt0O6tFHH027HdTChQvTbgd15ZVXpt0OwtUyx0uWLKGlpQWAlpYWlixZEiqOqwkGNlHBdClIBsrFh/Vc3MeJWs+ltra2Q5za2tpQcazn0vNxTN+BFVFGN7moer/QP/zww9C/yKMap7y8XE899dTQiSWhrq5O77nnntCJJaG2tlZvv/320IklwUUxpqsiysbGRl28eHHoxJLgaolsV3FM3xA0udhsMWP2obW1lUmTJlFWVsaECRMoKSlh0KBB2W6WMVlhs8WMwc1MpjVr1lBWVsYBBxxAWVkZa9ascdhCY3JTrycXETlcRB4SkfdFpE1EXu/knPEi8hsRKRWROhFZLSIXdnLeZBF5RUR2iUiZiCwQkXCLbZico+qmqHPy5MlMmDCBxsZGJkyYwOTJkx231Jjck42ey6eB04B1wPrUgyIyAHgROAH4HnA2sAJYKCLnJp2XDyzBu0B6NrAA+A5wcw+33/QR9fX1vPnmm4wfP54333wz9EymQYMGUVJSQnFxsQ2JGRNQNpLL71X1E6p6PvBBJ8c/BUwFvq2qT6rqK6r6TWA1cEHSeVcCg4FzVfUvqvogXmKZKyIH9vB7MH3AkCFDaGtrY9GiRbS1tTFkyJDQsQYNGsTnPvc5SyzGBNTryUVV97WObp7/tSZlfxxIvmfGqcCfVXVn0r6n8RLOCRk1sg9xVa3d0tLCihUrdtdQhNXc3Myrr75Kc3Nz1uPs2rULVeXwww9HVdm1a1foWC6Wgd61axePPPJIRu0Aq6w3fUSQKWU99QAWAa+n7BO8YbDXgU8CBwIXA03ASUnnVQA3dRKzHrh+X6+d7anILriqeWhubtb8/HwVEc3Pz9fm5uZQcZqamnTw4MEK6ODBg7WpqSmrcVzVy7hYTK2+vr5DW+rr60O1xepTTLbRVyv0/cafiterWo/Xg3kYuFRVX006NR+vN5Oq2j+2FxG5XERWiciqyspKtw3PAlfV2sXFxcTjcQYOHEg8Hqe4uDhUnGXLltHQ0ABAQ0MDy5Yty2qcp556Ku12UC6WgX7yySfTbgdllfWmr4hccvEv6D8OjMK7xjITuBv4tYiEW0zdp6oPq+pUVZ06ZsyYzBubZbNmzSIvzxtFzMvLY9asWaHiTJkyhVgsRltbG7FYjClTpoSKc+yxxzJ48GAABg8ezLHHHpvVOBdeeGHa7aDGjh3LtGnTKC8vZ9q0aYwdOzZrbXF1635bAsD0uCDdm5560Pmw2Fl4QwefTNn/n8D7SdsVwI86idlvhsVU3VVrNzc361tvvRV6SCyhqalJX3nlldBDWa7j1NfX68MPPxx6GCrBxTLQrtpilfUmm+gLFfoisggYraonJu37Hl7SGJpybof9IvIGUKqqc5LO+QTwMXCWqv4+3Wtbhb4xxnRfX67Q3wwMEZEjUvZPATYlbf8R+LKIDE/adwHQAPy1R1tojDEmrV6ftC8iQ/CKKAEOBg4UkfP87T/4j4+B50VkAVAJnA58Dbg6KdSDwLeAZ0XkZ8BE4CbgTu04PdkYY0wvy0ZF2FjgmZR9ie3DVHWTiJwM3Ar8HG8qcgle0eTDiSeoarV/3n3A7/Fmjt2Fl2CMMcZkUa8nF1XdRMdiyM7O+RA4P0CsNcBJblpmjDHGlShec+kXNGLV0a4q9Gtqapg3bx41Nak3WMhOe1zdwcDFHQPa2tooKSmhra0to7YY0xfYjZKyQNW7W+/y5cuZMWMGV1xxBSJpO3M9qqWlhXHjxhGPx4nFYmzbtm13/Ux31NTUEIvFAPjZz35GPB5nxIgRWWtPU1MTw4cPp6Wlhby8PGpra9l///27Hae5uZlYLEZDQwODBw8mHo+z3377dStGW1sb06dPZ+3atRQWFrJixQoGDrQbeJvcZT2XLIhadbSrCv1bb7017XZvt8fVHQxc3DFg06ZNrF27llgsxtq1a9m0aVOothjTV9hKlFnQH3ougPVckljPxeSKoHUullyyRFWpr69n6NChWU0sCS0tLRQXFzNlypRQv8gTampquPXWW7nhhhtCJRbX7WlqamLJkiXMmjUrVGJJaG5uZtmyZRx77LHdTiwJbW1tbNq0iYKCAkssps+y5LIP2U4uuSpqSdMY41ZfrtA3fVRiuC/TZYWNMX2fJRfjTNQmKhhjsseSi3HGbuNujEmw5NJNdXV13HHHHdTV1WW7KQA0NjayaNEiGhsbs90URITzzz+fgQMHcv7552d0zcVVwWFFRQXnnntuqAW+XLenqqqKyy+/nKqqqozaErUCXGM6Yxf0u6Guro7hw/fchLm2tpZhw4a5blpgjY2NDB06lPb2dgYMGEB9fT0HHHBA1tpTVVXFqFGjdjkqwFIAAA/lSURBVG/v2LGDkSNHdjuOq2m7FRUVjBs3bvf2tm3bQi305aI9rj6bqE1jN/2PXdDvAQ8++GDa7d62ePFi2tvbAWhvb2fx4sVZbc+8efPSbgflquDwyiuvTLvdm+1x9dnYdS3TV1jPpRus55Ke9Vy6Zj0XkyuC9lyyusxxNh9hlzmura3V22+/XWtra0M937WGhgZ95plntKGhIdtNUVXVHTt26GWXXaY7duzIKE5ra6t++OGH2tramlGcbdu26TnnnKPbtm3LentcfTa2PLHJJvrCMsfZZEWUxhjTfXbNxRhjTNZYcjHGGOOcJRfjlFoNhjEGSy7GIbV7ixljfJZcDODVyWzdunV33UwYLmswXC1zHKWlhZ977jlEhOeeey6jONY7NH2BJRdDe3s7RUVFHHPMMRQVFYVOMK7uLZZYLGzGjBmMGzcudIJJ1KccddRRTJ8+PasJ5rnnnuPcc88F4Nxzzw2dYKx3aPoKm4ps2Lp1K8cccwzjx4+nvLycN998k4MOOihULHWwnsuKFSuYMWMGAwcOpK2tjeXLlzN9+vRuxykpKeGoo44iFosRj8f529/+xqRJk0K1KVOdfRZh/u/V1dVx1VVXceihh7J582buv//+rBbymv7HpiKbwMaOHcu0adMoLy9n2rRpoarYE0SEYcOGZVQ1PmXKFGKxGG1tbcRiMaZMmRIqTkFBAYWFhcTjcQoLCykoKAjdpkw9++yzabeDsjtPm77Cei4G8IbGKioqGDt2LAMGZP9vDlfLHEdpaeHE0Nizzz7LOeecEzqOi96hMWHZMsf7YMnFGGO6L7LDYiJyuIg8JCLvi0ibiLzexXmfFZHFIlIjIrUi8raITEk5Z7KIvCIiu0SkTEQWiEh2/zw1xhjDoCy85qeB04AVQKfjHSJyFLAUeAG4wN99NDA46Zx8YAmwBjgbmAT8HC9h/rCH2m6MMSaAbCSX36vqCwAisggY3ck5D/rnFSXt+1PKOVfiJZtzVXUn8BcRORC4SURu8/cZY4zJgl4fFlPVtEUUIjIZmAbcu49QpwJ/TkkiT+MlnBMyamQ/ZIV56cXjcebOnUs8Hg8dI0oFncb0tOxPC9rbNP9rvoi8JyKtIlIiIv+Scl4hsDZ5h6p+DOzyj5mArDAvvXg8Tn5+PnfddRf5+fmhEkyUCjqN6Q1RTC6J6r3HgSeBL+ENif1KRE5LOi8f6Ox/ebV/bC8icrmIrBKRVZWVlQ6b3LfZ0rnpLViwIO12EK6Wbjamr4hicklM3P+Vqt6mqq+p6tXAa8ANmQRW1YdVdaqqTh0zZkzGDc0VVpiX3o033ph2O4goFXQa0xuycUF/X6r9r6+l7H8VuC7lvBGdPD8/KYYJQES44oorKCoqssK8TsRiMaqrq1mwYAE33ngjsVis2zEGDhzIihUrIlPQaUxPi2Jy+bv/NfU3nADJkwHWknJtRUQ+AQwh5VqM2bfEbVtM52KxGHfeeWdGMQYOHJi1e5sZ09uiOCy2HK/ncVLK/pOB95K2/wh8WUSGJ+27AGgA/tqjLTTGGJNWr/dcRGQIXhElwMHAgSJynr/9B1XdJSILgNtEJA68A3wVOJ6OU4wfBL4FPCsiPwMmAjcBd1qNizHGZFc2hsXGAs+k7EtsHwZsUtW7RWQA8G94CWMdcJ6qLk08QVWrReRk4D7g93gzx+7yzzfGGJNFvZ5cVHUTe19P6ey8O4G0g9yquoa9h8+MMcZkWRSvuRjjbJljY0x2RHG2mOnnEsscx+NxYrEY27Zty2hNF2NM77Oei4mc4uJi4vE4A/9/e/cebFVZxnH8+5NOCBaCQqUJamoSWqPCGNQ0xGgXK8XMCzNZYTqmdrG8VDhqeJnGNCVTEi85juMMpIaTlkZeolTGKTA1FRi1jjeUuBxAQEXt6Y937dzus/c5e++zzr5wfp+ZM5z9rnet9Z53Fuc5613rfZ9Bg1i3bh1LlixpdpPMrEYOLtZy8kpzbGbN4+BiLaejo4OVK1eyaNEiD4mZtSk/c7GW1NHRwcSJE5vdDDOrk+9czMwsdw4uZmaWOwcXMzPLnYOLtSSnXTZrbw4u1nKcdtms/Tm4WMtx2mWz9ufgYi3HaZfN2p8G6pDDhAkTYvHixc1uhlUQEWzatMlpl81ajKQlETGht3qeRGktyWmXzdqbh8XMzCx3Di5mZpY7BxczM8udg4uZmeXOwcVakmfom7U3BxdrOZ6hb9b+HFys5XiGvln7c3CxluMZ+mbtzzP0rSV5hr5Za/IMfWtrnqFv1t4aPiwmaU9JV0t6TNJbkhb2Un+WpJD08zLbxkm6V9JmSSsknS9pUL813szMqtKMO5d9gC8ADwEdPVWUNA44HthQZtsI4B7gSWAqsAdwKSlgnp1vk83MrBbNeKB/R0SMjoijgCd6qXsFcDnQVWbbScAQ4IiIuDsi5gDnAadJGpZri83MrCYNDy4R8d9q6kk6EhgLXFShyiHAgogovquZRwo4k/vUSDMz65OWfBVZ0hDSENePI6LSJIexwLLigoh4DticbTMzsyZpyeACzABeAm7qoc4IYF2Z8q5sm5mZNUnLvYosaXfgDGBK5DwJR9KJwIkAY8aMyfPQZmZWpBXvXC4C7gKWSxouaTipnYOzz4UZdV3A9mX2H0H5FwCIiGsiYkJETBg1alR/tN3MzGjN4LI3cAQpQBS+RgPfyb7/YFZvGSXPViSNBoZS8izGzMwaq+WGxYATgNKp2fOAvwBXAauysruAMyW9NyJeycqOAV7N6pqZWZM0PLhIGkqaRAnpLmRY9toxwJ0R0W3BL0mvAc9HxMKi4jnA94D5kn4GfAiYCVxW8nqymZk1WMMXrpS0G/DvCpt3j4jOMvt0ArdGxBkl5eOAK4FJpDfHrgNmRsRbVbRjFfBsDU236o0EVje7EVs593FjuJ+72zUien1oPWBXRbb+I2lxNaumWv3cx43hfq5fKz7QNzOzNufgYmZmuXNwsf5wTbMbMAC4jxvD/VwnP3MxM7Pc+c7FzMxy5+BiNZO0MMsOWu5rUlZHks6S9LykVyX9VdJ+zW57u6iyjzvLbHu52W1vJ5KmSXpY0kZJL0q6UdLOJXV8LdfBw2JWs2x+UWlCtvOB/YGdIuJNSTOAc4EzScvxnAYcCOwbEf4F2Isq+7gTeJCUVK9gS0Q83JhWtjdJhwG/A2YDtwE7AReSlpkaX8g95Wu5Pg4u1meS3g28DPwmIk6WtC2wErg0Is7P6mwHdAJXR4TTUNeotI+zsk7KTC626kiaB+wVEeOLygoBZ1xELPW1XD8Pi1kePk9ajXpu9vkTpL+6by5UyJK+3UHKIGq1K+1j67sOYH1JWSFHVGH1dV/LdXJwsTxMA14A7s8+jwXeAp4qqbcUZwmtV2kfFxwvaYuk9ZJulbRrE9rWrq4HPiXp65KGSfowaVjsvoh4Mqvja7lODi7WJ9lCpIcBNxcldxsBbCyzxlsXMDQb4rEqVehjSMM3pwAHkZ4HTALul1Quz5GViIg/ANNJc1nWA8uBQcBXiqr5Wq6Tg4v11aHAdni4pj+V7eOIODUi5kbE/RFxDfA5YGfguCa0se1ImkJaXf1yYArp7nAH4DZJg5rZtq1BK+ZzsfYyDXi6JFVCF/AeSYNK/uIbAWyOiC0NbWH7K9fH3UTE45KWAwc0pllt71Lg9oj4UaFA0iOkN8KmAvPxtVw337lY3bLhl0PofteyjDS8sGdJ+VicJbQmPfRxJZF9We/GAo8UF0TEclLCwT2yIl/LdXJwsb74MjCY7r/4FgEbgKMKBdlzg0NJGUStepX6uBtJ+5J+6S3p70ZtJZ6l5C5P0keAIaRXjcHXct08LGZ9MQ14NCKWFhdGxGuSLgLOkdTF2xPPtuGdE/6sd2X7WNIXgWOB3wMrSEHlbOA54IYGt7FdzQFmSVpBChTvJ02W7ATuBF/LfeHgYnWRNJL0ltI5FapcRPoPOAPYEVgMfCYiVjamhe2vlz5+Hngf8AtgOLAG+CNwltN8V+2XwBbgZOAk0hyXB4AZ2VyWAl/LdfAMfTMzy52fuZiZWe4cXMzMLHcOLmZmljsHFzMzy52Di5mZ5c7BxczMcufgYgOSpJmSVlfYdoOkHtfxKrPPblma4S/VuN+ns/327aXe0ZKm13Dc8ZK6JA2r5TxljnOkpOVeyNFq5eBilo+XSEveP9BPxz+atDx8tS4E5hRNqHyY1L5najzvfFLirK/VuJ8NcA4uZjmIiNcj4qGIWNd77f4laS9S5srrC2URsSFr36u1HCvLI38j8N18W2lbOwcXsypIGiNpnqS1kjZLWiBp76Lt3YbFJA2WdJWkdZLWSLpE0vcllVsWY6SkWyRtlPQvSacUHecGUgKrydk5QtLMHpr7DeCxiPh/9sRyw2LZ51Ml/VTSKkn/kTRb0uCS4/0WOEDSPlV2l5mDiw1skt5V+sXb+dMLdXYgDXftTVqD6mhS8q57JA3p4fAXk4ayzgO+CowBTq9Q91rgUdIqyAuB2ZIOzLZdAPwZ+AdpaGsScF0P5z2ItJpvNU4nJRg7FrgE+BZwanGFbNHMLuDgKo9p5oUrbUDbEXijwrbiZet/QAom+0XEWgBJD5JWz/0mMLt0Z0k7AicC50bErKxsAfB4hfPNjYgLs3oLSUu6HwH8LSKekbQW2CYiHurpB5IkYH/gpp7qFemMiOnZ9wskfTI778Ul9R4DDsSsSg4uNpCtp/xf4z8Bdir6fDBwN7Ahu7MBeIUUgCZUOPZHgW2B2wsFERGS7gDGlan/p6J6b0h6Ctilyp+j2AhS/peyb8L1dN7Mk5T/mVYDH6ijPTZAObjYQPZmudTBktbwzuAyEpgIHFPmGPdWOHbhF/GqkvLSzwWlLwJsIQWnWhX2eb3K+tWe9/U622MDlIOLWe/Wku5ALiiz7ZUK+7yc/Tsq25+iz/2pcK7hOR93OO/8Ocx65OBi1rt7SQ/xn6jhVd5/Aq8BU8meX2TPQw6tsw1V3clkmROfA3av8zyV7EZKRmZWFQcXs95dRnqb6j5JVwAvklLiTgYeiIhu+e0jYo2ka4HzJL0BLAWOA4YB9WToWwZMlXQ48AKwIiJWVKj7IDC+jnOUJWk7UhrlSllHzbrxq8hmvYiI1aRnLsuAWaSH4BcD25Peoqrkh6R89jOBucBK4NdAPWmIf5Wd93rg76Q30SqZT5oT09Nr0rX4LLAZWJDT8WwAcJpjswaSdA/QERGT+/Ec7ybd3Xw7Im7J4XhzgU0RcUKfG2cDhofFzPqJpCnAx0nrenWQ3jY7CDiqP88bEVskXUKaDNmn4CJpNOm50cfyaJsNHA4uZv1nI3A4MIP0MP4pYHpE3NqAc18JDJW0fUSs78NxdgFOioinc2qXDRAeFjMzs9z5gb6ZmeXOwcXMzHLn4GJmZrlzcDEzs9w5uJiZWe4cXMzMLHf/A8b5qRphBpahAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## set the figure dimensions\n",
    "scat_fig = plt.figure(figsize = (6,6))\n",
    "    \n",
    "## make a scatter plot\n",
    "_  = plt.scatter(\n",
    "    baseball_data[\"height\"],baseball_data[\"weight\"],\n",
    "    color = \"black\", s = 5, alpha = 0.5\n",
    ")\n",
    "\n",
    "## Set the tick and label fontsize\n",
    "plt.tick_params(labelsize=15)\n",
    "\n",
    "## Set the title\n",
    "_ = plt.title(\"Player heights vs. weights\", fontsize = 15)\n",
    "\n",
    "## Set the y-label\n",
    "_ = plt.ylabel(\"Weight (lb)\", fontsize = 15)\n",
    "\n",
    "## Set the y-label\n",
    "_ = plt.xlabel(\"Height (in)\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.2 Executing linear regression\n",
    "It should be reasonably apparent that these data trend positively together, i.e., as height rises so does weight. But what would be the best line to explain this relationship precisely? For practice, let's use a function and our formula from __Section 7.1.2__ to determine the best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.60288682] [-205.27886031]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def linear_model(x, y):\n",
    "    x_bar, y_bar = np.mean(x), np.mean(y)\n",
    "    m = sum((x - x_bar)*(y - y_bar))/sum((x - x_bar)**2)\n",
    "    b = y_bar - m * x_bar\n",
    "    return (m, b)\n",
    "\n",
    "m, b = linear_model(np.array(height_train), np.array(weight_train))\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.3 Depicting our model\n",
    "Now that we have estimates $\\hat{m}$ and $\\hat{b}$, we can form the model using vector operations:\n",
    "\n",
    "- `y_hat = baseball_data[\"height\"]*m + b`\n",
    "\n",
    "We can then plot these as a line over the data to see the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAGPCAYAAACQ4537AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzsnXl8VOX1/98HiMomQRAVW4iobcS2P62oiHVHxb11R9GqrWLVb90Va1HQWrVaN9SKtq5orVJXrFVxKSCyU21FtrBYCJAACWTPJDm/P547MDOZTG5mbjKTyXm/XveVPM997rnnTiZz5lk+zxFVxTAMwzCCpFO6HTAMwzCyDwsuhmEYRuBYcDEMwzACx4KLYRiGETgWXAzDMIzAseBiGIZhBI4FFyMKERknIhpxFIrI30Vk74g2z4vIvHT66RfvGa4JwE6eZ+vUFl53tHfdD5ppd66IXJKSk1mAiFzivV49WnjdOBHZ6KPdLSJydNIOGr6x4GLEYwtwmHfcBBwAfCwi3dPqVXpZh3s9ZrSS/XOBS1rJdnviPdzrXNlK9m8Bjm4l20YEXdLtgJGR1KnqLO/3WSLyLTAdOBl4PX1uxUdEuqpqVWveQ1VrgFnNNjRSQlWLgeJ0+2GkjvVcDD/M937mxTspInuIyLMiskJEqkRkqYj8TkR2iGgzR0Sej3Pt8yKyMKK8i4g8LSIbRKRaRGaKyKEx16iI3CAij4hIMfCfZvzvLCK/F5FiESkSkSdEZMcYmwNE5FUR2SwilSLygYh8P+J8o2ExEdlRRP4kIqUisklEHhCR60Qk3rYXfUXkdREp916nqyJfA+As4KiI4chx3rmfiMh0EdnqHf8WkXOaelARWSkiD8Spf11EZni/54jIgyLyrYjUeEOfb0b+vZpDRMaLyNKIcncRCYnIgoi6viLSICLHR9QdISL/8l7jTSLyjIj0jDjfaFjM+9u87723VnptJovIZ3H8OlBEZnn2F4rIERHnVgF9gDsjXuejvXO/EJFF3j02ej7u7/f1MBpjwcXwQ573c30T5/sCm4EbgBHAA8ClwISINn8Bzo750OgBnA0865V3BKYCw4GbgZ/ivsVOFZHdY+55M7AHcBHw62b8vxHoD4zyfBsNXBvhxy644a7vA1fihqi6e/ftmsDuH3BDWeOBC4EB3r3i8QzwJfAz4DPgCRE5xDt3N/ApsJDtw5F/FpGdgSnAClzwORt4CchN4NNrQFTw8V7nU4BXvarbPH/HAscD1+GGQjsnsBvLdGBfEdnNKw8D6oD/5/kNcATQAHzh+XE47u+73nuW63C94eeauomICPAOsB9wGe499mvg0DjNuwEvABNxr1cN8IaIdPPO/8x7zr+w/XVeICJHAk/hXtuTvPvMBHr5fjWMxqiqHXZsO4BxwEbckGkX4Hu4D76twB5em+eBeQlsdAEuAKqBHby6nYEK4NKIdpfhPgD6eOVfALXAvjG2CoAHIuoUWODzeRSYFlP3FjAronw3sAnYJaKuN+6D6GqvnOfZOtUr9wGqgJsjrhHga/dvta3uaO+6uyLqcnBB876IusnAZzF+DvGu7dmCv9+B3jVDI+pG4j74d/PKU4A/pvg+6Q6EgLO98l3A34FCYIRX9xAwN+Ka6cCnMXaO9fz9gVe+xCv38MqneOWDI67Z07v3ZxF147x2x0bUHeDVjYio2wiMi/HhJmB+uv/3su2wnosRjz64f94QsAQYBJynquviNRbHdeFhBe+6l4Edcd/mUdWtuA/QSyIuvQR4R1U3eeXhuCG4lSLSRUTCc4L/wn3QRvKPFjzPhzHlRcB3IsrDgY+ArRH3LfN8ib1vmB8CO+G+VQNeRIF3m/NBVUPAshgf4lEAlAOviMgZIpKoxxK2vRBYCpwXUX0e8C9V3eCV/w1cIm7l1I+83kGLUNUKYAGudwJwJDANF0Ai66YDeL2Hw4DXwq+x9zrPwL1fDmriVgcD61V1bsS917J9qDaSWlyvMMwi72dzr/O/gQNF5GERObIlw4NG01hwMeKxBfdPPQT3j5mnqu8naH8d8CDwJnAGcAhwtXdup4h2fwGOEJFB4pY2H4E3JObRFxjK9sAWPi4Fvhtzzw34pzSmXBvjV1/cB3DsfY+Jc98w4WG62Mnnpiajm/OhEapaghu2ysENdxWLyHsiMijRdcDfgHO8oL8zbqjy1YjzvwOeAK7CDdX9T0SubWymWabj/p474IappkfU9cT1HKZ7bXvjht2eJPo1rvGeL9HrHO81jVdXpqoN4YKq1nq/Nvc6T8W9x47EBaeN4ublOvLqyJSx1WJGPOpUtSU6lnOAyap6e7hCRAbHNlLVaSKyDNdjEdwQSmSvYjMwD/hVnHvUxJprgX/NsRnXA7k7zrmyJq4Jzz/t6l1PRDkw1K3aG+HN/QzHDTW9ggvCTfE33HzKT4C9cF8i34iwWQ3cAdwhIvvi5pkeEZElqvrPFrg3HbgeOA4XLP8N1OO+aByDCybhpduluL/ZOOL3OgubuMd64r+mu+KGXQNBVV8AXhCRXYEzgYdxf/sxQd2jo2HBxQiCrjT+8L+wibbP4r4xA7yoqvUR5z4GTgC+VdWiYF1MyMe4Sfyv1f+S5v/gPtzOwE3shyefT0vSh4Q9Gc+vd8WJMW9LZEhVvxaR/+J6Y3sBUyOGHmPbLhORm3A9zcFAS4OL4D6AP1fVBhH5D24u6kZgsbqlxahqhYjMAr6vqne14B5zcau7DlHVOQAisiduGO3zFtgJ09zrXAxMFJEzca+HkSQWXIwg+Aj4tYjMxs0TXAjs00TbF3DDMl1ovEroRdy36M9E5EHcKqk+uGG29ar6cCv4Dq43MAr4REQmAGuB3YCjgBmq+tfYC1R1k4g8A4wXkRDwDW5oZWeS61UtBs4QkZ8Ca3Df5A/ELXp4C/gWN5E9GvjEh72/4VbE9QIujzwhIm/i5iwW4gLB2bi/xzTv/EDc3/EyVX2xqRuo6mYRWYQbTrrNq2sQkc9xE/HPxFxyC06M24CbfyvDzcmdAtyuqktpzD9wQ3evichtnr934oZFG+K0b47FwCki8k/cfNYS3IT+LnhDYrjX/Sis15ISFlyMILgLN0zxO6/8Bm65aKPJbVVd7wUhYj9MVLVaRI7x7I3HfcAXAXOImDgPGlXdKCJDgXtwwyG5OEX+DOCrBJfegpsvGIf7oHsJN690XRJuPIn7UHsWNz8xHvgrLlD9HuiHm2eYAvzGh71XccN8NbjgFMlMXK/mZtyQ2SLgrIihUMENafmZk50O7I8XmCLqTiFmNwNVneEt+x2Pe606A6txvaW4c2iqqiJyBm558XNeu3twATEZFf/NuPmm93BLl4/B9Y6uB84Heno+jQMeTcK+4SFugYthtA2epmQtcI2q/iXd/gSNiEwFclT1qHT7kq2ISC9cr/ZxVb0z3f4Y8bGei9EmeKuHBuOGaspw38rbNV4v61DcktwcXG/gOGJEjEZqiMiVuJ7hMlwP+QbcMvdnE11npBcLLkZbcRBOjLkauFhVW2tjwrakHLeLwG24SeJlwCWqOjmtXmUf1cCtwEDcMOEcYLiqrk6rV0ZCbFjMMAzDCBwTURqGYRiBY8HFMAzDCJwOO+fSt29fzcvLS7cbhmEY7Yr58+dvVNVmd6LosMElLy+PefPaRaZewzCMjEFEfC2ksGExwzAMI3AsuBiGYRiBY8HFMAzDCBwLLoZhGEbgWHAxDMMwAseCi2EYhhE4FlwMwzCMwGnz4CIiZ4vITBHZJCLVIrJERH7r5eEOtxER+Y2I/E9EqkRkmogcEMfWYBH5WEQqRaRQRO4Skc5t+0SGYRhGLOkQUfbBZdJ7AJdX+xBcYp7dgWu8NmNwOcBvxmWOuwGYKiI/UNX1ACLSG5iKS3R0BrA38EdcwPxtGz2LYRiGEYc2Dy6qOjGm6lMR2Rm4WkT+D5enYQxwr6o+DiAiXwCrcMEnHDiuxOVuP1NVtwIfeXbGicgfvDrDMAwjDWTKnMsmIDwsNgyXh/y18ElVrcClzD0p4pqTgA9igsiruIBjWQANo51RWlrKDTfcQGlpabpdMQIgbcFFRDqLSDcR+Qku3/qf1CWXyQfqcYmXIvnGOxcmHzdktg1V/RaXVzuynWEYGU5paSm9e/fm4Ycfpnfv3hZgsoB09lwqvGM68C/c/ApAb6BcVetj2pcA3SIm/nvj5mxiKfHONUJErhCReSIyr7i4OFX/DcMIiLvuuith2Wh/pDO4DAOOAG7ETcg/3to3VNWnVXWIqg7Zdddmd4w2DKONuOOOOxKWjfZH2oKLqi5Q1Rmq+hBuWOxXIrI3rufRI86S4t5AparWeuUSoFcc0729c4ZhtBNyc3MpKSnh+uuvp6SkhNzc3HS7ZKRIpuRzWeD93As3j9IZ2AdYEtEmdo5lMTFzKyLyXaBbTDvDMNoBubm5PPTQQ+l2wwiITFktdrj3cyUwE9gKnBM+KSLdgNOA9yOueR84UUR6RtSdB1Th5nAMwzCMNNHmPRcR+SdO/Pg1blXY4bh5l7+paoHX5j5grIiUsF1E2QmYEGHqKdxw2hsicj8wCCfGfMg0LoZhGOklHcNic4FLgDygDlgB3IYLFmHuwwWT23CK/nnA8aq6IdxAVUtE5DjcQoB3cSvHHsYFGMMwDCONiJOWdDyGDBmi8+bNS7cbhmEY7QoRma+qQ5prlylzLoaR9agq5eXldNQvdEbHwoKLYbQBqsrEiRO56qqrmDhxogUYI+ux4GIYbUBFRQUzZ85k4MCBzJw5k4qKinS7ZBitigUXw2gDunfvzrBhw1i9ejXDhg2je/fu6XbJMFoVm9A3jDZCVamoqKB79+6ISLrdMYyk8DuhnykKfcPIekSEHj16pNsNw2gTbFjMMAzDCBwLLoZhGEbgWHAxDMMwAseCi2EYhhE4FlwMwzA6CKWlpdxwww1tkkbagothGEYHoLS0lN69e/Pwww/Tu3fvVg8wFlwMwzA6AHfddVfCctCYiNIwDKMDEO65hEk2nbTtimwYhmFsIzc3l5KSEq6//vqkA0tLMIW+YRhGByE3N5eHHnqoTe5lPRfDMAwjcCy4GIZhGIFjwcUwDMMIHAsuhtFBybS0y+Xl5Tz44IOUl5en2xUjACy4GEYHJNPSLpeXl9OzZ09uvvlmevbsaQEmC7DgYhgdkExLu/zUU08lLBvtDwsuhtEBybS0y1deeWXCstH+MIW+YXRQMi3tcnl5OU899RRXXnmlZezMYCzNsWEYCcm0tMs9evTgpptuSrcbRkC06bCYiJwjIu+IyFoRKReR+SIyMqbNziLyiIisEpFKEflGRK6TmK9WIrKniLwpImUislFEHheRbm35PIZhGEZ82rrncgOwErge2AicDLwiIn1VdYLX5nngSOA3wHLgGOAhQICHAUQkB/gAqAXOB3K9NrnAqDZ6FsMwDKMJ2jq4nKaqGyPKn4hIf1zQmeD1PM4ArlPVpyPa7I8LIg97dWcD+wH7qOpKABEJAa+KyHhVXdYWD2MYhmHEp02HxWICS5iFQH/v9844n7bEtCnF9VzCnATMDQcWj7dwPZkRwXhrGIZhJEsmLEU+DFgKoKplwGvALSJygIj0FJFTgXOBJyKuyQcWRxpR1VqgwDtnGEYzNDQ0sH79ehoaGtLtCpB5OwYYqZHW4CIixwE/Bf4YUX0xLnAsBLYC7wB3q+oLEW1643ozsZR45wzDSEBDQwOjRo3i8MMPZ9SoUWkPMJm2Y4CROmkLLiKSB7wCvK2qz0ecehg4FLgUOAr4LTBORH4RwD2vEJF5IjKvuLg4VXOG0W4pKipi9uzZ7LHHHsyePZuioqK0+pNpOwYYqZOW4CIiuwDvA6uBCyPqfwT8CrhMVZ9X1Wmq+nvgEeBBEQn7WwL0imO6t3cuLqr6tKoOUdUhu+66a0BPYxjtj379+nHooYeybt06Dj30UPr165dWfzJtxwAjddpcROmtCJsC7ACcqqqVEafD8yX/jrlsIW6ZcR+gGDdsFjW3IiI7AIMA25TIMJqhU6dOTJo0iaKiIvr160enTumdfhURRo8ezahRozJmxwAjNdpaRNkFeB3YFxihqrF98dXezx/H1B8EVOC0MeB6PQeLyMCINqcDOwL/DNRpw8hSOnXqxO677572wBImvGOABZbsoK17Lk/ihJPXAn1EpE/EuYXAPO94VkTuwAkufwJcBzyq22f5JgO3A2+IyFjcENnDwCumcTEMw0g/bR1cTvB+Phrn3F6qukpETgN+B9wB7IrrzYwjYkWZqoZEZATwOG7pcg3wKnBz67luGIZh+KVNg4uq5vlosx74pY92a3DLmA3DMIwMIzMGWw0jBhPUtT6ZJqI0sgsLLkbGYYK61ifTRJRG9mHBxcg4TFDX+mSaiNLIPiy4GBmHCepan0wTURrZh6U5NjKSTEvBm400NDRkjIjSaD9YmmOjXZNpKXizkbCI0jBaA/u6YhiGYQSOBRfDMAwjcCy4GIZhGIFjwcUwDMMIHAsuhtFGBLXrQFB2glLoZ9pzGZmBBRfDaAOC2nUgKDtBKfQz7bmMzMGCi2G0AUHtOhCUnaAU+pn2XEbmYMHFMNqAoHYdCMpOUAr9THsuI3Mwhb5htBFB7ToQlJ2gFPqZ9lxG62IKfcPIMILadSAoO0Ep9DPtuYzMwIbFDMMwjMCx4GIYhmEEjgUXwzAMI3AsuBhGB8XEj0ZrYsHFMDogJn40WhsLLobRATHxo9HaWHAxjA6IiR+N1sZElIbRQTHxo5EMJqI0DCMhJn40WhMbFjMMwzACp02Di4icIyLviMhaESkXkfkiMjJOu4Ei8lcR2SwilSLypYiMiGmzp4i8KSJlIrJRRB4XkW5t9zSGYRhGU7T1sNgNwErgemAjcDLwioj0VdUJACLyXeAL4EvgUqACOADoGjYiIjnAB0AtcD6QCzzk/RzVVg9jGIZhxKetg8tpqroxovyJiPTHBZ0JXt0DQAFwiqqGMxhNjbFzNrAfsI+qrgQQkRDwqoiMV9VlrfYEhmEYRrO06bBYTGAJsxDoDyAivYAzgScjAks8TgLmhgOLx1u4nsyI+JcYHZFsVI8HlZ44W7HXJzPIhAn9w4Cl3u8/BnIAFZHPRSQkImtE5DaJXuOYDyyONKKqtbgeT35bOG1kPtmoHg8qPXG2Yq9PM/z97zB+PJSUtPqt0hpcROQ44KfAH72qcHKJicB04ATgWeB3wK8iLu0NlMYxWeKda+p+V4jIPBGZV1xcnKL3RqaTjerxoNITZyv2+jRBYSGIwNlnw7hx8M47rX7LtAUXEckDXgHeVtXnw9Xez/dVdYyqfqqqdwAvALelek9VfVpVh6jqkF133TVVc0aGk43q8aDSE2cr9vrEoApnnAF77hldf955rX7rtCj0RWQX4HOgDDhaVSu9+pOAfwBXqurEiPajgJeAXqq6VUTmAF+r6qUxdr8GPlPVq5vzwRT6HYNsVI8HlZ44W7HXx+O11xoHkaeegtGjUzKbsQp9T4syBdgBODUcWDy+CTeLvcz7GR5AXUzM3IqI7AAMAp4K1GGjXZON6vGg0hNnKx3+9Vm7Fr7znei6YcPgX/+CLm33kd/WIsouwOvAvsAIVY0aEFXVVcDXwLExlx4HFKhquVd+HzhYRAZGtDkd2BH4Zyu4bhiGkdk0NMAppzQOLMuXw+eft2lggbbvuTyJE05eC/QRkT4R5xaqag0wFvi7iDwAfAgcDVwEXBzRdjJwO/CGiIwFegEPA6+YxsUwjA7Hq6/CyJjNTv78Z/jFL9LjD20fXE7wfj4a59xewCpVfVNELsYFj2uBb4GrVfXlcENVDXnbwTwOvAbUAK8CN7em84ZhGBnF//4HAwZE1x1xBHz6KXTunB6fPNo0uKhqns92k4BJzbRZg1vGbBiG0bEID4H9M2YWoKAABg1Kj08xdOClFIbhnyCU/pm2W4Ap2dsX294/kya5XklkYPnLX9yy4wwJLGDBxTCaJQilf6btFmBK9vaFqvLyvffSo2dP5KKLtp84+mioq4PLLkubb01hwcUwmiEIpX+m7RZgSvZ2REMD9ccdx6jbb4+uX7kyI+ZWmsKCi2E0QxBK/0zbLcCU7O2El16Czp3p8umn26o+ufhitKEB8vLS55cP0qLQzwRMoW+0hCCU/pm2W4Ap2TOY1asbBQ897jgq3niD7j17pvX9k7EKfcNojwSh9M+03QI6vJI9E6mvhxNOgE8+ia5ftQoZOJDMefc0j31dMQzDyAReeMGp6CMDy4svulVgAwc2fV2GYj0XwzCMdLJqFey1V3TdCSfA++9DOx6utOBiGIaRDurr4dhjYdq06PrVqxur7tsh7TcsGoYPghIKBmGnvr6egoIC6uvrU/IlFAoxa9YsQqFQSnaC8icoO0ERlFi1VUWvzz3nhsAiA8ukSW4ILAsCC1hwMbKYoISCQdipr69n6NChHHDAAQwdOjTpD+JQKMRuu+3GsGHD2G233ZIOMEH5E5SdoAhKrNpqotcVK1xGyEjR40knuV7MhRcGc48MwYKLkbUEJRQMws6qVatYvHgxubm5LF68mFWrViXly/z58yktLaVz586UlpYyf/78pOwE5U9QdoIiKLFq4KLX+nq3oeTee0fX/+9/8I9/tOu5labIvicyDI+ghIJB2MnLyyM/P5/S0lLy8/PJS1IAd9BBB5Gbm0t9fT25ubkcdNBBSdkJyp+g7ARFUGLVQEWvf/6zGwKbMWN73V//6obAYnOvZBEmojSymqCEgkHYqa+vZ9WqVeTl5dE5hS07QqEQ8+fP56CDDiInJydpO0H5E5SdoAhKrJqynYIC2Gef6LpTT4W3327XPRW/IkoLLoZhGEFSVwdHHglffBFdv2YN7LlnenwKEL/Bpf2GT8MwjEzj6achJyc6sPztb24ILAsCS0swnYthGEaqLF8O++4bXXfGGfDmm251WAfEgothGEay1NXB4YfDnDnR9WvXQv/+6fEpQ7BhMcMwjGR46ik3BBYZWF5/3Q2BdfDAAhZcjCynrq6Or776irq6unS7QklJCddccw0lJSUp2ck0hX5QSvZ2k3Z56VI31PWrX22v+9nPXF77s89On18ZhgUXI2upq6tj77335qCDDmLvvfdOa4ApKSlhl1124YknnmCXXXZJOsBkmkI/KCV7u0i7HArBkCHw/e9H1xcWwhtvdNi5laaw4GJkLYsWLaKwsJCddtqJwsJCFi1alDZfxo4dm7Dsl0xT6AelZM/4tMtPPAE77ACRr/ff/+6GwPbYI31+ZTAWXIysZfDgwfTv35/q6mr69+/P4MGD0+bL3XffnbDsl0xT6AelZM/YtMtLlrgeyTXXbK87+2w3BHbmmenzqx1gIkojq6mrq2PRokUMHjyYLl3SuziypKSEsWPHcvfdd9O7d++k7WSaQj8oRXxGpV0OheCQQ+Df/46uX7cOOnj2zsAV+iLSCxgC7A7sBGwGlqrq16k4mi4suBiGEZcJE+DXv46ue/NN+OlP0+NPhhGIQl9EdhaRq0VkNrAJ+Ah4CXgG+DvwlYhsFpG/iMhhPpw6R0TeEZG1IlIuIvNFZGSC9teKiIrI5Djn9hSRN0WkTEQ2isjjItKtOR8MwzDi8s03bggsMrCcd54bArPA0mKaHCcQkd8CN+CCyt+Be4AvgY1ADZAL5OF6MyOAT0TkC+DXqvrfJszeAKwErvfsnAy8IiJ9VXVCzP37AeOA4ji+5QAfALXA+Z4vD3k/RzX/2IZhGB61tW4V2H/+E12/fj3stlt6fMoCEg1CHwqcrqozmji/0TvmAU95w2ZXAocDTQWX01R1Y0T5ExHpjws6E2La3gtMAb4bx87ZwH7APqq6EkBEQsCrIjJeVZcleC7DMAzHI4/A9ddH1739Npx+enr8ySKaHBZT1dMSBJZ47beo6v2qOjFBm41xqhcCUXJWETkEOBcY04Spk4C54cDi8RauJzPCr8/Gdlo1pWsayaSUt7W1tXzyySfU1tam5Et1dTWTJ0+muro6JTtLlixh4MCBLFmyJCU7NTU1vPfee9TU1KRkp01FnYsWuSGwyMBywQVuCCzgwJKt/1vNkQl7ix0GLA0XxC03mQD8QVXXNrH6JB+IEi2oaq2IFHjnjBYQFsLNnDmTYcOGMXr06JRW/WQKQT1XEHZqa2vJzc2lqqqKrl27Ulpayg477NBiX6qrq+nevTsNDQ106tSJiooKdtpppxbbWbJkCfn57l8lPz+fxYsX8/1YcaAPampq6NmzJ6FQiJycHMrKythxxx1bbCcs6ly8eDH5+fnMmjUrqRVszf6tamvhwANdcImkqAh23bXF90vZnyzG93o/EfmhiLwiIstFpML7+bKI/CjZm4vIccBPgT9GVF8K7AY8mODS3kBpnPoS71xT97tCROaJyLzi4kZTOR2WwFO6ZgiZlPJ2xowZVFVVAVBVVcWMGb4HBaKYMmXKNvV6Q0MDU6ZMScrOCSeckLDsl6lTp27bJSAUCjF16tSk7LSJqPOhh2DHHaMDy7vvOiFkKwSWZv3JcnwFFxH5KTAfOBCYDIz1fv4YmOedbxEikge8Arytqs97db1wcy23qGpVS202h6o+rapDVHXIrq30ZmqPBJrSNYPIpJS3P/nJT+jatSsAXbt25Sc/+UlSvpx66qnbNCCdOnXi1FNPTcrOhx9+mLDsl+HDh2/T2uTk5DB8+PCk7LSqqPO//3VDYDfeuL3hRRe5IbAkX7+U/OkoqGqzB7AEeB1PFxNRL7ggs8SPnYjrdgG+AeYA3SLq7wfm4lZ9hY8ZwNve7529dnOA5+LY/Rp4wo8PBx10kBrbaWho0LKyMm1oaEi3K4ES1HMFYaempkY//vhjrampScmXqqoqff3117WqqiolO4sXL9YBAwbo4sWLU7JTXV2tU6ZM0erq6pTs1NXV6fLly7Wuri4lO9v+VlVVqvn5qq5vsv0oLk7JftL+ZMn/FjBPfXzG+hJRikgl8DNV/SDOuROBN1XVl8bE06JMxQ19HaaqRRHn3gLOSHD5Eao6Q0ReBPZV1W3aGhHZAdiC6/XErjxrhIkoDSOLeeABuOWW6Lr33oOTT06PP1mEXxGl3wn9ecD+OG1JLD8AFvh0qgsTIE9kAAAgAElEQVSuB7QvMCwysHj8Fngkpu4RXNC4EwgvRH8fuEBEBqrqaq/udGBH4J9+fDEMIwv5z3/gRzHTwD//OTz3nO1a3MYkElFG9kRuwGlIcnBLfouAfsDPgF/ihIx+eBInnLwW6CMifSLOLdQ44ksRKQU2qupnEdWTgduBN0RkLNALeBh4RU3jYhgdj5oa+OEPYVnMv//GjdCnT/xrjFYlUc+lHIgcMxPcZPvvY+oAZgN+1g2Gl6Q8GufcXsAqHzZQ1ZCIjAAeB17D7RjwKnCzn+sNw8gi7r8fxsRI4t5/H0aY5C2dJAoulxEdXFJGVfOSuOboJurX4JYxG4bREfnqK/h//y+67rLL4M9/tiGwDKDJ4KLe8mDDMILZur+mpoapU6cyfPjwpISGYSorK3n55Ze58MIL6dYt+b1ag9pyf/PmzYwZM4b77ruPXXbZJWk7vrfcr66G/feHFSui6zdtgl12cXY2bMiMrfs7MPbKG0YzBJEuOaxkP/XUU+nZs2fSW6VUVlbSvXt3rrjiCrp3705lZWVSdoJKc7x582b69OnDM888Q58+fdi8eXNSdnynOf7976Fr1+jA8sEHbpGxF1gyPl1yB6HJ4CIic0Vkjt+jLZ02jLYkiHTJQSnZX3755YRlvwSliB8TM9cRW/ZLs2mO//1vN9R1++3b6y6/3AWViN0FMj5dcgeiSZ2LiDxPC+ZcVPXSgHxqE0znYvgl3HMpLCykf//+FBQUtHhoLKg9uMI9lzAVFRVJDY0FtZdXuOcSZtOmTUkNjYV7HLNnz+bQQw9l0qRJbkiruhry82H16ugLvCEw33aMwPCrc/Gtqs+2wxT6RksIhUL65ZdfaigUStpGUEr2iooKffrpp7WioiIlO0Ep4jdt2qSXX365btq0KSU79fX1um7dOq2vr3cVd9/dWF3/0Uctt2MECkEq9LMR67kYRoayYAEcdFB03ejR8NRT6fHHiCJlhb6I/AW4T32KEj2B5UVASFVf8u2pYRgGQFUVfO97sGbN9rrOnZ0QMjc3fX4ZSZFoMLIC+FJEpnu57A8Wka6RDURkoIicKSLPAIU45X1qmYcMw+h4jB8P3bpFB5aPP4a6Ogss7ZREOpdfi8gDuNTFN+K2V1ERqcYp4nfGKfTrcXuOXY7bPr9jjrMZhtFy5s2Dgw+OrrvqKnjiifT4YwRGwmUUqvo/Vb1dVQfgMjyOwm0u+QBwNXAckKsuJfJbFliMoGhoaGD9+vUp6xRCoRCzZs3atgw4WYJIUbxw4UJyc3NZuHBhSr5UVlbyzDPPJK1xCbN+/XpOPvlk1q9fn5Kduro6vvrqq5bpfyoroX//qMCiOTnM/fBDQo/E7l2bHlQ7ZnriwPAz65+Nh60Wy1zq6+t15MiROmjQIB05cmTSq35qa2u1d+/eKiLau3dvra2tTcpOTU2Ndu3aVQHt2rVrUvlYFixYoLil/QroggULkvKloqIiyk6yK8bWrVsXZWfdunVJ2QmFQjpgwADt0qWLDhgwwN9qujvuaLQKLPTRR4H8rYKioaFB//SnP+lFF12kf/rTn7ImF0sQ4HO1mC0ANzKOoIRw8+fPp7S0lM6dO1NaWsr8+fOTshNEiuJjjjkmYdkvQYkoL7vssoRlv7RIYDp3rhNC3nXX9rr/+z9QZV6PHoH8rYKiI6cnDgw/ESgbD+u5ZC7Wc2madtlzKS9X3W236N7KTjuplpZuaxLU3yoorOfSNPjsuaT9Qz5dhwWXzCYoIVxtba1+8cUXKX9YBZGieMGCBdqrV6+kA0uYoESU69at05NOOinpwBImocD09ts1dghM//WvuHaC+lsFRbalJw4Kv8HFRJSGYQTP7NkwdGh03bXXQoZM1hvJE2iaYxEZAKxT1UZLbrzUxf1V9duWu2kYRlZRUQEDB7q9v8J07w6FhbDzzunzy2hz/E7orwQObOLc//POG4bRkbntNujRIzqwTJsG5eUWWDogfrd2TZTWbSecqNIwjI7IF1/AsGHRdTfeCA8+mB5/jIwg0d5iPwIOiKg6WUTyY5rtBJwLLG0F3wzDyGTKy2HAACgp2V63885uC5eePdPnl5ERJBoW+xnwvHcocEdEOXw8BewF3NRaDhptQ1Iq6zioBqNqDspOUGr2iooKJkyYkJLeYcuWLYwZM4YtW7ak5MuKFSvYb7/9WBGb5reFFBYWcuyxx1JYWNjyi2+5xQWQiMBSOmUKbNmSdGCpr6+noKAg6ayYYYJ672Qa7e65mlpGBuQA3YEeQANwtFeOPHL8LEnLxMOWIm8nKZV1HILSBgRlJyhNSHl5eZSd8vLyFtsoLS2NslEaofFoCQUFBVF2CgoKkrKzdu3aKDtr1671d+HnnzdaWnx/hJ3i4uKk/Kmrq9MhQ4Zojx49dMiQIUnnmMlWfUomPRepKvRVNaSqFaparqqdVPUzrxx5pLZhk5ERBJHGF4JTNQdlJyg1+7PPPpuw7Id77703Ydkvp5xySsKyX0aNGpWw3IiyMujVCw4/fHtd79784txzuTWi2XXXXZeUP0GlXc5WZX27fC4/ESh8AN8DjgVOjj1aYicTDuu5bMd6Lonp8D2XG29s1FvRmTNVVbW4uDjKjvVcWodMei6CVOgDg4GvcNvrN8Q56v3YyaTDgks0QaTxVQ1O1RyUnaDU7OXl5frYY48lFVjClJaW6q233pp0YAlTUFCg+fn5SQeWMGvXrtVjjjmm6cAyfXrjoDJmTKNmxcXFeuGFFyYdWMIElXY5W5X1mfJcfoOLL4W+iEwH+gG3AIuARvuOq+rqFnSY0o4p9A2jCbZuddvhRw699O0LK1c6HYvRofGr0PcrojwQuFFV31bVZaq6Ovbw6dQ5IvKOiKwVkXIRmS8iIyPO7ywi40VkjohsEZH1IvKmiHwvjq1eIvKciJR4bV8WkT4+n8cwjHhcf72bW4kMLLNmQXGxBRajRfgNLgU4TUuq3ACUA9cDpwOfAq+IyP955wfgMlp+AJwNjAb2AGaLyHdjbL2GW8H2S+AS4GDgrQB8NIyOx7Rpbjv8yL2/fvMbNxh26KHp88tot/hV6N8I/EFEFqhqKovrT1PVjRHlT0SkPy7oTMBtI7O3qlaFG3hDct8ClwHjvbrDgBOAo1R1mle3FheEhqvq1BR8NIxWQVWpqKige/fuiCTa9KIN2bIFdt8dqqu31/XrBytWuD3BDCNJEin05+JWgITZE1gsIquA0tj2qnpIczeLCSxhFgJneecbra9T1c0ishroH1F9ErAhHFi8dnNEZKV3zoKLkVGoKhMnTmTmzJkMGzaM0aNHpz/A/PrXMGFCdN3s2XBIs//KhtEsiYbFvo45/gG8DHwe59zXKfhwGAm2jxGRXYF9YtrkA4vjNP/GO2e0ENXMUtaHQiFmzZpFKJSalGrDhg2cfvrpbNiwISU7qSr9Kyoq+PDDD5k2bRoffvhhSjqF4uJizjvvPIqLi5Mz8NlnbggsMrCMHeuGwJIILGvXruXII49k7dq1yfnj0dDQwPr162loaEjJjpEh+FlS1loHcBxuKfMlCdq8CGwC+kTUfQS8FaftJGCmn3vbUuTtZJo+JaishOvXr4/SYKxfvz4pO0HoZdasWRNlY82aNUn5UlRUFGWnqKjI/8Wlpao5OVFLi9eAdgUtLCxMyp+gniuo7KNG60OqCv3WRkTygFeAt1X1+Sba/AoYBfxSVTfFa9PCe14hIvNEZF7S3/qykExT1s+fPz+QfOqXX355wrJfglD6X3TRRQnLfrnmmmsSlpvk6qshNxcieoJDgO8AVcDPf/7zpPwZOXJkwrJfioqKmD17NnvssQezZ8+mqKgoKTtGBuEnAgHPJjj+DDwEXAz08GlvF9wQ1hygWxNtTgfqgJvjnHsN+DRO/XvAe358sJ7LdrK157J8+fKob9XLly9Pyk4QPZek9/KKocU9l48/biyEHDdOCwsLo+xYz8XwCwEr9OcC63FDWOtwav11Xnk9sAQnrPwf8L1mbHUDZuKWN/dros3hQCXweBPn78JlxoytLwD+6OeZLLhEk2nK+iDyqTc0NOj48eN199131/Hjx6fkUxBK/2YV8T4pKirSc889N3Fg2bxZtVOn6KDyne+oVlZua1JYWKjHH3980oElzJo1a/SII45IOrCEqa+v13Xr1llgyXD8Bhe/Cv2TgIeBi1R1bkT9IcBLwM3Af7yewzJVPaMJO12At4FDgGGquixOm/2B6cBnwNmq2mh2z1uKPBM4QlVneHVDcEHwePWxFNkU+h0D1Qxc/tvaXHklTJwYXTd/Pvz4x+nxx8gq/Cr0/epc/gDcGRlYYNvy33HA/aq6n4jcBzyawM6TuI0urwX6xCjqFwK9gH/ihJaPAYdEfCBsVdVF3n2/EJEPgRdF5CZcD+p+YIafwGJ0HESEHh1FWT51Khx/fHTdXXe5lWCG0cb4DS774Ob94lEJ5Hm/rwZ2TGDnBO9nvAC0l2fnO17505jz/8Ip8sOch+tNPYtbUj0F+HWCexvtiA7Z40iWkhLYZZfouoED4ZtvoGvX9PhkdHj8rhZbCNwpIrtHVorIHsCdQHg5z0CgybR2qpqnqtLEsUpdzpimzh8dY6tUVS9V1VxV3VlVL9D4Ik2jnaHqBIdXXXUVEydODM+nGbGowhVXNA4sCxbAqlUWWIy04je4XIlTyK8Skc9F5C0R+Ry3XcvuwK+8dv2BZ4J302htVIMRPwaRqjbIxEhFRUWceeaZKS9t3bp1K3fccQdbt25N2sb06dPp0qUL06dPT8mXyspKPrzqKujUCZ6J+He75x4XcA480Jedmpoa3nvvPWpqalLyJyg7QRHUe9lIDV8T+gAi0hW3v9cQXEBZj5tAf04j9gJrL9iE/nbCPYVUtyapr69n6NChLF68mPz8fGbNmkXnzp3T5k9RURG77bbbtvKGDRvo169fi+1s3bqVXr16bStv2bKFnXfeuUU2pk+fzpFHHrmtPG3aNI444ogW+1K1fDld9903unLQIPj6a9jJ/96yNTU19OzZk1AoRE5ODmVlZey4Y6IR7da1ExRBvXeMpgl6y31UtUpVn/CGok7yfj7ZHgOLEU1QPYWgUtWKCKNHj+bJJ59M6cPhyiuvTFj2y4MPPpiw7IdjjjkmYblZVKFPn0aB5e9jx0JBQYsCC8DUqVO3ba0TCoWYOjW5dTBB2QmKdpkOOEtJm0LfyBy6d+/OsGHDWL16NcOGDaN7krvh5uXlkZ+fT2lpKfn5+eTl5SXtU3iVVyrfOp966qmEZb/cdNNNCct++PTTTxOWE/LEE24IbPPmbVVTAAFOGjOmxb4ADB8+nJycHABycnIYPnx4Wu0ERVDvZSMAmhLAAEXAgd7vxV65ycOPqCaTDhNRRhOU+DGoVLVBsWHDBv3Zz36mGzZsSMnOli1bdOzYsbply5akbUybNk07d+6s06ZN83fBt99GiyC9o6KoKJDUzdXV1TplyhStrq7OCDtBkSnpgLMVUhVRisidwDOqWuhpWRJOzqjq+CCCXVthcy5GxqLq9gGLXTzw0UeQ5p6BYaQsoowMFqo6LiC/DMNIxKOPwnXXRdedcQa8tT3JqpoGyGgH+BVRAiAivYEfAN8F3lfVEhHZCajVONu0GIbhk9WrId4cVUUFdOu2rai2GspoJ/ia0BeRLiLyB2ANTin/Ek5RD/B3nJDSMIwEaDz9hapLJxwbWD7+2J2LCCxgq6GM9oPf1WL3AJcD1wCDcAtVwrwNnBawX4aRVYR7HFG7Djz8sFsFFpnd8qyzXFA59ti4dmw1lNFe8BtcLgbGqOpzuG31IynABRyjHVNXV8dXX31FXV1dSnbifjtPgkxJTxxmxYoV7LfffqxYsSKp6ysqKnjnnXd48803mfXqq0inTnDDDbHOwuTJCe2ICBdccAF5eXlccMEFKQ2JBaWsr66uZvLkyVRXV6dkJ6j3TlB2jBTxs6QMt2nlcO/3zrhdiH/slU8CyvzYyaTDliJvJxQK6YABA7RLly46YMAADYVCSdkJKllYJqUnVlUtKCiIslNQUNBiG19//bUCWh1nabF++qlvO1u3bo3yZevWrS32RdUtH87JyVFAc3Jykl5GXFVVpZ06dVJAO3XqpFVVVUnZybSEdUbTEHCa4/8CcXO0eMFlQUsCmpFZLFq0iMLCQnbaaScKCwtZtGhRUnYqKiqYMWMGffr0YcaMGUnPB2RSemKAU045JWHZD5MPOwwlZsvw885z4eXoo33beeyxxxKW/RKUsn7KlCk0NLi1PA0NDUyZMiUpO5mWatsIAD8RCBdY6nApjU8E6nH7jN0N1AAn+rGTSYf1XLYTVM+lrq5OhwwZoj169NAhQ4YkLaRct25d1LfzdevWJWUnI3ouBQWNeyqg3yxcmJQv1nNpGztG0xBkmmNnj3OBVbghsfDxP+BcvzYy6bDgEk0oFNIvv/wy6cCi6oLCoEGDdOjQoTpo0KCkg0JZWZmOGDFCBw0apCNGjNCysrKkfQoiPbGqCzD5+fn+A0tDg6pIo6Dy0z59dNGiRSn5snXrVv3d736XdGAJE5SyvqqqSl9//fWkA0uYTEu1bcTHb3DxvStyGBH5HtAX2Aws0ZYayBBMoR88DQ0NjBo1itmzZ3PooYcyadIkOnVq+fZ1qu1cy3HffXDbbdF1F1wASQ7LGUYm4Vehn2j7l6HAfFUNBe1cJmDBpXVoaGigqKiIfv36JRVYwqi2QxV6QQHss0/j+upqSOM29IYRJClv/wLMBKpFZD7wuXfMVNVNAfloZCGdOnVi9913b75hM4R3RW4XqDq9SiwzZsDhh7e9P4aRAST6ankicB9QAYzGiSWLRGSxiPxFRH4hIvlt4aRhZCz33NM4sFx8sQs4FliMDkyTwUVVP1LVu1R1BLAL8CPgKmAWcAQunfHXIrJRRN5pE2+NViMoIVwQaY4BqqqqeOmll6iqSi0X3aRJkxARJk2alJKdb7/9loMPPphvv/3WVSxbBiLw299GN6yuhhdeiGtj7ty59OjRg7lz56bkS1AC04aGBtavX79tKXGylJSUcM0111BSUpKSHdVgxI9BvXeC+p/osPiZ9Y93AEfjchbVA/XJ2knXYavFthPUctKgliJXVlZGLbetrKxMys5LL70UZeell15Kys7q1au32ZB4IkhQnTkzoY05c+ZE+TJnzpykfAlKYFpfX68jR47UQYMG6ciRI7W+vj4pO5s3b47yZ/PmzUnZCWoJcVDvnaD+J7IRghRRikh3ETlWRH4rIv8Qkc3AR7jdkZ/BaV6MdkpQQrig0hxPjtkCJbbsl4suuihh2S9nnXUWAGNx6++juOwyF14OOyyhjZTTHHsEJTAtKipi9uzZ7LHHHsyePZuioqKk7IwdOzZh2S9BiR+Deu8E9T/RoWkq6gAXAI/j1PchYCOup/Jb4Digp5/olamH9Vy2Yz2XxKz95JP4vZWaGt82rOeSGOu5tB9IVUSJ+5JWBjwB7O/HWHs6LLhEE5QQLqg0x5WVlfriiy8m/eEQJhxgkgos9fXxg8qsWUn5MmfOHO3evXvSgSXM+vXr9bTTTks6sISpr6/XdevWJR1YwmzevFmvvvrqpANLmKDEj0G9d4L6n8g2/AaXRDqX+4HDgCHet4D5wBfeMVNVk+tHZwimczEScuedcNdd0XWXXw5PP50efwwjQ/Crc0m0WuxWVT0S2Bk3eT8ZGAg8BqwXkQIRmSQiV4vIj306dY6IvCMia0WkXETmi8jIOO0uF5FlIlLttTkuTps9ReRNESnzVqw9LiLdYtsZRov45hu3Ciw2sNTUpBxYVG0reKPj0GyaY1WtA+Z6x2PgPtiBYcDPgUf92gJuAFYC1+PmcE4GXhGRvqo6wbM9EngKGAfMAC4FpojIwar6X69NDvABUAucD+QCD3k/R/nwwzCiaWiAzp0b18+ZAwcfnLJ51Xa+pY1htBDf+3OIyI4i8hMRuQU3D/M4Ljh0wqU/9sNpqnqBqr6mqp+o6k3AX3FBJ8w44AVVvVtVPwUuAZYDYyLanA3sB5ylqu+p6svA/wEXiMi+fp/JMAC4/fbGgeXKK90MSwCBBWwreKPj0WRvQ0T643on4eMAYAfc1vv/Bl7BbREzU1XX+rmZqm6MU70QOMu75yDge8C1Edc0iMjrkXW4HDJzVXVlRN1buJ7MCGCZH3+MDs6iRbD//o3ra2shJyfQW4XTE4d7Lpae2Mh2EvVc1gB/w6U4LgbG4+ZeeqnqIap6vaq+7jewJOAwYKn3e3g7mcUxbb4BdhGRXSPaRbVR1VpcymXbkiYJglI1BzWvsGzZMvbee2+WLUvte8LKlSv5wQ9+wMqVEd9D6uvdvEpsYJk3z/VW4gSWtWvXcuSRR7J2bXJvdxHh/PPPp2/fvpx//vkpDYlt3ryZK664gs2bNydtA7JXoW9kCE0tI8MJI/P9LDlL9sDpZRqAS7zyhbiVabkx7YZ79d/zysuAR+LYmwG84ufethR5O0FpA4LSKixdujTKn6VLlyZlZ8WKFVF2VqxYoXrrrY2XFl9zTUI7a9asibKzZs2aFvtSWloaZaO0tDSpZ9q0aVOUnU2bNiVlJ1t1LkbrQ6oKfVV9VlVjexCBISJ5uKG1t1X1+da6T8w9rxCReSIyr7i4uC1u2S4IStUc1LzCiBEjEpb9ctppp237fX9gr0GD4P77oxuFQjBhQkI7I0eOTFj2w7333puw7JcxY8YkLPslWxX6RgbhJwIFfeA2wvwGmAN0i6g/GfcNaGBM+3O8+l298hzguTh2vwae8OOD9Vy2k809l05N7QW2YIFvO9ZzaRrruXQ8CDrNcVAH0A23EKAA6BdzbpD3Jj0xpn4ssCmi/CLwRUybHYAq4P/8+GHBJZqgVM1BqayXLl2qgwYNSjqwqKrqTTc1DirXXpuUqTVr1ugRRxyRVGAJU1paqrfeemvSgSXMpk2b9PLLL086sITJVoW+0br4DS4tTnOcCiLSBZcX5hBgmKo2mq0VkSXAdFX9pVfuhFud9pWqjvLqRgIvAXur6mqv7mzgNeD78ezGYgr9LGb1asjLa1SttbVIwKvADKOjEUQmytbgSdzQ17VAHxHpE3FuoarW4HQuk0RkFS775c+BfXEbaYaZDNwOvCEiY4FewMO4yXxbhtxRqa+H44+HTz+Nqn7i8suZXV3NkzU19LDgYhhtQlsHlxO8n4/GObcXsEpV/yoiPYBbccNhXwOnqqfOB1DVkIiMwAk5XwNqgFeBm1vTeSODef55uPTSqCp94QUmVlYy27QlhtHmtOmwWCZhw2JZwsqVMGhQdN3xx8M//wmdOqGqVFRU0L17d9tuxTACIOWNK432gWowwrMtW7YwZswYtmzZkpKd0tJSbrjhBkpLS1Oy06wwr74ejjqqcWBZvRo+/HBbXvtly5bxwx/+MGUxZhDPtXXrVu644w62bt2aki9BiRaDElEaRlz8zPpn45ENq8WCWr4Z1DLZkpKSKDslJSVJ2Wl2eeuf/9x4FVicfC1LliyJsrNkyZKk/AniubZs2RJlY8uWLUn5EtTS36CWIhsdD4JMc2xkJkEJz4IS+N0Vs019bNkvTQrzVqxw27b88pfbT554ouvFjGq8GfaJJ56YsOyXIJ7rwQcfTFj2S1CixaBElIbRJH4iUDYe1nPZTsb3XIqLVQ8/vHFv5dtvE9qxnkvTWM/FSBYyVUSZKUc2BBfV4IRnQQn8li1bpvvss48uW7YsJTthYV75I480DiqvvOLbzpIlSzQvLy/pwBKmpKREr7/++qQDpqoLMGPHjk06sIQJSrQYlIjS6Fj4DS62WswIjI0bN7LrrrtuKxcXF9O3b9/kjBUUwD77RNedfDK8++62yXrDMNoeWy1mtDnXXXddwrIv6urgsMMaB5Y1a+C999IWWOrr6ykoKKC+vj4t9zeM9oYFFyMwHnnkkYTlZpk40eVSmTVre92rr7rBsD33DMDD5Kivr2fo0KEccMABDB061AKMYfjAgosRGH379qW4uJgLL7ywZUNiy5a5VWBXXrm97rTTXF77885rHWdbwKpVq1i8eDG5ubksXryYVatWpdslw8h42nr7FyPL6du3L5MmTfLXuK4Ohg2DuXOj69euhf79g3cuSfLy8sjPz2fx4sXk5+eTF2dTTMMworGeiwEEp9b2nQ74T39yQ2CRgeX1190QWP/+gaU5Liws5Nhjj6WwsDBpG507d+axxx4jFArx2GOP0blz56TslJWVcc8991BWVpa0LxBcSupNmzZx6aWXsmnTppTsBLUrg5Fl+FlSlo1HtixFDoKgNA++kmotWdJ4afFPf6oasZQ6qGRha9eujbKzdu3apOzMnj07ys7s2bNbbGPr1q1RNrZu3ZqUL0Eldtu4cWOUnY0bNyZlJyhtk9F+wBT6hl+CUmsnTAccCsFBB8H3vx99UWEhvPmmm3PxCCrN8agY1X5s2S/HHHNMwrIfHnvssYRlvwSVkvqmm25KWPZLULsyGFmInwiUjYf1XLbT6j2Xxx9v3FuZPLlJO9ZzaRrruRjpBlPoW3BpCUGptaPSAX/zTeOgctZZUUNgTRFImmN1AeaYY45JOrCEmT17tnbr1i2pwBImKIV+UCmpN27cqJdccknSgSVMELsXGO0Hv8HFFPpG8IRCcPDB8OWX0fXr1sHuu6fHpzSjqkycOJGZXuKy0aNHW34Zo11iCn0jLeijj8IOO0QHljfecP2WDhpYILgdrA2jvWA6FyMYvvkGBg8m8ru4nnsu8uqrUZP1HZXu3bszbNiwbT0XS7lsZDs2LGakRm2tWwX23/9GVV9z9tnc99xz9OjRI02OOVQzJ81xJvliGMliw2IZTlAbIabVziOPwI47RgWW10eNYo/dd6f/gQem9O28oqKCCRMmpDR8pKo88sgjHHXUUTzyyCOk8kWqqKiIM888M6WkWmVlZfzhD39IWURZU8ckuXsAABokSURBVFPDe++9R01NTUp2VINJkR2UHSPL8DPrn41HOleL1dXV6ZAhQ7RHjx46ZMgQraura192/vvfxqvAzj9f1xUWRi1LXbduXVL+lJeXR9kpLy9Pys66desC8WfDhg1RdjZs2NBiG0ElC6uurtacnBwFNCcnR6urq5OyE1SiuaDsGO0HTESZuQS1EWKb26mthf33hx/8ILp+wwb461+57Be/iKq+7LLLkvLn2WefTVj2S+weZ773PIvhysgNNeOU/RBUmuOpU6cSCoUACIVCTJ06NSk7QS0wsIUKRpP4iUDZeFjPpYV2HnywcW/l3XejmgTVUwiq51JWVhZlp6ysLCk71nNpfTtG+wETUWZucFF1H+jLly9POiC0mZ3//KdxUBk1qkkh5Lp16/Skk05KOrCEKS8v18ceeyzpwBKmrKxMH3jggaQDS5ggxJhBiSirq6t1ypQpSQeWMEGlyA7KjtE+8BtcbLWYEZ+aGvjRj2Dp0uj6oiKISGXcEairq2PvvfemsLCQ/v37U1BQQJcutorf6JjYajEjeR54AHbaKTqwvPee67e0s8CimvpKpkWLFlFYWMhOO+1EYWEhixYtCtBDw8hO2jy4iMg+IjJRRL4SkXoR+SxOmz1E5DkRWSsi5SKyUEQujNNusIh8LCKVIlIoIneJSHLJNgz46isneLzllu11F1/sMkKefHL6/EoSVbflylVXXcXEiROTDjCDBw+mf//+VFdX079/fwYPHhywp4aRfaSjb78/cDIwC8iJPSkinYB3gD7ALcB64GxgkohUqeobXrvewFRgEXAGsDfwR1zA/G3rP0YWUVPjVoAtXx5dX1wMflMVZyAVFRV8/vnn9O/fn88//5xRo0YlJers0qULBQUFLFq0iMGDB9uQmGH4IB3DYu+q6ndV9Rzg6zjnvwcMAa5V1ZdV9WNV/RWwEIhMqH4l0BU4U1U/UtWngPHADSKycys/Q/Zw771uCCwysLz/vhsCa8eBBaBbt27U19czefJk6uvr6datW9K2unTpwo9+9CMLLIbhkzYPLqraXB7dcG9mS0x9KURtXXUS8IGqbo2oexUXcI5Kycl2RNJq7S+/dENgv/nNtqqiU08lVFMDSSbnAqitreWTTz6htrY2aRtB2amsrERV2WeffVBVKisrk7YVRBroyspKnnnmmZT8AFPWG+0EP0vKWusAJgOfxdQJbsjsM2BfYGfgEqAGODaiXREwLo7NCuDm5u6d7qXIQZCU5qGqSjUvr9Hy4kG9eqmIaO/evbW2tjYpf2pqarRr164KaNeuXbWmpiatdoLSywSRTK2ioiLKl4qKiqR8MX2KkW5orwp9z/mTcL2qpbgezNPAZar6SUTT3rjeTCwl3rlGiMgVIjJPROYVFxcH63gaaLFa+557oGtXiFTgf/ABs774gpVbt9K5c2dKS0uZP39+Uv7MmDGDqqoqAKqqqpgxY0Za7bzyyisJy34JIg30yy+/nLDsF1PWG+0GPxGotQ7i91w6Ae/i5mPOBY4G/gBUAyMi2oWA6+LYXAP8vrl7d6iey4IFjXoq+stfbhNC1tbWau/evbOu5xJUb8F6LoaxHdqDQr+J4HK69w+4b0z9X4GvIspFwJ1xbHaYYTHVZtTalZWqAwY0DiybNjVqWltbq1988UXSgSVMTU2Nfvzxx0kHhKDtVFRU6NNPP530h3mYINJAB+WLKeuNdOI3uKRVoS8ik4G+qnp0RN0tuKDRPaZtVL2ITAPWqurIiDbfBb4FTlfVdxPdO+sV+nffDXfcEV334Ydw/PHp8ccwjKygPSv0VwPdROT7MfUHAasiyu8DJ4pIz4i684Aq4F+t6mEmM3++WwUWGVhGj3Z9FgsshmG0EW2+aF9EuuFElAB7AjuLyNle+R/e8S3wlojcBRQDp+DmX66OMPUU8GvgDRG5HxgEjAMe0ujlyR2DykrYd18oLNxe16kTbNwIveOubzAMw2g10tFz6Qe87h1DgcER5X6qWgYcB/wXp7h/CzgWJ5r8U9iIqpZ47TrjFgCMBx4G7myrB8kYxo2D7t2jA8vHH0N9vQUWwzDSQpv3XFR1FdFiyHhtlgPn+LC1CBd4Oibz5sHBB0fX/epX8OST6fHHMAzDIxPnXDoEqimooysrYY89ogNLTg6UlCQdWEKhELNmzdqmm0mWLVu2MGbMGLZsid1gIT3+BJVvPogdA+rr6ykoKKC+vj4lXwyjPWAbJaUBVbdb78yZMxk2bBijR49GJGFnbjt3/P/27j1IzqrM4/j3l9kBcqWHS8JFIIiuWdStaLIkJCrLouviriKRCFWb3cVVAaOARlguJVmM1so9sAIGZC3LcgtW2KSUrMrKJRgTwyUuoMJQgE5QSCaQpJNMMiGXefaPt5t0enpm+vLOdHfm96mamrynT5/3zFudeeY97znnmZ/MBCv08MNw6qlV92fXrl1MmDCBbDZLJpOhs7OT1tZee4oOaPPmzWQyGQCuvfZastksBx98cN3688YbbzB27Fh27dpFa2srW7du5cADD6y4nZ07d5LJZOju7mbkyJFks1kOOOCAitrYs2cP06dPp729nUmTJrFq1SpaWryBt+2/fOdSB1Wtjn788WQWWGFg+cIXkllgNQQWgNWrV5PNZmteof+Nb3yj3+Oh7k9a+ebT2DGgo6OD9vZ2MpkM7e3tdBTukmC2H3Imyjqo6M5l2zY4/vhk+/u8gw6CdeugiruCUgbjzgXwnUsB37nY/qLcdS51XaFfz696r9Ava3X0lVf2Xl2/bNmg9CetFfrZbDYuu+yyyGazDdGftPLNp7FjwO7du+PFF1+M3bt319QXs3qiGVbo11NDr9B/7DGYPn3fsosvhptvrk9/KhARbNu2jdGjR5f/HMnMmka5dy5+oN9IurrguONg48a9ZaNGwdq1MK7x859FLRMVzGy/4gf6jeKKK2Ds2H0Dy89/njxzaYLAAt7G3cz2cnCpt5Urk1lg11yzt2zevOQJy/vfX79+VWH06NHMmDGDNWvWMGPGDEaPHj3wm8xsv+TgUqGuri5uuOEGurq6am0IMhmYOXNv2bhxsGUL3Hhj2c3s2LGD++67jx07dtTWnxRIYvbs2bS0tDB79uyahsTSWnC4fv16Zs2aVVWCr7T7s3HjRs477zw2Ft6dViFqWYBrNlTKeeq/P35VM1ts69at+yR82rp1a8VtRETEJZf0ngW2fHnFzXR3d8eIESMCiBEjRkR3d3d1/UnJhg0b9rk+G0rkjSnH7t27Y+rUqTFmzJiYOnVq1bOrOjs79+lPZ2dn3fqT1rVxki+rN5o1zXEjW7RoUb/HA1qxIhkCu+GGvWWXXpqEl/e9r+L+LF26lJ6eHgB6enpYunRpxW2k6fLLL+/3uFxpLTi84IIL+j0eyv6kdW38XMuahaciV6Crq4uxY/emj9m6dStjxowZ+I1btsDRRydDYXmZDLz8cvIQv0o7duxg9OjR9PT0MGLECLZt28ZBBx1UdXu12rhxI4ceeuibxxs2bOCQQw6puJ20FhyuX7+eCRMmvHnc2dnJ+PHj69KftK5NeEae1ZkXUQ7CsFhEMjR2/fXXlz8kNm9e7yGwlSurOncp3d3dce+999Z9SCxvw4YN8dnPfrbqYZ+8tBYcdnZ2xplnnln1kFia/Unr2jg9sdUTXkTZv0FfRLl8OXzgA/uWXXbZvrPCzMyajBdR1suWLXDEEZDb6BCAww6D3/8eyhlCMzPbD/iBfpq++MVkM8nCwPLLXyabTjqwmNkw4uCShkcfTWaB3XLL3rIrr0yesBTvEbafi/AaDDPzsFhtNm+GCROgMMvh+PHwu98lOe2HmfBMJjPL8Z1LtS68MJlOXBhYHnsMOjubMrD09PSwbt26N9fNVCPNNRhppTlupNTCS5YsQRJLliypqR3fHVozcHCp1K9/nQyB3Xrr3rKrrkqGwE46qX79qkFPTw9z5sxh5syZzJkzp+oAk9beYvlkYTNmzGDChAlVB5j8+pTJkyczffr0ugaYJUuWMGvWLABmzZpVdYDJ3x3OnTuXO+64wwHGGpanIldq1izI/2I46ih44YVkW/wmtm7dOmbOnMmRRx7J2rVrWbFiBUcccURVbUXUns9l1apVzJgxg5aWFvbs2cPKlSuZXsWzq5deeonJkyeTyWTIZrM89dRTnHDCCVX1qValrkU1//e6urqYO3cuxx13HGvWrOH2228vbyGvWUrKnYrsO5dKXXQRnH46PPEEvPJK0wcWgPHjxzNt2jTWrl3LtGnTqlrFnieJMWPG1PSsZcqUKWQyGfbs2UMmk2HKlClVtTNx4kQmTZpENptl0qRJTJw4seo+1Wrx4sX9HpfLO09bs/CdiwHJ0Nj69esZP348I0bU/2+OXbt2sXr1aqZMmUJra2vV7ezZs4eOjg4mTpxY95z1+aGxxYsXc+aZZ1bdThp3h2bVKvfOxcHFzMzK1rDDYpLeJukOSc9I2iNpWR/13i1pqaTNkrZKelzSlKI6J0p6SNJ2Sa9KWiCpvn+emplZXda5vBP4CLAKKDneIWkysBz4IXB2rvgvgJEFddqAB4FngTOAE4AbSQLmVwap72ZmVoZ6BJf7I+KHAJLuAw4rUWdRrt6cgrKfFtW5gCTYzIqILcDPJI0DrpZ0Xa7MzMzqYMiHxSKi30UUkk4EpgHfHKCp04EHioLIPSQB55SaOjkMeWFe/7LZLPPmzSObzVbdRiMt6DQbbPWfFtTbtNz3NklPS9ot6SVJny6qNwloLyyIiJeB7bnXrExemNe/bDZLW1sbCxcupK2traoA00gLOs2GQiMGl/zqve8B/wl8iGRI7C5JHymo1waU+l++KfdaL5LOk/SkpCdfe+21FLvc3Jw6t38LFizo97gcaaVuNmsWjRhc8hP374qI6yLikYj4PPAIcEUtDUfEnRExNSKmHn744TV3dH/hhXn9mz9/fr/H5WikBZ1mQ6ERd0XelPv+SFH5w8CXiuodXOL9bQVtWBkkcf755zNnzhwvzCshk8mwadMmFixYwPz588lkMhW30dLSwqpVqxpmQafZYGvE4PJc7nvxbzgBhZMB2il6tiLpGGAURc9ibGD5bVustEwmw0033VRTGy0tLXXb28xsqDXisNhKkjuPvyoqPw14uuD4J8CHJY0tKDsb6AYeHdQemplZv4b8zkXSKJJFlABHA+MknZU7/nFEbJe0ALhOUhZ4AvgE8AH2nWK8CLgIWCzpWuCtwNXATV7jYmZWX/UYFhsP3FtUlj8+HuiIiJsljQAuJAkYzwNnRcTy/BsiYpOk04BbgftJZo4tzNU3M7M6GvLgEhEd9H6eUqreTUC/g9wR8Sy9h8/MzKzOGvGZi1lqaY7NrD4acbaYDXP5NMfZbJZMJkNnZ2dNOV3MbOj5zsUazurVq8lms7S0tJDNZlm9enW9u2RmFXJwsYaTVppjM6sfBxdrOK2trXR2drJy5UoPiZk1KT9zsYbU2trK9OnT690NM6uS71zMzCx1Di5mZpY6BxczM0udg4s1JKddNmtuDi7WcJx22az5ObhYw3HaZbPm5+BiDcdpl82an4brkMPUqVPjySefrHc3rA8RwbZt25x22azBSFodEVMHqudFlNaQnHbZrLl5WMzMzFLn4GJmZqlzcDEzs9Q5uJiZWeocXKwheYW+WXNzcLGG4xX6Zs3PwcUajlfomzU/BxdrOF6hb9b8vELfGpJX6Js1Jq/Qt6bmFfpmzW3Ih8UkvU3SHZKekbRH0rIB6i+UFJJuKPHaiZIekrRd0quSFkhqGbTOm5lZWepx5/JO4CPAKqC1v4qSTgQ+DWwp8Vob8CDwLHAGcAJwI0nA/Eq6XTYzs0rU44H+/RFxTETMBn47QN1vArcAm0q8dgEwEpgVET+LiEXAV4F5ksal2mMzM6vIkAeXiOgpp56ks4BJwDV9VDkdeCAiCu9q7iEJOKfU1EkzM6tJQ05FljSSZIjr8ojoa5HDJKC9sCAiXga2514zM7M6acjgAlwBrAW+30+dNiBbonxT7jUzM6uThpuKLOl44BLg1Eh5EY6k84DzAI499tg0mzYzswKNeOdyDfAT4HlJGUkZkn4emDvOr6jbBBxc4v1tlJ4AQETcGRFTI2Lq4YcfPhh9NzMzGjO4vAOYRRIg8l/HAF/I/fvoXL12ip6tSDoGGEXRsxgzMxtaDTcsBnwGKF6afQ/wKPAt4LVc2U+ASyWNjYitubKzge5cXTMzq5MhDy6SRpEsooTkLmRcbtoxwI8joteGX5J2AH+IiGUFxYuAi4DFkq4F3gpcDdxUND3ZzMyG2JBvXClpIvD7Pl4+PiI6SrynA7gvIi4pKj8RuBU4mWTm2F3A1RGxp4x+vAasqaDrVr7DgNfr3Yn9nK/x0PB17u24iBjwofWw3RXZBo+kJ8vZNdWq52s8NHydq9eID/TNzKzJObiYmVnqHFxsMNxZ7w4MA77GQ8PXuUp+5mJmZqnznYuZmaXOwcUqJmlZLjtoqa+Tc3Uk6UpJf5DULennkibXu+/Nosxr3FHitXX17nszkXSOpF9J6pL0iqTvSTqqqI4/y1XwsJhVLLe+qDgh2wLgPcCREbFb0hXAfOBSku145gEnAe+KCP8CHECZ17gDWEGSVC9vZ0T8amh62dwkfQz4IXAbsAQ4Evg6yTZTU/K5p/xZro6Di9VM0gHAOuC/IuJzkg4COoEbI2JBrs5ooAO4IyKchrpCxdc4V9ZBicXFVh5J9wBvj4gpBWX5gHNiRDznz3L1PCxmafgbkt2o784dzyD5q/sH+Qq5pG/3k2QQtcoVX2OrXSuwuagsnyMqv/u6P8tVcnCxNJwD/BFYnjueBOwBXiiq9xzOElqt4muc92lJOyVtlnSfpOPq0Ldm9R3g/ZL+UdI4SX9KMiz2cEQ8m6vjz3KVHFysJrmNSD8G/KAguVsb0FVij7dNwKjcEI+VqY9rDMnwzVzgNJLnAScDyyWVynNkRSLif4BzSdaybAaeB1qATxRU82e5Sg4uVquPAqPxcM1gKnmNI+LiiLg7IpZHxJ3Ah4GjgE/VoY9NR9KpJLur3wKcSnJ3eAiwRFJLPfu2P2jEfC7WXM4BXixKlbAJGCOppegvvjZge0TsHNIeNr9S17iXiPiNpOeB9w5Nt5rejcCPIuKyfIGkp0hmhJ0BLMaf5ar5zsWqlht+OZ3edy3tJMMLbysqn4SzhFakn2vcl8h92cAmAU8VFkTE8yQJB0/IFfmzXCUHF6vFmcCB9P7FtxLYAszOF+SeG3yUJIOola+va9yLpHeR/NJbPdid2k+soeguT9KfASNJphqDP8tV87CY1eIc4OmIeK6wMCJ2SLoGuErSJvYuPBvBvgv+bGAlr7GkvwXmAEuBV0mCyleAl4HvDnEfm9UiYKGkV0kCxQSSxZIdwI/Bn+VaOLhYVSQdRjJL6ao+qlxD8h/wCuBQ4EngQxHROTQ9bH4DXOM/AOOBm4EMsAH4KXCl03yX7d+BncDngAtI1rj8Argit5Ylz5/lKniFvpmZpc7PXMzMLHUOLmZmljoHFzMzS52Di5mZpc7BxczMUufgYmZmqXNwsWFJ0tWSXu/jte9K6ncfrxLvmZhLM/x3Fb7vL3Pve9cA9T4p6dwK2p0iaZOkcZWcp0Q7Z0l63hs5WqUcXMzSsZZky/tfDFL7nyTZHr5cXwcWFSyo/BVJ/16q8LyLSRJn/UOF77NhzsHFLAUR8UZErIqI7MC1B5ekt5NkrvxOviwituT6111JW7k88t8DLky3l7a/c3AxK4OkYyXdI2mjpO2SHpD0joLXew2LSTpQ0rckZSVtkHS9pC9KKrUtxmGS7pXUJel3kuYWtPNdkgRWp+TOEZKu7qe7/wQ8ExFvZk8sNSyWO75Y0r9Jek3Sekm3STqwqL3/Bt4r6Z1lXi4zBxcb3iT9SfEXe/On5+scQjLc9Q6SPag+SZK860FJI/tp/jqSoayvAn8PHAt8uY+63waeJtkFeRlwm6STcq99DXgE+D+Soa2Tgbv6Oe9pJLv5luPLJAnG5gDXA+cDFxdWyG2auQn4YJltmnnjShvWDgV29fFa4bb1XyIJJpMjYiOApBUku+f+M3Bb8ZslHQqcB8yPiIW5sgeA3/Rxvrsj4uu5estItnSfBTweES9J2giMiIhV/f1AkgS8B/h+f/UKdETEubl/PyBpZu681xXVewY4CbMyObjYcLaZ0n+N/ytwZMHxB4GfAVtydzYAW0kC0NQ+2n43cBDwo3xBRISk+4ETS9T/34J6uyS9ALylzJ+jUBtJ/peSM+H6O2/Os5T+mV4HjqiiPzZMObjYcLa7VOpgSRvYN7gcBkwHzi7RxkN9tJ3/RfxaUXnxcV7xRICdJMGpUvn3vFFm/XLP+0aV/bFhysHFbGAbSe5Avlbita19vGdd7vvhufdTcDyY8ufKpNxuhn1/DrN+ObiYDewhkof4v61gKu+vgR3AGeSeX+Seh3y0yj6UdSeTy5z4MnB8lefpy0SSZGRmZXFwMRvYTSSzqR6W9E3gFZKUuKcAv4iIXvntI2KDpG8DX5W0C3gO+BQwDqgmQ187cIakjwN/BF6NiFf7qLsCmFLFOUqSNJokjXJfWUfNevFUZLMBRMTrJM9c2oGFJA/BrwMOJplF1Zd/IclnfzVwN9AJ/AdQTRri23Pn/Q7wBMlMtL4sJlkT09806Ur8NbAdeCCl9mwYcJpjsyEk6UGgNSJOGcRzHEByd/P5iLg3hfbuBrZFxGdq7pwNGx4WMxskkk4FppHs69VKMtvsNGD2YJ43InZKup5kMWRNwUXSMSTPjf48jb7Z8OHgYjZ4uoCPA1eQPIx/ATg3Iu4bgnPfCoySdHBEbK6hnbcAF0TEiyn1y4YJD4uZmVnq/EDfzMxS5+BiZmapc3AxM7PUObiYmVnqHFzMzCx1Di5mZpa6/wceLIVq99uKggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## find the points along the line (predictions)\n",
    "## from the data x values\n",
    "y_hat = baseball_data[\"height\"]*m + b\n",
    "\n",
    "## set the figure dimensions\n",
    "scat_fig = plt.figure(figsize = (6,6))\n",
    "    \n",
    "## make a scatter plot\n",
    "_  = plt.scatter(\n",
    "    baseball_data[\"height\"],baseball_data[\"weight\"],\n",
    "    color = \"black\", s = 5, alpha = 0.5\n",
    ")\n",
    "\n",
    "##\n",
    "_  = plt.plot(\n",
    "    baseball_data[\"height\"],\n",
    "    y_hat,\n",
    "    color = \"red\", lw = 2\n",
    ")\n",
    "\n",
    "## Set the tick and label fontsize\n",
    "plt.tick_params(labelsize=15)\n",
    "\n",
    "## Set the title\n",
    "_ = plt.title(\"Player heights vs. weights\", fontsize = 15)\n",
    "\n",
    "## Set the y-label\n",
    "_ = plt.ylabel(\"Weight (lb)\", fontsize = 15)\n",
    "\n",
    "## Set the y-label\n",
    "_ = plt.xlabel(\"Height (in)\", fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.4 Exercise: Understanding error and objectives&mdash;what's the difference between $x$ and $y$?\n",
    "Non-vertical/horizontal lines are generally invertible, meaning that if you know $y$ from $x$ through the relationship $y = mx + b$, then you know $x$ through the relationship $x = \\frac{y}{m} - \\frac{b}{m}$, i.e., another line. Considering this, we could switch $x$ and $y$ in our $SSE$ and solve the optimization for $x$:\n",
    "$$\n",
    "SSE = \\sum_{i = 1}^n (ky_i + c - x_i)^2\n",
    "$$\n",
    "But here's the question:\n",
    "\n",
    "> Would we then find that $c = \\frac{b}{m}$ and $k = \\frac{1}{m}$?\n",
    "\n",
    "Investigate this question by applying our linear regression code from __Sec. 7.1.2.2__ with $x$ and $y$ switched, and plotting this new line in blue along with the points and other line __Sec. 7.1.2.3__. Discuss your observations in the markdown cell below and how the changing the role of $x$ and $y$ in $SSE$ impacts the outcome of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.5 Using sklearn to perform linear regression\n",
    "While our implementation of linear regression was pretty straightforward, this was mainly because of the solvability and simplicity of the model. In many cases, models will be significatly more complicated and rely on heavy machinery (often, complex versions of gradient descent) to optimize them. So, evem though we can benefit from forming an intuition by modeling linear regression 'from scratch', what we really need to get into is the primary means by which models like these are implemented. As mentioned, the `sklearn` module will do much of this modeling work for us, so let's explore how it handles the problem&mdash;it is just about as easy as anything else in sklearn, relying only on a single function: \n",
    "\n",
    "- `sklearn.linear_model.LinearRegression(x,y)`\n",
    "\n",
    "Note that since we're using `sklearn`, we benefit from the module's flexibilty on type of input data. In particular, coding the regression by hand we had to be sure to have basic `numpy.array()`s, while in `sklearn` we could pass through the `pandas` series objects, as possessed from data load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the slope:  [[5.60288682]]\n",
      "Here's the intercept:  [-205.27886031]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/base.py:485: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create linear regression object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "lm.fit(height_train, weight_train)\n",
    "\n",
    "# Train the model using the training sets\n",
    "print(\"Here's the slope: \",lm.coef_)\n",
    "print(\"Here's the intercept: \", lm.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.6 Evaluating the model\n",
    "So is this a good fit? That's what $SSE$ is supposed to tell us! To compute, we can once again built the model from $x$ and the regressed parameters, or now with `sklearn`, use the `lm.predict()` method to have them built for us! Regardless, once the predictions are computed we must then add up the squared errors to measure the SSE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[312436.3165535]\n"
     ]
    }
   ],
   "source": [
    "predictions = lm.predict(height_test)\n",
    "\n",
    "SSE = sum((np.array(weight_test) - predictions)**2)\n",
    "\n",
    "print(SSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.7 Evaluating your own model\n",
    "Compute the predictions that came out of the 'from scratch' implementation (__Sec. 7.1.2.2__) and compute the $SSE$ to exhibit consistency between the two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.8 Stepping back, what does the 'objective' $SSE$ actually mean?\n",
    "Perhaps the problem with interpreting our $SSE$ number is that it is 1) in the wrong (squared) units and 2) the total error across all point. To make error more interpretable, folks will often use the _root mean square error (RMSE)_: \n",
    "$$RMSE = \\sqrt{\\frac{SSE}{m}}.$$\n",
    "\n",
    "This quantity resolves both issues of interpretation and could be utilized in place of the $SSE$ as an objective function to produce the exact same output parameters! However, it's a bit more difficult to analyze, so the $SSE$ is generally utilized for optimization. Now that we've transformed our evaluation score we should be in the same units as the target variable&mdash;pounds. Apparently, we're only off by around 20 pounds, on average!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21.37237096]\n"
     ]
    }
   ],
   "source": [
    "predictions = lm.predict(height_test)\n",
    "\n",
    "SSE = sum((np.array(weight_test) - predictions)**2)\n",
    "\n",
    "RMSE = np.sqrt(SSE/len(predictions))\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.9 Linear regression with multiple variables\n",
    "As it turns out, a linear model may have _many_ predictor ($x$) variables. In other terms, we might now have a _matrix_ $x$ ($n$ rows by $k$ columns) of variables, where for each data point ($i$-row), the $j$-indexed columns $x_{i,j}$ are _all_ predictors for the target variable $y$: \n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\hat{m}_1x_{i, 1} + \\hat{m}_2x_{i, 2} + \\cdots +\\hat{m}_kx_{i, k} + \n",
    "\\hat{b}\n",
    "$$\n",
    "\n",
    "A best set of all $k + 1$ parameters to this type of linear model fortunately still exist. However, implementation now requires an algorithm like gradient descent! Fortunately, we don't have to pull out our code from __Chapter 5__ right here; this has been made _very_ convenient for us by `sklearn`! However, the first thing we'll need to do is organize our data for the experiment. In particular, let's set _the same_ training/test set of weights, but correspond them to the rest of the numeric columns (as a matrix) in our baseball player dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2072, 24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jw3477/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:8: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "baseball_data = pd.read_csv('./data/2008_merged_baseball_data.csv')\n",
    "\n",
    "y = baseball_data[['weight']]\n",
    "\n",
    "x = baseball_data[[u'G', u'AB', u'R', u'H', u'2B', 'salary',\n",
    "                   u'3B', u'HR', u'RBI', u'SO', u'HBP', u'G_F', u'PO', u'A', u'E',\n",
    "                   u'height', u'G_P', u'W', u'L',\n",
    "                   u'SO_P', u'ER', u'HR_P', u'H_P', u'HBP_P']].as_matrix()\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data are lined up we can predict the weights from our matrix using the same procedure&mdash;we just have to `.fit()` and `.predict()` using the data matrix as the first argument, and pass the vector of weights as the second (the target/prediction variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20.84737798]\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# create training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model using the training sets\n",
    "lm.fit(x_train, y_train)\n",
    "\n",
    "# predict on a separate testing set\n",
    "predictions = lm.predict(x_test)\n",
    "\n",
    "# compute the SSE\n",
    "SSE = sum((np.array(y_test) - predictions)**2)\n",
    "\n",
    "# compute the RMSE\n",
    "RMSE = np.sqrt(SSE/len(predictions))\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3 PCA and dimensionality reduction\n",
    "\n",
    "#### 7.1.3.1 Eigenvectors and Eigenvalues (review)\n",
    "Now that we've into utilizing multiple variable in our regression experiements, it's a good opportunity to discuss some more of the utility we get out of linear algebra. So, recall: \n",
    "\n",
    "+ if $A$ is an $(n \\times n)$ matrix and $v$ is an eigenvector of $A$, then for some non-zero scalar (constant), $\\lambda$:\n",
    "     \n",
    "$$A\\cdot v = \\lambda v$$\n",
    "\n",
    "$\\lambda$ is called the eigenvector's _eigenvalue_. In other words, matrix-times-eigenvector returns a vector that points in the same exact direction as the original eigenvector. You can keep multiplying the result by $A$ and get back scalings, i.e., growing/shrinking of the same vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.2 Eigenvectors and eigenvalues intuition (review)\n",
    "Geometrically, eigenvectors tell you the directions along which your data spread out. This means we can use eigenvectors to tell us about the variation present in a spreadsheet of data, i.e., about how columns and rows of data covary. So, if each point in a data set is a row, represented by two variable columns, we would be able to use eigenvectors to show us something like:\n",
    "\n",
    "![eigen](images/eigenvectors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.3 But eigenvectors only exist for square matrices!\n",
    "This means there's no such things as eigenvectors and eigenvales for an $m>2$ row by $2$-column matrix. This is why we need to bring together our statistical discussion of variance into matrices, now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.4 The covariance matrix\n",
    "Recall our formula for the variance of a _vector_ $x = [x_1, x_2, \\cdots, x_n]$:\n",
    "\n",
    "$$\n",
    "\\sigma_{x,x}^2 \n",
    "= \\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2 \n",
    "= \\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})(x_i-\\overline{x})\n",
    "$$\n",
    "\n",
    "It's _covariance_ with another vector, $y = [y_1, y_2, \\cdots, y_n]$ is simply\n",
    "\n",
    "$$\n",
    "\\sigma_{x,y}^2 \n",
    "= \\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})\n",
    "$$\n",
    "\n",
    "This quantity describes how much the two vectors vary together. The covariance matrix of the $n$ columns of an $(m \\times n)$ matrix $A$ then records the covariance of every pairwise comparison of columns of $A$. Since $\\sigma_{x,y}^2 = \\sigma_{y,x}^2$, i.e., a different order records the same covariance, these values are recorded on both sides of this _symmetric_ square matrix. So, writing $\\sigma^2_{i,j}$ to indicate the covariance between the $i^\\text{th}$ and $j^\\text{th}$ columns, the covariance matrix is:\n",
    "\n",
    "$$\n",
    "\\Sigma_A = \n",
    "\\begin{bmatrix}\n",
    "    \\sigma^2_{1,1} & \\sigma^2_{1,2} & \\dots  & \\sigma^2_{1,n} \\\\\n",
    "    \\sigma^2_{2,1} & \\sigma^2_{2,2} & \\dots  & \\sigma^2_{2,n} \\\\\n",
    "    \\vdots  & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    \\sigma^2_{n,1} & \\sigma^2_{n,2} & \\dots  & \\sigma^2_{n,n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Once again and fortunately for us, `numpy`'s got the goods to compute covariance easily. However, this numpy function defaults in a bit of a funny way, treating rows as variables and columns as observations. Thus, we'll have to set `rowvar = False` to get the covariance of our column-variables, which is much more common in spreadsheet-style data.\n",
    "\n",
    "Note: You should ask youself in the below example: why are all of the covariance values the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "\n",
      "[[15. 15. 15.]\n",
      " [15. 15. 15.]\n",
      " [15. 15. 15.]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,    2,  3],\n",
    "    [ 4,    5,  6],\n",
    "    [ 7,    8,  9],\n",
    "    [ 10,  11, 12]\n",
    "])\n",
    "\n",
    "print(A)\n",
    "print(\"\")\n",
    "\n",
    "## compute the covariance matrix of the columns\n",
    "print(np.cov(A, rowvar = False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.5 Principal Component Analysis\n",
    "Now that we've done all of the hard work thinking about eigenvectors and covariance matrices, it's relatively easy to talk about a  pretty high-level&mdash;but common&mdash;exploratory statistical procedure called _principal component analysis (PCA)_. A principle component analysis simply consists of finding the eigenvectors of a dataset's correlation matrix. It allows for variable-by-variable comparision in context of _all_ columns, as opposed to just pairwise comparisions. Here's a nice intuitive/geometric interpretation from Wikipedia:\n",
    "\n",
    ">  PCA can be thought of as fitting an n-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component.\n",
    "\n",
    "Note: principal components refer to the eigenvectors of the covariance matrix. In truth, we've already seen this picture:\n",
    "\n",
    "![Eigenvectors](./images/eigenvectors.png)\n",
    "\n",
    "There are quite a few high-level toolkits out there for computing PCA. For example, there are PCA tools in both of `sklearn` and `matplotlib`. However, we can compute PCA easily in two steps with numpy: \n",
    "\n",
    "1. find the covariance matrix\n",
    "2. find the covariance matrix's eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "\n",
      "[[-0.81649658  0.57735027  0.        ]\n",
      " [ 0.40824829  0.57735027 -0.70710678]\n",
      " [ 0.40824829  0.57735027  0.70710678]]\n",
      "\n",
      "[ 0. 45.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,    2,  3],\n",
    "    [ 4,    5,  6],\n",
    "    [ 7,    8,  9],\n",
    "    [ 10,  11, 12]\n",
    "])\n",
    "\n",
    "print(A)\n",
    "print(\"\")\n",
    "\n",
    "## compute the covariance matrix of the columns\n",
    "A_cov = np.cov(A, rowvar = False)\n",
    "\n",
    "## compute the eigenvectors of the covariance matrix\n",
    "e_vals, e_vecs = np.linalg.eig(A_cov)\n",
    "\n",
    "## the eigenvectors are the principal components\n",
    "print(e_vecs)\n",
    "print(\"\")\n",
    "\n",
    "## the eigenvalues tell us how much variance the components explain\n",
    "print(e_vals)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.6 PCA dimensionality reduction\n",
    "So far, we've only discussed linear algebra and PCA dryly with made-up data. One of the core usages of PCA is _dimensionality reduction_, where we will try to decide if all of our variables are actually telling us something unique. To make this less dry, let's see what happens with the baseball players' heights, weights, and numbers of home runs hit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00048286  0.54664363 -0.02487939]\n",
      " [ 0.54664363  1.00048286  0.1482903 ]\n",
      " [-0.02487939  0.1482903   1.00048286]]\n",
      "\n",
      "[0.42736321 1.56104753 1.01303783]\n",
      "\n",
      "[[ 0.67939766 -0.685854   -0.26081242]\n",
      " [-0.70268621 -0.71048902  0.03791369]\n",
      " [ 0.21130762 -0.15751081  0.96464472]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## restrict to numeric columns and drop any NAs\n",
    "d = baseball_data[[\"height\",\"weight\", \"HR\"]].dropna()\n",
    "\n",
    "## standardize the data\n",
    "d[\"height\"] = (d[\"height\"] - np.mean(d[\"height\"]))/np.std(d[\"height\"])\n",
    "d[\"weight\"] = (d[\"weight\"] - np.mean(d[\"weight\"]))/np.std(d[\"weight\"])\n",
    "d[\"HR\"] = d[\"HR\"]/np.std(d[\"HR\"])\n",
    "\n",
    "## find the covariance matrix\n",
    "d_cov = np.cov(d, rowvar = False)\n",
    "\n",
    "print(d_cov)\n",
    "print(\"\")\n",
    "\n",
    "## find the eigenvectors and eigenvalues\n",
    "d_e_vals, d_e_vecs = np.linalg.eig(d_cov)\n",
    "\n",
    "## the eigenvalues weight the eigenvector/principal components\n",
    "## by how much variance they explain\n",
    "print(d_e_vals)\n",
    "print(\"\")\n",
    "\n",
    "## here are the eigenvectors/principal components\n",
    "print(d_e_vecs)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.6 Interpreting PCA\n",
    "How do we interpret this? Presently we have a bunch of abstracted eigenvectors and eigenvalues, but there's some real opportunity for interpretation with PCA. In particular we may:\n",
    "\n",
    "1. Normalize the eigenvalues (divide by their sum)\n",
    "    - This lets us see the total variance explained by each eigenvector (column in the final print out, above). \n",
    "    - We can then look at the vectors by this \"importance\" measure.\n",
    "    - Each vector has directions that correspond to the different variables.\n",
    "2. Sort the eigenvectors by eigenvalue\n",
    "    - The two \"biggest\" vectors explain almost 86% of the variation.\n",
    "    - If all variables were equally \"important\", only 66.67% would be explained.\n",
    "    - So, we might be able to drop a variable without losing much information.\n",
    "3. Interpret the directions of the eigenvectors and get rid of suppressed variables until an acceptable portion of the  total variance is explained.\n",
    "    - So, our \"best\" vector is height-weight balanced, with little 'home run hitting' relevance. \n",
    "    - Having a bit more emphasis on weight therefore indicates we can get rid of height and maintain the majority of our variation\n",
    "    - Now, our  second-best vector is almost all 'home run hitting' and nothing else. So age is largely independent of the other two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52.009804323699946, 33.751630404550916, 14.23856527174914)\n",
      "\n",
      "[-0.685854   -0.71048902 -0.15751081]\n",
      "\n",
      "[-0.26081242  0.03791369  0.96464472]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## normalize the eigenvalues into percents to\n",
    "## to describe the  percent of total variation explained\n",
    "eig_percents = d_e_vals/sum(d_e_vals)*100\n",
    "\n",
    "sort_ix,  sort_eig_percents = zip(*sorted(enumerate(eig_percents), reverse = True, key = lambda x: x[1]))\n",
    "\n",
    "## Let's looks at the percent of total variation explained\n",
    "## first two account for 86% of the total variation!\n",
    "## if they were all balanced\n",
    "print(sort_eig_percents)\n",
    "print(\"\")\n",
    "\n",
    "## The most-important vector is balanced across height and weight, \n",
    "## with a little more emphasis on weight. This means we can probabily get rid of height!\n",
    "print(d_e_vecs[:,sort_ix[0]])\n",
    "print(\"\")\n",
    "\n",
    "## The second most important vector is almost all about home runs\n",
    "## but it does exhibit covariation with height!\n",
    "print(d_e_vecs[:,sort_ix[1]])\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.7 PCA projection\n",
    "While discussing PCA it's also important (and very relevant) to touch on the topic of feature extraction. From our PCA algorithm we wind up with a covariance matrix's eigenvector decomposition&mdash;the principal components are a list of eigenvectors:\n",
    "\n",
    "$$v_1,\\cdots, v_n;$$\n",
    "\n",
    "ordered by the decreasing size of their corresponding _normalized_ eigenvalues:\n",
    "\n",
    "$$\\lambda_1,\\cdots, \\lambda_n.$$\n",
    "\n",
    "Again, if normalized these values indicate the portion of variation described by each eigenvector. Above, we read through the eigenvectors to see which variables provided similar information so that we might be able to eliminate some. But now we're going to use a special property of the eigenvectors/principal components:\n",
    "\n",
    "- The $n$ principal components let you _transform_ your data into more independent features through matrix multiplication.\n",
    "\n",
    "So, what's that matrix by which you would multiply your data? Supposing the data has $n$ rows (records) by $k$ columns (variables), we can define the principal components matrix: $W = [v_1, \\cdots, v_k]$, which has each component as a column vector. With this matrix, the transformed matrix, or engineered features are the matrix product: $T = XW$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.8 PCA projection for feature extraction\n",
    "We already have all of the nuts and bolts to do PCA feature extraction. However, just like we've done with linear regression we're going to perform PCA as a learning algorithm provided by `sklearn`, using `sklearn.decomposition.PCA()`. Let's work with our baseball players' statistics matrix `x` and weights in `y`, from __Sec. 7.1.2.9__.\n",
    "\n",
    "If we perform the PCA decomposition with numeric data we'll get a dataset with as many columns as there were numeric columns before. Note: these columns don't have values with specific meanings anymore. So if you're looking for interpretable feature, PCA extraction does not provide this. The key insight is that the new features are more independent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start out, let's give the `sklearn` implementation a try and look at the percentage of variance explained by the first 10 components. How could one feature explain such a high percentage of the variance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of variance explained:\n",
      "[9.99999994e+01 3.78163153e-07 1.13117140e-07 2.70818435e-08\n",
      " 2.12169121e-08 4.78196551e-09 1.84932530e-09 1.41529784e-09\n",
      " 1.26224248e-09 7.98933760e-10]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## perform PCA on our data\n",
    "pca = PCA()\n",
    "pca.fit(x)\n",
    "\n",
    "## let's check out how much variance \n",
    "## is explained by the first ten components\n",
    "print(\"Percentage of variance explained:\")\n",
    "print(100*pca.explained_variance_[0:10]/sum(pca.explained_variance_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting features we might like to put into our classifer are now accessible to use via: the `.transform()` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2072, 24)\n",
      "(2072, 24)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "\n",
    "x_ENG = pca.transform(x)\n",
    "\n",
    "print(x_ENG.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.9 Exercise: standardizing the columns of a matrix\n",
    "Utilize the two vectorized matrix methods `.mean()` and `.var()` to standardize the `x` columns (see __Sec. 7.1.3.6__ for an example with standardization) in preparation for dimensionality reduction and fit the result with PCA as in __Sec. 7.1.3.8__. Exhibit the resulting percentage of variance explained by the components are they more reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.10 Dimensionality reduction\n",
    "What we've done above is compute $T = X\\cdot W$ through the principal components. However, we know most of the variation&mdash;over 90%&mdash;is explained by the first $3$ of $24$ components. Thus, we could perform a _projection_ by using the first $3$ components, only, i.e., instead of $W$, multiply $X$ by the smaller matrix $W_3 = [v_1, v_2, v_3]$. Then, $T_3 = X\\cdot W_3$ will still represent almost $95\\%$ of the data variation, while only having $3$ instead of $24$ columns. To complete the dimensionality reduction, all we have to do is initialize the PCA with a specified number of components, i.e., `n_components=3`. We'll explore just how this can help out our ML algorithms, below. Here's the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2072, 24)\n",
      "(2072, 3)\n"
     ]
    }
   ],
   "source": [
    "## perform PCA on our data\n",
    "## reduce down to 3 components\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(x)\n",
    "\n",
    "## print out the original data shape\n",
    "print(x.shape)\n",
    "\n",
    "## transform the data\n",
    "x_ENG = pca.transform(x)\n",
    "\n",
    "## print out the new, engineered data shape\n",
    "print(x_ENG.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1.3.11 Linear Regression with PCA Projection\n",
    "Our linear regression utilizes _a lot_ (24) different variables to predict the weights of the players. However, our PCA dimensionality reduction can take the 24-column matrix and create a reduced, 3-column matrix of _engineered features_ that represents almost $95\\%$ of the original data set's variation. Let's see how well the dimensionally reduced features do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.69814209]\n"
     ]
    }
   ],
   "source": [
    "# Create linear regression object\n",
    "lm = LinearRegression()\n",
    "\n",
    "# create training and test sets\n",
    "x_train_ENG, x_test_ENG, y_train_ENG, y_test_ENG = train_test_split(x_ENG, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model using the training sets\n",
    "lm.fit(x_train_ENG, y_train_ENG)\n",
    "\n",
    "# predict on a separate testing set\n",
    "predictions = lm.predict(x_test_ENG)\n",
    "\n",
    "# compute the SSE\n",
    "SSE = sum((np.array(y_test_ENG) - predictions)**2)\n",
    "\n",
    "# compute the RMSE\n",
    "RMSE = np.sqrt(SSE/len(predictions))\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Cross-validation\n",
    "Now, since we're doing the training/test fit, there's always the unfortunate chance that we're overfit to the specific data in the training set, and/or reality of the test set. To alleviate this issue it's not uncommon to _cross-validate_ an experiment. Here, a train vs. test split is still done, but afterwords the training set is split again into _$k$ folds_, i.e., samples of equal size. After the folds are drawn, each of the $k$ pairings of $1$ _validation_ and $k-1$ _training_ records are used to train and evaluate a model. In other words, if there are $k=5$ folds ${A,B,C,D,E}$, the $k$ validation pairs are:\n",
    "\n",
    "1. train: $\\{A\\}$; validate: $\\{B,C,D,E\\}$\n",
    "2. train: $\\{B\\}$; validate: $\\{A,C,D,E\\}$\n",
    "3. train: $\\{C\\}$; validate: $\\{A,B,D,E\\}$\n",
    "4. train: $\\{D\\}$; validate: $\\{A,B,C,E\\}$\n",
    "5. train: $\\{E\\}$; validate: $\\{A,B,C,D\\}$\n",
    "\n",
    "For each pair, there are resulting coefficients&mdash;in our example, a slope and intercept, leaving us with two sets: $\\{m_1,\\cdots, m_5\\}$ and $\\{b_1,\\cdots, b_5\\}$. For a pair that are potentially _robust_ to overfitting, we can then take the averages, $\\overline{m}$ and $\\overline{b}$, and use _these_ in a _final_ evaluation on the test set. Here's a good graphic to visualize the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K Folds](./images/k-folds.png)\n",
    "\n",
    "As the picture indicates, we can also average things like accuracy, or precision, and recall along the way to get more-solid estimates of performance.\n",
    "\n",
    "### 7.2.1 Implementing a $k$-fold cross-validation\n",
    "As usual, there is module for it. This time we'll be using:\n",
    "\n",
    "- `kf = sklearn.model_selection.KFold(n_splits = k, random_state = n)`\n",
    "\n",
    "but we'll be sure to use it on the _training_ data. Afterwords, the resulting object can be run with the `.split()` on a target dataset to get indices for training and validation data:\n",
    "\n",
    "- `train_index, validate_index = kf.split(data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.74703160e-02 -2.59612904e-02 -4.35391302e-02  6.26657484e-02\n",
      "  7.92044248e-02 -7.47308072e-08 -2.71670267e-01  5.43296722e-01\n",
      " -7.36332458e-02  7.59477391e-02  3.41327172e-01 -1.60545050e-02\n",
      "  9.09621978e-03 -7.61457777e-03  4.70709808e-01  5.39330129e+00\n",
      "  3.48680097e-02  9.89852066e-01 -5.07434261e-01  2.77654384e-02\n",
      " -1.34374404e-01  8.79880140e-01 -3.20351477e-02 -1.61402964e-01]\n",
      "-194.3558997289556\n",
      "20.399791832197252\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "## initialize the model\n",
    "kf = KFold(n_splits=5, random_state = 42)\n",
    "\n",
    "## make lists to store our coefficients\n",
    "ms = []\n",
    "bs = []\n",
    "## make a list to store our RMSE\n",
    "RMSEs = []\n",
    "\n",
    "## loop over the folds\n",
    "for train_index, validate_index in kf.split(x_train):\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    lm.fit(x_train[train_index,], pd.DataFrame(y_train.values[train_index]))\n",
    "    \n",
    "    # Train the model using the training sets\n",
    "    ms.append(lm.coef_[0])\n",
    "    bs.append(lm.intercept_[0])\n",
    "    \n",
    "    # predict on a separate testing set\n",
    "    predictions = lm.predict(x_train[validate_index,])\n",
    "    \n",
    "    # compute the SSE\n",
    "    SSE = sum((np.transpose(y_train.values[validate_index]) - predictions[:,0])[0]**2)\n",
    "    \n",
    "    # compute the RMSE\n",
    "    RMSEs.append(np.sqrt(SSE/len(predictions)))\n",
    "    \n",
    "# let's look at the output from one fold\n",
    "print(ms[0])\n",
    "print(bs[0])\n",
    "print(RMSEs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.2 Build a Final Model\n",
    "One of the main points of the cross-validation was to control the variation in our model that was due to the pecularities of our training set. Now that we've seen 5 different ways to build this model, we can average them to smooth out the differences. With these averaged coefficients, we can then build our final model and apply it to the test set.\n",
    "\n",
    "Note: since we did this with many (24) variables, we technically have $24$ $m$s and $1$ $b$ per fold. We can put each list (of lists) of coefficients into a data frame and just use the `.mean()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -200.276911\n",
      "dtype: float64\n",
      "\n",
      "0     1.637851e-02\n",
      "1    -2.196354e-02\n",
      "2    -2.908700e-02\n",
      "3     6.158689e-02\n",
      "4     7.617558e-02\n",
      "5     1.753282e-08\n",
      "6    -2.873389e-01\n",
      "7     6.684239e-01\n",
      "8    -1.283583e-01\n",
      "9     7.746344e-02\n",
      "10    3.528479e-01\n",
      "11   -2.388121e-02\n",
      "12    9.818989e-03\n",
      "13   -2.421846e-03\n",
      "14    2.910881e-01\n",
      "15    5.476121e+00\n",
      "16   -1.667955e-02\n",
      "17    6.273025e-01\n",
      "18   -3.985396e-01\n",
      "19    7.926775e-02\n",
      "20    1.665966e-02\n",
      "21    5.811662e-01\n",
      "22   -7.417358e-02\n",
      "23   -3.087816e-01\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## here's the intercept\n",
    "bs_avg = pd.DataFrame(bs).mean()\n",
    "print(bs_avg)\n",
    "print(\"\")\n",
    "\n",
    "## here's the slopes\n",
    "ms_avg = pd.DataFrame(ms).mean()\n",
    "print(ms_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2.3 Wait, what does our final model look like?\n",
    "Well, we did this with many (24) regression variables, which means that we now have $24$ $m$-values, $\\hat{m}_1, \\cdots, \\hat{m}_{24}$, and one slope $\\hat{b}$. For a given record, e.j., the $j^\\text{th}$ row in our dataset:\n",
    "\n",
    "$$\n",
    "[x_{j, 1}, x_{j, 2}, \\cdots, x_{j, n}]\n",
    "$$\n",
    "\n",
    "our model now looks like:\n",
    "\n",
    "$$\n",
    "\\hat{y}_j = \\hat{m}_1x_{j, 1} + \\hat{m}_2x_{j, 2} + \\cdots +\\hat{m}_nx_{j, n} + \\hat{b}\n",
    "$$\n",
    "\n",
    "This is just $\\hat{b}$ plus the inner product of our $\\hat{m}$-values and the data set! Let's do this to build the final model and see how well it does on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.851193859519636\n"
     ]
    }
   ],
   "source": [
    "## compute our model of predictions from the aveaged values\n",
    "predictions = x_test.dot(ms_avg) + bs_avg[0]\n",
    "\n",
    "# compute the SSE\n",
    "SSE = sum((np.array(y_test['weight']) - predictions)**2)\n",
    "\n",
    "# compute the RMSE\n",
    "RMSE = np.sqrt(SSE/len(predictions))\n",
    "\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Classification\n",
    "Returning to our discussed distinction betwen classification and regression:\n",
    "> In general, for regression models the output variables take the form of continuous values, and in classification the output variables takes class labels.\n",
    "\n",
    "the most simple scenario through which to first approach classification is when there are _exactly two_ possible outcomes, i.e., class labels. This scenario is referred to as _binary classification_, and to study it we'll have to develop an objective function, i.e., a means for measuring performance. Here, we no longer have continuous variables as output, so there's no such thing as an $SSE$. This means that the output of any algorithm will simply be correct, or not, and the best place to start will be with simply counting up the 'hits' and 'misses' our algorithm makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.1 Class Imbalances: Accuracy isn't all it's cracked up to be\n",
    "Even in this simplest classification scenario (binary), 'accuracy' is a fraught measure of success. This is because we are often interested in finding &quot;positives&quot; that are few in number. \n",
    "\n",
    "For example, in the medical domain a deadly disease might be rare, with a prevalence of $1\\%$ in the general population. One could easily build a &quout;classifier&quot; that simply guesses no one has the disease. This classifier would have an accuracy $99\\%$, but that's no good! _No one_ would get diagnosed who _actually_ had the disease. This misrepresentation of classifier performance is symptomatic of a problem called a _class imbalance_, in which the classes appear in the larger population at _very_ different rates. \n",
    "\n",
    "A good preliminary view into to this discussion is provided by the following cartoon, which highlights the different ways you can be right vs. wrong in binary classification:\n",
    "\n",
    "![Binary Classification](./images/precisionrecall.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.2 Evaluating binary classification: the confusion matrix\n",
    "So, what are better measures of performance in this scenario? Let's talk about the _confusion matrix_, which is an unfortuantely terminology for an enumeration of the possible outcomes for an individual record being processed by a calssification algorithm. For this conversation, there are two big things to keep straight: class _label_ vs. _prediction_. Accuracy just measures the overall accord between truth an prediction, however, both a label and its prediciton can each take two flavors in binary classification, _positive_ and _negative_. Thus, we enumerate the four possible outcomes:\n",
    "\n",
    "- True Positive (__TP__): a label and prediction are both positive\n",
    "- True Negative (__TN__): a label and preduction are both negative\n",
    "- False Positive (__FP__): a label is negative, but its prediction is positive\n",
    "- False Negative (__FN__): a label is positive, but its prediction is negative\n",
    "\n",
    "The last two scenarios encode all of a model's errors. These outcomes are often recorded in a tabular fashion, referred to as the confusion matrix:\n",
    "\n",
    "![Confusion Matrix](./images/confusion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.1 Understanding class imbalances\n",
    "In the case of our class imbalance, we had very high TNs and no FPs, but _no_ TPs and very relatively few FNs. So how can we use this broken-down evaluation information to understand a model's performance in the face of a class imbalance (which is common)? To start this conversation, let's just put up model _accuracy_ in terms of the confusion matrix quantities:\n",
    "\n",
    "$$acc = \\frac{TP + TN}{TP +TN +FP + FN}$$\n",
    "\n",
    "Now, in the case of our class imbalance there might be _very_ few positively-labeled pieces of data, i.e., $TN + FP \\gg TP + FN$. In this case it can pay off _big_ in terms of accuracy to keep $FP,TP = 0$, optimizing for accuracy with:\n",
    "\n",
    "$$acc = \\frac{TN}{TN + FN},$$\n",
    "\n",
    "So, if _positives_ are rare and _actually_ what we're interested in predicting, they we might instead interest ourselves specifically in _positive prediction_ assessment (below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.2 Example: binary prediction data\n",
    "Pulling from a dataset on the mortality of Titanic (the ship) passengers provided by Stanford CS:\n",
    "\n",
    "- http://web.stanford.edu/class/archive/cs/cs109/cs109.1166/problem12.html\n",
    "\n",
    "Let's investigate the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Siblings/Spouses Aboard</th>\n",
       "      <th>Parents/Children Aboard</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. Owen Harris Braund</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. John Bradley (Florence Briggs Thayer) Cum...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Miss. Laina Heikkinen</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mrs. Jacques Heath (Lily May Peel) Futrelle</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Mr. William Henry Allen</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass                                               Name  \\\n",
       "0         0       3                             Mr. Owen Harris Braund   \n",
       "1         1       1  Mrs. John Bradley (Florence Briggs Thayer) Cum...   \n",
       "2         1       3                              Miss. Laina Heikkinen   \n",
       "3         1       1        Mrs. Jacques Heath (Lily May Peel) Futrelle   \n",
       "4         0       3                            Mr. William Henry Allen   \n",
       "\n",
       "      Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare  \n",
       "0    male  22.0                        1                        0   7.2500  \n",
       "1  female  38.0                        1                        0  71.2833  \n",
       "2  female  26.0                        0                        0   7.9250  \n",
       "3  female  35.0                        1                        0  53.1000  \n",
       "4    male  35.0                        0                        0   8.0500  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = pd.read_csv('./data/titanic.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.3 Exercise: reviewing the balance of classs\n",
    "Investigate the Titanic data just a bit more and determine the numbers of positives (survivals) and negatives (fatalities). Discuss the severity of these data's class imbalance below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.4 Example: setting a baseline\n",
    "First things first, to talk about performance we'll need some predictions. Let's just use a baseline, \"women and children first\" and predict that only these individuals survive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline = ((titanic['Sex'] == 'female') | (titanic['Age'] < 18)).astype(int)\n",
    "baseline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to compute the accuracy we'll need to know each of the various confusion statistics. Let's do that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "258 123 422 84\n"
     ]
    }
   ],
   "source": [
    "TP = ((baseline == 1) & (titanic['Survived'] == 1)).astype(int).sum()\n",
    "FP = ((baseline == 1) & (titanic['Survived'] == 0)).astype(int).sum()\n",
    "TN = ((baseline == 0) & (titanic['Survived'] == 0)).astype(int).sum()\n",
    "FN = ((baseline == 0) & (titanic['Survived'] == 1)).astype(int).sum()\n",
    "print(TP, FP, TN, FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the confusion statistics, let's see how accurate our baseline was!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7666290868094702\n"
     ]
    }
   ],
   "source": [
    "print((TP + TN)/(TP + FP + TN + FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.3 Positive Prediction Assessment: Precision, Recall, &  F1\n",
    "_Precision_ and _recall_ help evaluate models amidst class-imbalanced data. But wait, what are precision and recall?? \n",
    "\n",
    "Probabilistically:\n",
    "\n",
    "- Precision: \n",
    "$$P(\\text{a prediction is correct}\\mid\\text{the prediction was positive}) = \\frac{TP}{TP+FP}$$\n",
    "\n",
    "- Recall: \n",
    "$$P(\\text{a prediction is correct}\\mid\\text{a label is positive}) = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "Essentially, precision tells you how _precise_ your classifier's positive predictions were. This is why is is also referred to as the _positive predicive value_. It tell's you about the chance that any positive guess is correct.\n",
    "\n",
    "On the other hand, recall tells you about you model's ability to _recover_ positive labels. This is why recall is also referred to as the model _sensitivity_, or _recovery rate_.\n",
    "\n",
    "We can see the broader picture of what's going on here in the whole cartoon from above:\n",
    "\n",
    "![Precison and Recall](./images/PrecisionRecall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3.1 Working with two evaluation measures\n",
    "When we move away from accuracy, we suddenly have multiple measures (precision and recall). This is a bit of a challenge, since people and machines both have an easier time judging if something's good by a single number. Just think about the stock market; people commonly track the Dow Jones, instead many individual stocks to assess overall market health. Well, that's exactly what's usually done with binary classifier assess. How? With averages! Specifically, since both precision and recall are _rates_, the accepted way to average the two for an overall model evaluation is called the [$F_1$ score](https://en.wikipedia.org/wiki/F1_score), which is the _harmonic mean_ of precision and recall:\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}} = \\frac{2PR}{P+R}$$\n",
    "\n",
    "While an $F_1$ score is no longer a probability,  it is bounded by $0$ and $1$ and can be thought of as a mixed measure of _sensitivity_ and _positive predictive_ performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3.2 Computing precision,  recall, and $F_1$ with sklearn\n",
    "This is once again a scenario where life is made easy for us by modules. Using the functions:\n",
    "\n",
    "- `sklearn.metrics.precision_score(predictions, labels)`\n",
    "- `sklearn.metrics.recall_score(predictions, labels)`\n",
    "- `sklearn.metrics.f1_score(predictions, labels)`\n",
    "\n",
    "we can move right along. So, were are classes heavily imbalanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's the accuracy: \n",
      "0.7666290868094702\n",
      "\n",
      "Here's precision, recall, and F1\n",
      "0.7543859649122807\n",
      "0.6771653543307087\n",
      "0.7136929460580913\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "## look at our accuracy\n",
    "print(\"Here's the accuracy: \")\n",
    "print((TP + TN)/(TP + FP + TN + FN))\n",
    "print(\"\")\n",
    "\n",
    "## compare accuract to the model's precision, recall, and F1\n",
    "print(\"Here's precision, recall, and F1\")\n",
    "print(precision_score(baseline, titanic[\"Survived\"]))\n",
    "print(recall_score(baseline, titanic[\"Survived\"]))\n",
    "print(f1_score(baseline, titanic[\"Survived\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3.4 Logistic Regression\n",
    "Why is this 'regression' in our classification section? Well, one way to view logistic regression is as a means to obtain a probabilistic output variable from numeric input. For binary classification, it turns out we can train a logistic regression model and accept high-probability outputs as predictions of 'truth', accepting the prediction of a positive class upon receiving a high probability as output and leave all other data points as labeled 'negative'.\n",
    "\n",
    "So, logistic regression is actually often used as a _classification_ algorithm, to be used in circumstances where the target variable is binary. So, going back to the Titanic dataset, we can approach the prediction of the `'Survived'` column as a binary output. However, as we'll see in a moment all input variables must be numeric, so let's do a bit of feature engineering. \n",
    "\n",
    "#### 7.3.4.1 Feature engineering for logistic regression\n",
    "Presently, the Titanic dataset has the binary `'male'`/`'female'` column. We can convert this to a numeric `0`/`1` variable, mapping `'male'` to `0` and `'female'` to `1` . After this, we can drop the name column for our predictors data and be sure to separate the `'Survived'` column as the $y$ value for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n",
      "   Pclass Sex   Age  Siblings/Spouses Aboard  Parents/Children Aboard     Fare\n",
      "0       3   0  22.0                        1                        0   7.2500\n",
      "1       1   1  38.0                        1                        0  71.2833\n",
      "2       3   1  26.0                        0                        0   7.9250\n",
      "3       1   1  35.0                        1                        0  53.1000\n",
      "4       3   0  35.0                        0                        0   8.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jw3477/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/jw3477/Library/Python/3.7/lib/python/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "titanic['Sex'][titanic['Sex'] == 'male'] = 0\n",
    "titanic['Sex'][titanic['Sex'] == 'female'] = 1\n",
    "\n",
    "y = titanic['Survived']\n",
    "x = titanic[['Pclass', \"Sex\", \"Age\", \"Siblings/Spouses Aboard\", \"Parents/Children Aboard\", \"Fare\"]]\n",
    "\n",
    "print(y.head())\n",
    "print(x.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.2 Defining logistic regression \n",
    "So setting up, each record looks like a numneric vector, $x$, from which we want to estimate a probability that the corresponding $y$ value is a $0$ or $1$ label.  The first most important thing we need here is the (standard) _logistic function_ (__also discussed in Ch. 5.0.3.3__), $q(t)$: \n",
    "\n",
    "$$q(t) = \\frac{1}{1+e^{-t}}$$\n",
    "\n",
    "Recall: the logistic function looks like an \"S\":\n",
    "\n",
    "![Logistic Function](./images/Logistic-curve.png)\n",
    "\n",
    "For any number, $t$, $q(t)$ returns a non-negative number in $[0,1]$ that can interpreted as a probability. So, _generalizing linear regression for probabilistic output_, we'll look for parameters, $\\vec{m} = [m_1,\\cdots, m_n]$ and $b$ to transform our data into logistic-function input: \n",
    "$$t_i = \\vec{m}\\cdot \\vec{x_i} + b$$\n",
    "to define the class probability model, $q(t_i)$. Here, a higher probability is considered a higher chance of positive prediction:\n",
    "\n",
    "$$\n",
    "P(x_i\\:\\text{ has class }\\:y_i = 1) = \n",
    "\\frac{1}{1+e^{-(\\hat{m}\\cdot \\hat{x_i} + b})}\n",
    "$$\n",
    "\n",
    "So, viewing the output as a 'probability', we can apply a threshold for classification. The \"best\" parameters $\\hat{\\vec{m}}$ and $\\hat{b}$, define $\\hat{t}_i = \\hat{\\vec{m}}\\cdot \\vec{x}_i + \\hat{b}$ to maximize independent joint logistic function probabilities:\n",
    "\n",
    "$$\n",
    "\\prod_{i = 1}^m\n",
    "q(\\hat{t}_i)^y_i\n",
    "\\left(1 - q(\\hat{t}_i)\\right)^{1-y_i}\n",
    "$$\n",
    "\n",
    "\n",
    "Optimization of $\\hat{\\vec{m}}$ and $\\hat{b}$ is done like with linear regression (likely some variation on gradient descent), but now towards _maximizing_ likelihood (above), instead of _minimizing_ $SSE$ as an objective. Generally, a threshold is applied to test-set probabilities for final classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.3 Implementing logistic regression\n",
    "Now, just like with regression we'll want to have training and test data separated first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709, 6) (178, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since we've modified our data, let's compute a baseline on our _test_ set for later comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 23 88 22\n",
      "Here's precision, recall, and F1\n",
      "0.6716417910447762\n",
      "0.6617647058823529\n",
      "0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "baseline = ((x_test['Sex'] == 1) | (x_test['Age'] < 18)).astype(int)\n",
    "baseline.head()\n",
    "\n",
    "TP = ((baseline == 1) & (y_test == 1)).astype(int).sum()\n",
    "FP = ((baseline == 1) & (y_test == 0)).astype(int).sum()\n",
    "TN = ((baseline == 0) & (y_test == 0)).astype(int).sum()\n",
    "FN = ((baseline == 0) & (y_test == 1)).astype(int).sum()\n",
    "print(TP, FP, TN, FN)\n",
    "\n",
    "## compare accuract to the model's precision, recall, and F1\n",
    "print(\"Here's precision, recall, and F1\")\n",
    "print(precision_score(baseline, y_test))\n",
    "print(recall_score(baseline, y_test))\n",
    "print(f1_score(baseline, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our baseline in place, let's if `sklearn`'s implementation:\n",
    "- `sklearn.linear_model.LogisticRegression(solver = 'lbfgs')`\n",
    "\n",
    "can do any better with our split up training data. Note that initializing the classifier with the argument: `solver = 'lbfgs'` is telling `sklearn` _which_ optimizer, e.g., gradient descent, we'd like it to use. `'lbfgs'` refers to one (likely complicated) such algorithm. To see what else is available and try out them out, review the docs!\n",
    "\n",
    "Note: after running `.fit()` on the classifier object, it is then possible to retrieve the model's (built in) predictions by the `.predict()` method, but more deeply understand the model's output probabilities through the `.predict_proba()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1]\n",
      "[[0.65327307 0.34672693]\n",
      " [0.82110738 0.17889262]\n",
      " [0.9760575  0.0239425 ]\n",
      " [0.72339466 0.27660534]\n",
      " [0.89775756 0.10224244]\n",
      " [0.32754277 0.67245723]]\n",
      "Precision, recall, and F1 were:\n",
      "0.5522388059701493\n",
      "0.7254901960784313\n",
      "0.6271186440677965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Logistic_classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "Logistic_classifier.fit(x_train, y_train)\n",
    "\n",
    "predictions = Logistic_classifier.predict(x_test)\n",
    "\n",
    "print(Logistic_classifier.predict(x_test)[:6])\n",
    "print(Logistic_classifier.predict_proba(x_test)[:6])\n",
    "\n",
    "print(\"Precision, recall, and F1 were:\")\n",
    "print(precision_score(predictions, y_test))\n",
    "print(recall_score(predictions, y_test))\n",
    "print(f1_score(predictions, y_test))  \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.4 Exercise: understanding prediction probabilities\n",
    "To `.predict()`, the `sklearn` module _must_ have some criteria for interpreting the output of `.predict_proba()` to the optimal predictions. To explore, set $11$ threshold values: `thresholds = [i/10 for i in range(0,10)]`, and, using the output of the `.predict_proba()` method determine your own set of predictions and performance statistics for each. When finished, indicate which (if any) of the thresholds is likely most similar to that used by `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [i/10 for i in range(11)]\n",
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.5 Bringing other feature engineering into the 'pipline'\n",
    "Since our model once again (like with linear regression) relies on a matrix of numeric data, we can try out our other tricks. This doesn't necessarily mean they'll help, but part of the game with modeling is exploring the effects of the application and combination of various algorithms. So, let's do some PCA on our Titanic data for dimensionality reduction and see if this helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(887, 6)\n",
      "(887, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## perform PCA on our data\n",
    "## reduce down to 3 components\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit((x - x.mean(axis=0))/x.var(axis = 0))\n",
    "\n",
    "## print out the original data shape\n",
    "print(x.shape)\n",
    "\n",
    "## transform the data\n",
    "x_ENG = pca.transform(x)\n",
    "\n",
    "## print out the new, engineered data shape\n",
    "print(x_ENG.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've modified our data, we have to do our training/test split again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(709, 6) (178, 6)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_ENG, y, test_size=0.2, random_state=42)\n",
    "print(x_train.shape, x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that we have the modified pieces we can apply the classifier and evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 1]\n",
      "[[0.65327307 0.34672693]\n",
      " [0.82110738 0.17889262]\n",
      " [0.9760575  0.0239425 ]\n",
      " [0.72339466 0.27660534]\n",
      " [0.89775756 0.10224244]\n",
      " [0.32754277 0.67245723]]\n",
      "Precision, recall, and F1 were:\n",
      "0.5522388059701493\n",
      "0.7254901960784313\n",
      "0.6271186440677965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "Logistic_classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "Logistic_classifier.fit(x_train, y_train)\n",
    "\n",
    "predictions = Logistic_classifier.predict(x_test)\n",
    "\n",
    "print(Logistic_classifier.predict(x_test)[:6])\n",
    "print(Logistic_classifier.predict_proba(x_test)[:6])\n",
    "\n",
    "print(\"Precision, recall, and F1 were:\")\n",
    "print(precision_score(predictions, y_test))\n",
    "print(recall_score(predictions, y_test))\n",
    "print(f1_score(predictions, y_test))  \n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.6 Exercise: exploring dataset variation through model performance and evaluation\n",
    "Even through our particular combination of algorithms _with_ PCA projection didn't appear to result in any model improvement, the execution of the model on fewer feature columns may have provided a critical boost to speed. This is a nice aspect of PCA, but there may in fact be other reasons to consider its maintenance in our final combine algorithm.\n",
    "\n",
    "Throughout this chapter we've discussed the importance of repeatability and randomness, but the particular randomization we've created might actually be the reason why we see no improvement with, e.g., projection. Specifically, the data we've split might just happen to have organized the test set (a smaller portion) to be easily predicted from the variation in the training set. In other words, we might accidentally be overfitting still. To explore, let's modify the random seed to a few different values and see how widely the performance changes, for both the original, and PCA-projected models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.7 Bringing cross-validation into the picture\n",
    "Now that we've explored the effects of model training on different data splits, let's see if some of this effect is additionally controlled by cross-validation. In paricular, utilize the cross-validation routine executed in __Sections 7.2.1&ndash;7.2.3__ but now in the context of our binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
