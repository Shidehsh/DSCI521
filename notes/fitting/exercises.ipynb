{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 521: Methods for analysis and interpretation <br> Chapter 7: Approaching regression and classification problems\n",
    "\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.4 Exercise: Understanding error and objectives&mdash;what's the difference between $x$ and $y$?\n",
    "Non-vertical/horizontal lines are generally invertible, meaning that if you know $y$ from $x$ through the relationship $y = mx + b$, then you know $x$ through the relationship $x = \\frac{y}{m} - \\frac{b}{m}$, i.e., another line. Considering this, we could switch $x$ and $y$ in our $SSE$ and solve the optimization for $x$:\n",
    "$$\n",
    "SSE = \\sum_{i = 1}^n (ky_i + c - x_i)^2\n",
    "$$\n",
    "But here's the question:\n",
    "\n",
    "> Would we then find that $c = \\frac{b}{m}$ and $k = \\frac{1}{m}$?\n",
    "\n",
    "Investigate this question by applying our linear regression code from __Sec. 7.1.2.2__ with $x$ and $y$ switched, and plotting this new line in blue along with the points and other line __Sec. 7.1.2.3__. Discuss your observations in the markdown cell below and how the changing the role of $x$ and $y$ in $SSE$ impacts the outcome of regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.2.7 Exercise: Evaluating your own model\n",
    "Compute the predictions that came out of the 'from scratch' implementation (__Sec. 7.1.2.2__) and compute the $SSE$ to exhibit consistency between the two implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1.3.9 Exercise: standardizing the columns of a matrix\n",
    "Utilize the two vectorized matrix methods `.mean()` and `.var()` to standardize the `x` columns (see __Sec. 7.1.3.6__ for an example with standardization) in preparation for dimensionality reduction and fit the result with PCA as in __Sec. 7.1.3.8__. Exhibit the resulting percentage of variance explained by the components are they more reasonable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2.3 Exercise: reviewing the balance of classs\n",
    "Investigate the Titanic data just a bit more and determine the numbers of positives (survivals) and negatives (fatalities). Discuss the severity of these data's class imbalance below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.4 Exercise: understanding prediction probabilities\n",
    "To `.predict()`, the `sklearn` module _must_ have some criteria for interpreting the output of `.predict_proba()` to the optimal predictions. To explore, set $11$ threshold values: `thresholds = [i/10 for i in range(0,10)]`, and, using the output of the `.predict_proba()` method determine your own set of predictions and performance statistics for each. When finished, indicate which (if any) of the thresholds is likely most similar to that used by `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [i/10 for i in range(11)]\n",
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.6 Exercise: exploring dataset variation through model performance and evaluation\n",
    "Even through our particular combination of algorithms _with_ PCA projection didn't appear to result in any model improvement, the execution of the model on fewer feature columns may have provided a critical boost to speed. This is a nice aspect of PCA, but there may in fact be other reasons to consider its maintenance in our final combine algorithm.\n",
    "\n",
    "Throughout this chapter we've discussed the importance of repeatability and randomness, but the particular randomization we've created might actually be the reason why we see no improvement with, e.g., projection. Specifically, the data we've split might just happen to have organized the test set (a smaller portion) to be easily predicted from the variation in the training set. In other words, we might accidentally be overfitting still. To explore, let's modify the random seed to a few different values and see how widely the performance changes, for both the original, and PCA-projected models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.4.7 Exercise: Bringing cross-validation into the picture\n",
    "Now that we've explored the effects of model training on different data splits, let's see if some of this effect is additionally controlled by cross-validation. In paricular, utilize the cross-validation routine executed in __Sections 7.2.1&ndash;7.2.3__ but now in the context of our binary classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
