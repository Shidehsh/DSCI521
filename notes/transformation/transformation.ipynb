{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 521: Methods for analysis and interpretation <br> Chapter 5: Data transformation, and analysis of functions\n",
    "\n",
    "## 5.0 Functions\n",
    "We've talked about functions a bit already in __Chapter 0.__, though this was in the context of computation! The basic interaction of a computing function is intuitively well linked to the mathematical concept, as it is common with a computing function to provide some input to recieve some output. The formal definition for a mathematical function is basically this, but requires a bit more precision from computing, as a function in computing is just a piece of code that, e.g., may complete some computing task and produce no output. Really, a computing function is about reuse of code. This is an excellent framework to complete the same or similar calculations abstractly, which is what mathematical functions often do. So, here's a formal definition for a mathematical function:\n",
    "\n",
    "> A function is a relationship between a set of inputs and a set of possible outputs where each input is related to exactly one output.\n",
    "\n",
    "More than often, functions will be denoted symbolically by a letter like $f$, accept some input named $x$, and have output denoted by $f(x)$. As taught in primary mathematics, it is helpful to consider input-output collections of points: \n",
    "\n",
    "$$\\{(x_1, f(x_1)),(x_2, f(x_2)), \\cdots, (x_3, f(x_3))\\}$$\n",
    "\n",
    "visually, i.e., as a graph. We'll discuss graphing data and functions at length in the next chapter, but to motivate this type of visual analysis makes it easy to see different types of relationships, like linear (green) vs. non-linear (orange):\n",
    "\n",
    "![Linear vs. non-linear functions](./img/linear_function_vs_nonlinear_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.1 Linear functions\n",
    "To make things concrete, let's talk about the most basic functions of real ($\\mathbb{R}$) numbers&mdash;linear functions. Generally, a linear function, $f$, may be expressed by specifying two _parameters_ (fixed constants), often denoted by $m$ and $b$ and referred to as the _slope_ and _intercept_, respectively. Given a real number, $x$, the output, $f(x)$ is calculated as:\n",
    "\n",
    "$$f(x) = mx + b$$\n",
    "\n",
    "So, the slope $m$ multiples (scales) the input, which the intercept $b$ then offsets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.1.1 Defining a linear function.\n",
    "For practice, let's construct a _computational_ function called `linear()` that accepts three values: the input, `x`, on which to operate, and slope and intercept parameters, $m$ and $b$ in a list called `c`, i.e., `c = [b, m]`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, c = [1,2]):\n",
    "    return(c[1]*x + c[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we wanted to compute values along the line which 1) doubles values and then 2) subtracts 1, we can easily over a range of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input values: \n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "output values: \n",
      "[2, 1, 0, -1, -2, -3, -4, -5, -6, -7]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "print(\"input values: \")\n",
    "pprint(list(range(10)))\n",
    "print(\"output values: \")\n",
    "pprint([linear(x, [2, -1]) for x in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.2 Linear functions of higher dimensionality\n",
    "Linear functions can interact with multiple dimensions/columns of input/data, too, and perhaps unsurprisingly linear algebra (introduced in __Chapter 1__) is a real piece of this picture! For example, a linear function $g$ might operate on three real dimensions, $x, y, z\\in \\mathbb{R}$. For this, the defining aspect as a linear will once again be multiplication and addition by constants. While we again need only one intercept, $b$, there'll be one separate slope/multiplier for each dimension: $m_x, m_y, m_z$. These are then composed in a similar way:\n",
    "\n",
    "$$g(x,y,z) = m_xx + m_yy + m_zz + b$$\n",
    "\n",
    "So, functions can in general have multiple inputs (a vector), while pointing to only a single output value (a number). More complicated scenarios certainly exist, e.g., when a function's output is a vector too. However, for the depth of our discussion and this course it will be sufficient to keep this dimensional complexity of functions to multiple inputs, like $g$.\n",
    "\n",
    "#### 5.0.2.1 Exercise: defining a function of multiple data dimensions\n",
    "In the manner of __Sec. 5.0.1.1__, re-define `linear` as a general linear function (like $g(x,y,z)$ in __5.0.2__) that 1) accepts a vector (array) of inputs, a vector of slopes, and an intercept; 2) checks to make sure there are as many inputs as there are slopes; and 3) computes the linear result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.2 Non-linear functions\n",
    "Linear functions are easy to define and interpret, only relying upon basic multiplication and addition and producing graphs as simple as straight lines, planes, and cubes, etcetera. However, a great many more functions are non-linear, and these things can get complicated fast. Let's look at a few types here and discuss.\n",
    "\n",
    "#### 5.0.2.1 Polynomials\n",
    "Perhaps the most basic types of non-linear functions are called _polynomials_, which turn out to subsume the class of linear functions. Polynomials layer on complexity from linear function by allowing for exponentiation (self-multiplication) of input values by positive integers. As with linear functions, there's an intercept, $b$, but the multiplying constants (generalizing slope) scale the different powers. Thus, it turns out to be very important to define the concept of polynomial _degree_, $n$, which refers to the largest power of the input $x$ multipled by a non-zero constant. So, for a polynomial, $f$, of degree $n$ operating on a single variable/dimension of data, we must have $n$ multiplying constants, referred to as _coefficients_: $c_1, c_2, \\cdots, c_n$, in addition to an intercept $c_0$ (i.e., $b$) that still offsets the mutiplied result:\n",
    "\n",
    "$$f(x) = c_nx^n + c_{n+1}x^{n+1} + \\cdots + c_1x + c_0$$\n",
    "\n",
    "Note that it makes sense to call $b$ $c_0$&mdash;just another coefficient&mdash;since $b$ multiples the only exponentiation of $x$ guarenteed to be a constant, i.e., $b = bx^0$, since $x^0 = 1$.\n",
    "\n",
    "#### 5.0.2.2 Exercise: defining a general polynomial\n",
    "The task here again is to generalize the function `linear` from __Sec. 5.0.1.1__, but now differently, as a polynomial of a single dimension. This new function should be called `poly` and accept a single input $x$ on which to perform the polynomial calculation, a vector (array) of coefficients $c$, and an intercept. The function should then 1) determine the degree of the polynomial from the number of coefficients (assumed to be in a fixed order, i.e., low to high), and 2) compute the polynomial output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.3 Polynomials of multiple dimensions\n",
    "Once we bring multiple dimensions into the definition of polynomial we're immediately faced with enumerative concerns over how the various dimensions of data are multipled together. Supposing again we have three real inputs: $x, y, z\\in\\mathbb{R}$, we now have to be concerned over terms like $x^2yz^4$, which brings together $x$ in degree $2$, $y$ in degree 1, and $z$ in degree $7$. However, the degree of the entire term is the sum of these: $7 = 2 + 1 + 4$. This means the degree of a polynomial of multiple dimensions now refers to the largest sum of powers of any term. Given a degree, there must now be a coefficient for each combination of powers of the inputs that sum to less than or equal to the degree. So, how many coefficients would this be? This involves a lot of counting, specifically using tools from a mathematical topic referred to _combinatorics_, which we'll discuss a bit in __Chapter 6__ when introducing probability. \n",
    "\n",
    "#### 5.0.3.1 Challenge exercise: defining a general polynomial of multiple dimensions\n",
    "To define a general polynomial of multiple dimensions, we'll specify only the non-zero coefficients for specific terms. To complete this exercise, have your function accept 1) a vector of input values and a 2) dictionary of coefficients such that each key is a tuple referring to the relevant powers of inputs and each value is the corresponding coefficient. The function should be called `multi_poly` and output the real-valued result. So for the relatively sparse (degree-7) polynomial:\n",
    "\n",
    "$$h(x,y,z) = 3x^2yz^4 +  8xy^2z^3 + 2yz^4 + 7x^2y +5$$\n",
    "\n",
    "the parameter-setting dictionary required as a second argument would be:\n",
    "```\n",
    "{\n",
    "    (2,1,4): 3,\n",
    "    (1,2,3): 8,\n",
    "    (0,1,4): 2,\n",
    "    (2,1,0): 7,\n",
    "    (0,0,0): 5\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.3 Exponential functions\n",
    "An exponential function is a function of the form:\n",
    "$$f(x) = a^x; a \\gt 0.$$\n",
    "\n",
    "Common values for the _base_, $a$, are $2$, $e$, and $10$. Exponential functions tells us what the powers of $a$ are. The intuition is generally that a small change in x will result in a _very_ large change in $f$. Since it's hard to do powers of a base in your head, it's often easiest to stick with the base $a = 10$, which comports with our _decimal_ number system. One of the big reasons why exponentials are so useful to us is because of the ways they let us handle data. These exponential rules are really mathematical \"tricks\" that let us cheat our way through computations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.3.1 Rules for exponential functions\n",
    "The two primary rules are the _multiplication-to-addition_ rule:\n",
    "\n",
    "$$a^xa^y = a^{x+y}$$\n",
    "\n",
    "which allows one to replace a difficult multiplication with addition in the exponent, and the the _power-to-multiplication_ rule:\n",
    "$$(a^x)^c = a^{cx}$$\n",
    "\n",
    "which allows one to replace a difficult exponentiation with a multiplication in the exponent. One special, but perhaps non-obvious fact about exponentials is the following identity:\n",
    "$$a^0 = 1,\\text{ for any } a \\gt 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.3.2 Computing exponential functions\n",
    "Another reason why exponentials are so important is that a lot of data exhibit exponential variation, like earthquakes and the changing sizes of hard drives. However (as we'll see) it's their complimentary functions, logarithms, that will make it easy to work with expontial data. Exponential functions are quite easy to work with by using the math module's function `math.power(a,x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "## find the exponential value (base 10) of a number\n",
    "print(math.pow(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, lots of times we'll want to take the exponential of a batch of numbers, say, in a list, which is where numpy comes in handy. Here's how to do a base-10 exponential to a list of numbers using `numpy.power(x, a)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.000000e-10 1.024000e+03 1.000000e+10 9.765625e+06]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## perform exponentiation base 10 on a list of numbers\n",
    "print(np.power([0.1, 2, 10, 5 ], 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.3.3 Exercise: defining the logistic function\n",
    "The [_logistic function_](https://en.wikipedia.org/wiki/Logistic_function) is a special mathematical function based on the exponential function that is used widely in neural network applications. It's a common \"S\" shape (sigmoid curve), with equation:\n",
    "\n",
    "$$f(x)= \\frac {L}{1+e^{-k(x-x_{0})}},$$\n",
    "\n",
    "where\n",
    "\n",
    "- $x_0$ = the x-value of the function's midpoint,\n",
    "- $L$ = the function's maximum value, and\n",
    "- $k$ = the steepness of the 's'.\n",
    "\n",
    "Your job here is to create a (computational) implementation of the logistic function, `logistic(x, k)`, accepting a values for $x$ and the above parameters, but with presets: $k = 1$, $L=1$, and $x_0 = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## place code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.4 Logarithmic Functions\n",
    "\n",
    "The other side of the exponential coin is the _logarithm_. With many types of data exhibiting exponential variation, it's very helpful to be able to undo exponential functions. Logarithms answer the question: \"To get $x$, what is the exponent I would have to raise the base $a$ to?\" Thus, just like an exponent requires specifying a base, so does a logarithm:\n",
    "\n",
    "$$f(x) = \\log_a(x); a\\gt0, a\\neq 1,$$\n",
    "\n",
    "Note that $a$ cannot be 1! The exremely important relationship between exponentials and logarithms is that they undo each other:\n",
    "\n",
    "$$a^{\\log_a{(x)}} = \\log_a{\\left(a^x\\right)} = x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.4.1 Rules for logarithmic functions\n",
    "Like exponentials, logarithms have their own (reverse) versions of the two rules. For logarithms, the _multiplication-to-addition_ rule:\n",
    "$$\\log_a{(xy)} = \\log_a{(x)} + \\log_a{(y)}$$\n",
    "\n",
    "lets you break down a difficult product into a sum of logarithms, while the _power-to-multiplication_ rule:\n",
    "\n",
    "$$\\log_a{\\left(x^c\\right)} = c\\log_a{(x)}$$\n",
    "\n",
    "lets you avoid taking an exponent by doing a multiplication outside of the logarithm, instead. This last rule and the exponential-logarithm undo property let us see something else that's important:\n",
    "\n",
    "$$ x = a^{\\log_a{(x)}} = \\log_a{\\left(a^x\\right)} = x\\log_a{(a)} \\Rightarrow \\log_a{(a)} = 1$$\n",
    "\n",
    "Finally, if we combine our special exponential identity with our logarithm rules, we get one more special identity:\n",
    "\n",
    "$$\\log_a{(1)} = \\log_a{\\left(a^0\\right)} = 0\\log_a{(a)} = 0,\\text{ for any } a \\gt 0, a\\neq1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.4.2 Computing logarithmic functions\n",
    "Taking arbitrary-base logarithms is also quite easy with the math module's function, `math.log(a,x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0000000000000002"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## take the logarithm base 10 of a list of numbers\n",
    "math.log(10, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.4.3 Changing logarithmic bases\n",
    "While it's once again extremely helpful to be able to perform a logarithm on a list of numbers—and `NumPy` can do this—there is no built-in way to take an arbitrary-base logarithm of a list of numbers. This is probably because changing a logarithm's base is as simple as multiplying by a constant:\n",
    "\n",
    "$$\\log_b{(x)} = \\frac{1}{\\log_a{(b)}}\\log_a{(x)}$$\n",
    "\n",
    "This makes it easy to use numpy's base-ten logarithm function: `np.log10(x)`, for just about anything we want:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.       0.30103  1.       0.69897] \n",
      "\n",
      "[-1.18329466  0.35620719  1.18329466  0.82708748]\n"
     ]
    }
   ],
   "source": [
    "##  find the log base 10 of a list of numbers\n",
    "print(np.log10([0.1, 2, 10, 5 ]), '\\n')\n",
    "\n",
    "## find the log base 7 of a list of numbers\n",
    "print(np.log10([0.1, 2, 10, 5 ]) / np.log10(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.5 Transcendental functions for data handling\n",
    "Transcendental functions&mdash;derived from logarithms/exponentiaton, mostly&mdash;are one of those math topics that often get tossed into a calculus course even though they are not specifically about calculus. Why? Because they are _really_ useful. They're really just an advanced algebra topic that require re-iteration and emphasis. So, just like in calculus we'll put our conversation about logarithms here. In fact, we'll get deeper into logarithms and use them more often on a day-to-day basis than any calculus concepts. Why? Because they give us _really_ useful tricks for handling data and performing computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.5.1 Using logarithms to perform problematic computations\n",
    "\n",
    "It's not an uncommon scenario in data science to have to multiply a _bunch_ of numbers together. Computers aren't perfect, and one of the things they have a hard time with is multiplication. However, it's much easier for a computer to do addition well. This makes logarithms/exponentials a big win! __The important thing to understand is that logarithms turn a multiplication job into an addition job, and can then be undone by an exponentiation__. So, supposing you have to multiply a bunch—$n$—of numbers, $[p_1, p_2, \\cdots, p_n]$, together:\n",
    "\n",
    "$$x = \\prod_{i=1}^np_i = p_1*p_2*\\cdots*p_n,$$\n",
    "\n",
    "doing this directly would probably cause Python to come up with the wrong answer, or break entirely. Instead, we could simply find the logarithm of $x$:\n",
    "\n",
    "$$\\log_a{(x)} = \\log_a{\\left(\\prod_{i=1}^np_i\\right)} = \\log_a{(p_1*p_2*\\cdots*p_n)},$$\n",
    "\n",
    "and use our awesome multiplication-to-addition rule:\n",
    "\n",
    "$$\\log_a{(p_1*p_2*\\cdots*p_n)} = \\log_a{(p_1)} + \\log_a{(p_2)} + \\cdots + \\log_a{(p_n)} = \\sum_{i=1}^n\\log_a{(p_i)},$$\n",
    "\n",
    "which Python would be _much_ more likely to succeed at. To find our answer, all we have to do is apply our friend, the exponential function, to un-logarithm our answer:\n",
    "\n",
    "$$x = a^{\\log_a{(x)}}.$$\n",
    "\n",
    "Putting this together, we have a recipe for problematic computations:\n",
    "\n",
    "1. Take logarithms of your data.\n",
    "2. Perform computations by using logarithm rules.\n",
    "3. Un-logarithm your result with an exponential function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.5.2 Example: calculation through logarithmic transformation\n",
    "Here's this process in an example. Suppose we want to take the geometric mean (we'll discuss what this _average_ is a bit more in __Chapter 5__) of the first 1000 numbers:\n",
    "$$\\overline{x}_g = \\sqrt[1000]{\\prod_{i=1}^{1000}i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    }
   ],
   "source": [
    "## make some data that we want to multiply, but can't\n",
    "nums = range(1, 1001)\n",
    "\n",
    "## let's see what happens when we try and multiply the\n",
    "## first 1000 integers together as floats\n",
    "## note: by starting x out as a float, all subsequent\n",
    "## operations will result in a float.\n",
    "## if we had done this with integers, \n",
    "## Python would use its arbitrary precision,\n",
    "## which is only limited by machine memory\n",
    "x = 1.\n",
    "\n",
    "## loop over the integers\n",
    "for num in nums:\n",
    "    ## multiply the result by the current integer\n",
    "    x *= num\n",
    "\n",
    "## take the 1000th root to complete the average\n",
    "print(x ** (1 / 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! We got `inf`. Guess we need to use our recipe. Let's see if we can work this out, noting:\n",
    "\n",
    "$$\\log_{10}{(\\overline{x}_g)} = \\frac{1}{1000}\\sum_{i=1}^{1000}\\log_{10}{(i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2567.6046442221304 \n",
      "\n",
      "369.4916634719579\n"
     ]
    }
   ],
   "source": [
    "## make some data that we want to multiply, \n",
    "## and take their logarithms\n",
    "log_nums = np.log10(range(1, 1001))\n",
    "\n",
    "## add  the logarithms up with sum()\n",
    "sum_log_nums = sum(log_nums)\n",
    "print(sum_log_nums, '\\n')\n",
    "\n",
    "## divide our log-sum by 1000 and \n",
    "## exponentiate base 10 to get a final answer \n",
    "print(10 ** (sum_log_nums / 1000.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.5.3 Exercise: functionalizing products by logarithmic summation\n",
    "Write a function called `safe_product()` that takes an arbitrary list of _positive_ numbers and computes their product by completing the sum of logarithms routine outlined above. If any of the numbers to be multiplied are zero, the function should detect this and just `return  0` immediately. For any of the numbers that are negative, they should be counted and negated before taking any logarithms. The final output should then `return product` if there were an even number of negative numbers, and `return -product` if there were an odd number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.0.5.4 Example: using logarithms to space wild data out for visualization\n",
    "Here's another wonderful property of logarithms and their ability to undo exponents. If data are distributed in an exponetial (as opposed to normal) way, it's quite difficult to make comparisons of data points and get a big picture. As an example, let's look at some bikeshare data and look at a histogram of the trip durations. Note: this example utilizes logarithms to improve visualization, which is covered in detail in __Chapter 3__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips:  276785\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcAAAAGPCAYAAADLKwDKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xm4HFWd//H3B0KACIEAAYEhBAQnMsyIclHDqMCwIwoiEVFnXEHGBRRXNkF0FFRABnCAcRB/g4CyiowhLBKNIJgbEQQJe9hkSSABYjZIvr8/zmmoFH3v7Zvb3bc79Xk9Tz9969Sp06e6+va369Q5pxQRmJmZVc0qw10BMzOz4eAAaGZmleQAaGZmleQAaGZmleQAaGZmleQAaGZmleQAaB1N0gmS5vSx7nxJvYXlj0oKSWs1WPbrc/nrNqu+KytJX5f0uKRlks7vI0/t/a89Fku6R9LRklYt5Buf1+9bSJsl6ftNrvMJpfrUe0wdoIyLJf2umfWyzjFiuCtg1kT/B0wEFjSY//XA8cD5wLwW1anrSeoBvgEcDUwFnh5gk38BFgJrAO8AvpnTv52fnyAdp5nNrmvJj4BrCsufy3V7byHt+QHKOBYY2eR6WYdwALSVRkTMBmYPdz36I0nA6hGxaLjrMggT8vNZETFQwACYHhHz899TJf0jsD85AEbEYuCW5ldzeRHxGPBYbVnSgcDiiBjwtSWtGRELI+L+VtbRhpebQG2lUa8JVNJRku6XtEjSU5KukfRaSTsDv8zZHsrbzSpst52kGyQtkDRX0k8lbVR6vXGSJktaKOmh/PqXFpvVak24kt4uaTqwCJgk6TWSzsxNhAvy9mdJGl16jZD0BUmnSHoml/WlvO4jkh6UNE/SeZLWWIH3bNVcx0dyk+Vdkj5YWH8+8L958blcn50H+TIvAKsVynxVE2idem0qaaak6yWNymlrSPqupEdzXW+XtM8g69LX6z0p6duSTpT0V/JZbrkJVNJhue5vknRz/lzNlPSuUnk75/UvSHpO0h8l7deMulrz+AzQuoKkep9VDbDNv5Ga7b4K3AWsT2oCew3wR+BLwPeBA0jNcovzdmNJTX13Ax8E1gJOAq6T1BMRS/KZ3FXAusDHSYHtOGAs8ECpKqOAnwDfBe4F/prTVgWOIZ21bpb/vgTYs7T9F0nNuwcD+wLfk7QhsANwODAOOC2XfVJ/70kdJwJfITVxTgfeB/xUUkTERaTmy0dJTYG1ps2/DFDmqvl4rQ68E3g/8L1GKyRpPHADqYn0fYWz5UuBt5CarR/I5V6Vj8mfGi2/Hx8D/gQcysDfjZcAZ5Hen38HrpD0xoi4W9L6pB9XPwO+TjrO/wSMaUIdrZkiwg8/OvYBnABEP4/eQt6P5rS18vKZwGX9lL1vzj++lH4S6Zrg6ELaW3Peg/Pyu/LyDoU8mwIvAlPr1H+/AfZzBPDPOe+4QnoANxaWVyEF67ml+v0cuHWQ7+16wN+A40vpvwLu6et97ae8Wr7y4zJgRCHf+Jy+byFtFunHyFbAI3mbkYX1u+Ztdiq95m+BSxrc3+8Ds/pY92R+3dVK6RcDvyssH5brcWQhbVXgIeD8vPx2YBmpqXvY/4f86PvhJlDrBs+RznbKj6sH2O5PwD6SviHpLcWeiAN4C3BtFK53RcStpC/pt+ekHYAnI2J6Ic/jwIw65QUwuZwo6V8l3SZpPilw1praXl/KekPhNZaRvmxnxPLX4+4nBeDB2JZ0JnpJKf1nwOvzmfCKeCfp/ZkIfIL04+G/G9ju70kB7XfAQRGxpLBuN1KQuknSiNqD9N70rGA9y66NiBcbzHtF7Y+IWEpqDXhLTrqX1CJwsaR3S1qnSfWzJnMTqHWDlyKit5wo6Rlg4362Ow9Ym9Sk9XXgGUlnk854lvaz3cakJtOyp0hnTQCvpX6Hm9n5NYvmlr7MkfRe4P8B/0Vqpn02v+4VpN6TReUeqkv6SBvsNcDae/dUKb22vB4r1qnotnilE8wtkuYBl0k6JSLu7Ge7HfNr/igiXiqt24D0ntcLUP0dy8Eovw/9KfeEfZr8fkbE05L2JH3mLgOQdA3wuYh4uBkVteZwALSVVj5bOg04TdJmwIeA/yD1DDy7n02fADask74Rr5zhPUm63lc2lvTrf7mq1Mk3idRk+elagqSd+qlTKzyRnzcEnimk1zr7PNuk17k7P78B6C8A/hgYDVwpabeI+ENh3bPA46TepK0ymHvDbUg6Ey8u195PImIasLuk1wC7kz6HPwF2Hno1rVncBGqVEBGPRsRJpKbCbXJy7aysfOZ0K7CnpJfP5CTtQLp2VWumnA68VtJbCnk2BbZvsEprkjvdFHyowW2b5U7SmMlJpfT3A/dGGlbSDNvm50cbyHsYqWl7stLwiZobSGeA8yOit/xoUj0H4+WxhLlp/T3AH8qZIuJvEXEl6Wx/m/J6G14+A7SVlqRzSGcOt5CuI+4CbE3qFQpwT37+lKSLgQUR8WfgVFLPvimSTuaVXqB/JjdpkTqK3A78XNJRpN6Rx5Oa0ZY1UL3rgLMkHUMKuPuQOnq0TUQ8K+kHwLGSXgJ6ST1i9yH1OF1RO0haSPp+eQOph2lvfgxUp2W59+5lwLWS3hFpLN51wBRST9yTSU3Uo4HtgDUi4qgh1HdFfFrSMtJn6DDg70i9fJF0APAB4Bek1obNSD2Ff93mOtoAHABtZfZ74BDgU6SzvPuBQ/IvciLi4Tym7nDSLCGPkXqEzpa0C3AKcBHpTPFXwBdq1/IiIvK4rnNITXdPkZpXD6SxmWjOAbYEjsh1u4405KLlA8RLvg68RAr4G5Heow9HxMVDKLP2Rb+U9J7+Evh6net6dUXES5Len7e7QdLbI+LRHFiOBj5PGvrxLKmj0xlDqOuKOog0DGI74GHScI3a8JB7Sd+tJ5OaxJ8mdZI5ehjqaf1QxGCavc2sL7m334PAmRFx/HDXx5pP0mGkjkurNRrQrXP5DNBsBeUvw2XAfaRf+keSBn+fN5z1MrPGtLUTjKRJkq5SmlV+vqQZkg4u5Zmq+rO2r1HKt6mkK/JUQ3OUppUaVec1D5F0X56yaIakV11naWZZVimLgC+QZmn5Mek64G7u6m7WHdraBCrp96Suw1cCc0gX278IHB4RZ+Q8U0m/qsvt5bdGrqyk1YDbSNdmjiNNR3UqcF1EfLjwegcDF5Bm4/gdaaqjSaTZO+5sdllmZtY92h0AN4iIOaW0C4GJEbFFXp4KzImIA/sppxaMtoqIh3La+0nTFv19RNyX0+4BboqIj+flVUg9926vBbdmlmVmZt2jrU2g5eCX3QZsMsii9ibdcqU4EPVK0lncXgCStiRNKfXzwusvI037tHeLyjIzsy7RCZ1gJpK6DRftIanWlXwa8OWIuKOwfgKlGekjzdD/AK/cu6z2XL7p5t3AepLG5oG+zSyrTxtssEGMHz++vyxmZlYyY8aMORGxovPS9mtYA2DuRLI/aZBozW9IUwbdD2xOukXMtHyrkVk5zxjq38F7Lq/ccqT2XM43t7B+dpPLWo6kQ0nzUDJu3Dh6e4djwgozs+4lqWWdyoZtKrR8z68LgV9ExPm19Ig4PiJ+HBHTIuIC0uwdQRr82lUi4tyI6ImInrFjW/IDxszMVtCwBEBJ65FuD/MwA8x/GBFPAjcBby4kzwXq3WJkDK+cldWey/nGlNY3sywzM+sSbQ+AeXzd1cBI0g0xG5k2qnZjzZqZvHJdrlbuSNLUUjMLeSjny8vPFq7ZNbMsMzPrEu0eCD+C1HNya2CviCjfU6veNq8l3YS0eKPRyaQJdzcvpL2HNAvHNQAR8SCpc83LM93noQuTWP7mpM0sy8zMukS7O8H8kDT4/QhgfUnrF9bdRroj9HdIQfJh0oS3R5EGxv+gkPdSUueYyyUdR2qaPA24sDZuLzsBuEDSLFIz6kdIwfeDLSrLzMy6RLsD4B75+fQ667Yg3ZRTpCC4PvACMBXYPyIeqWWMiBcl7QWcSRqbt5g0cP3LxQIj4iJJa5Fuf3Mc6RYq+xZnbmlmWWZm1j18N4g26enpCQ+DMDMbHEkzIqKnFWX7jvBmZlZJDoBmZlZJDoBmZlZJDoBmZlZJDoBmZlZJDoBmZlZJnXA7JGuApLrpHsZiZrZifAZoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV5ABoZmaV1NYAKGmSpKskPS5pvqQZkg6uk+8QSfdJWpTz7Fonz6aSrpD0gqQ5ks6UNGq4yzIzs+7Q7jPAI4H5wBeA9wA3AhdK+lwtQw6IZwP/D9gbuAu4WtK2hTyrAVOAzYEPAEcAk4Bziy/W7rLMzKx7KCLa92LSBhExp5R2ITAxIrbIy/cAN0XEx/PyKsDtwO0R8eGcdjBwAbBVRDyU094PXAz8fUTcNxxl9aenpyd6e3tX5G2rvU9109t5/MzM2k3SjIjoaUXZbT0DLAe/7DZgEwBJWwKvB35e2GYZcAnprKtmb2B6LWBlVwJLgL2GsSwzM+sSndAJZiJwb/57Qn6eWcpzN7CepLGFfMvliYglwAOFMoajLDMz6xLDGgBzJ5L9gVNy0pj8PK+UdW5p/Zg6eWr5xpTytrOs5Ug6VFKvpN7Zs2fXy2JmZsNk2AKgpPHAhcAvIuL84apHK0XEuRHRExE9Y8f6JNHMrJMMSwCUtB4wGXgY+FBhVe2Map3SJmNK6+fWyVPLN7eUt51lmZlZl2h7AMzj664GRgL7RsSCwuraNbYJpc0mAM9GxOxCvuXySBoJbFkoYzjKMjOzLtHugfAjSD0ntwb2ioini+sj4kFSh5hJhW1WycuTC1knAztI2ryQ9h5gdeCaYSzLzMy6xIg2v94PgX1Ig83Xl7R+Yd1tEbEYOAG4QNIs4CbgI6SA+cFC3kuBY4DLJR1Hapo8DbiwNm4va3dZZmbWJdodAPfIz6fXWbcFMCsiLpK0FvBV4DjSjCv7RsSdtYwR8aKkvYAzSWPzFpMGrn+5WGC7yzIzs+7R1plgqswzwZiZDd5KMxOMmZlZp3AANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzSnIANDOzShpSAJS0ZrMqYmZm1k4NBUBJn5B0ZGF5W0kPAvMl3SRp45bV0MzMrAUaPQP8ArCosHwGMA/4BDAa+E6T62VmZtZSIxrMtzlwN4CkDYB3AHtGxA2S5gOnt6h+ZmZmLdHoGeCLwMj8986ks8Hf5OXZwJjmVsvMzKy1Gg2AvcChkl4HfAaYEhEv5XVbAE+0onJmZmat0mgT6JeA/wPuIwW73Qvr3g/8vsn1MjMza6mGAmBE3CFpHLAJ8FTh7A/geODxVlTOzMysVRo9AyQighzoJK0dES/k9OktqpuZmVnLNDwQXtK/SPq1pBeAeZJeyMu7trB+ZmZmLdHoQPiPAdcBqwLHAf+an0cA1+b1ZmZmXaPRJtCvA+dFxCGl9B9I+h/SdcAfN7VmZmZmLdRoE+iGwM/7WPczYGxzqmNmZtYejQbA3wI79rFuR+Cm5lTHzMysPRptAj0ZOF/SusCVwNOks8L3AvsDH5W0ZS1zRDzY7IqamZk1U6MB8Nf5+Qjg8EK68vMNheUgdZYxMzPrWI0GwL1bWgszM7M2a3QmmCmtroiZmVk7DemO8GZmZt2qzzNASY8A746I2yU9Srq216eIGNfsypmZmbVKf02gPwXmFP7uNwCamZl1kz4DYEQcBSBJwPeAhRGxoF0VMzMza6VGrgGOAJ4k3QnezMxspTBgAIyIF4FHgZHNeEFJW0k6R9IdkpZKmlonzyxJUXo8WSffNpJukLRA0l8lnShp1VIeSTpa0qOSFkr6raTtWlmWmZl1vkZ7gZ4CfC3PBDNU/wDsA9wD3NtPvguBiYXHPsWVksYA15OuTe4HnAh8EfhGqZyvke5ccTLwbmA+cL2k17aiLDMz6w6NDoR/GzAOeETSLcBTLN8pJiLiIw2W9cuI+AWApEuBDfrI90RE3NJPOYcBawIHRMTzwHWSRgMnSPpuRDwvaQ1S0PpORJyZX/P3wCzgs8CxLSjLzMy6QKNngFsBDwN3AWvn5a1Lj4ZExLJB1rEvewNTcsCquZgUyHbKyzsCoyncySIi/gb8kuVnt2lmWWZm1gUaCoARMXGAR193ihiKT0haIuk5SZdK2ry0fgIws1TPR4AFeV0tz1LgvtK2dxfyNLssMzPrAn0GQEkPSnpjOytT8Avg08CuwJdJ1wCnSVqnkGcMMK/OtnPzulqe+RGxtE6eUZJGFvI1q6yXSTpUUq+k3tmzZ9cp3szMhkt/Z4DjgdXbVI/lRMQREXFRREyLiHOBPYFNgI8NR31WVEScGxE9EdEzdqzvGWxm1km6Yi7QiLiT1Gv0zYXkucA6dbKPyetqedYqD2fIeRZExJIWlGVmZl1goADYSdOfBcvXZyala2+SNgNG8cr1vJmkexNuVSqrfM2vmWWZmVkXGCgA3pIHqw/4aGUlJW1LCjQzCsmTgT0lrV1IOwhYCPwmL98MPA9MKpQ1ijSGb3KLyjIzsy4w0DjAU0nj3JomB43aoPZNgdGSDszLvwJ2AT4MXA38lRT4jgUeAc4vFHU26e70l0s6GdgSOAE4tTacISIWSToJOE7SXNKZ2pGkwH9Gi8oyM7MuMFAAvCQi/tDk19wQuKT8Ovl5C9K0axsCPwDWBZ4BrgGOLo7Ti4i5knYFziSNxZsHnEYKXEUnkYLUUcD6QC+we0Q81YqyzMysOyii/mU+ScuAt7UgAFZST09P9Pb2rvD26aYcr9bX8TMzWxlImhERPa0ouyt6gZqZmTVbfwHwY8AD7aqImZlZO/V3Q9yftLMiZmZm7eQmUDMzqyQHQDMzq6T+JsMeJ2m1dlbGzMysXfo7A3wIeBOApF9L8i1/zMxspdFfAFxImgsTYGfSzWDNzMxWCv3NBHMbcLqk6/Ly5yQ90UfeiIivNrdqZmZmrdNfADwE+B6wH+kuDLsCi/vIG4ADoJmZdY3+xgHOJN3poDYt2v6eFs3MzFYWA02GXbMF0Ffzp5mZWddpKABGxMOSRkg6CHg7sB7wLDANuDwiXmphHc3MzJquoQAoaUPgWuCfSPcHfAqYCHwGuF3SHhExu1WVNDMza7ZGZ4I5lXT/u7dFxJYRMTEitgTemtNPbVUFzczMWqHRALgP8NVyJ5iImE66Oey7ml0xMzOzVmo0AK4OvNDHuheAkc2pjpmZWXs0GgBvAb4q6TXFxLz81bzezMysazQ6DOKLwI3Ao5KuJXWC2RDYExBpqjQzM7Ou0dAZYET8CdgaOBcYC+xOCoBnA1tHxO0tq6GZmVkLNHoGSETMAb7WwrqYmZm1jW+Ia2ZmleQAaGZmleQAaGZmleQAaGZmlTRgAJS0uqRjJL2xHRUyMzNrhwEDYEQsBo4B1m19dczMzNqj0SbQW4E3t7IiZmZm7dToOMCvABdKehH4FWkmmChmiIgFTa6bmZlZyzQaAG/Nz/8JnN5HnlWHXh0zM7P2aDQAfpzSGZ+ZmVk3aygARsT5La6HmZlZWzU8FyiApG2A7YHNgPMi4klJWwFPRURf9ws0MzPrOA0FQElrAecBBwIv5u2uAZ4Evg08AnypRXU0MzNrukaHQZwK7AjsCqxNugdgza+AvZpcLzMzs5ZqtAn0AOCIiLhRUrm358PA5s2tlpmZWWs1ega4JvBMH+vWBpY2pzpmZmbt0WgAnA78Wx/rDgRubk51zMzM2qPRJtDjgOskXQ9cQhoTuI+kL5AC4DtbVD8zM7OWaOgMMCKmkTrArA6cSeoE8w1gS2C3iJjeshqamZm1QMPjACPiJuAdktYExgDzPP+nmZl1qxW5Ie4i0ljAhU2ui5mZWds0HAAl7SPpZlIAfBJYJOlmSe9qWe3MzMxapKEAKOlTwC+B+cARwKT8PB+4Kq83MzPrGo1eAzwaOCciPl1KP1vS2aQ7xp/T1JqZmZm1UKNNoOsDV/Sx7jJgveZUx8zMrD0aDYA3Ajv1sW4n4LfNqY6ZmVl79NkEmm99VPOfwI8krQ9cCTwNbAi8F9gb+GQrK2lmZtZs/V0DvJPl7wIv4FP5ESx/R4hrgPIk2WZmZh2rvwC4S9tqYWZm1mZ9BsCI+E07K2JmZtZODU+FViNpBDCynO5p0czMrJs0OhB+HUk/lPQEaSaYF+o8GiJpK0nnSLpD0lJJU+vkkaSjJT0qaaGk30rark6+bSTdIGmBpL9KOrF8w97hKMvMzDpfo2eA55OGO/w3cD+wZAiv+Q/APsAtwGp95Pka6RZMXwZmAkcC10vaNiKeBJA0Brge+AuwH/A64BRSUD92uMoyM7MuEREDPoDngYMbydtAWasU/r4UmFpavwbwHPD1QtprgNnAtwppRwFzgdGFtK8AC2ppw1FWX4/tt98+hoLU8/ZVDzOzlRnQG02IPfUejQ6EfyQHgyGLiGUDZNkRGA38vLDN30hzke5dyLc3MCUini+kXQysySuD9oejLDMz6wKNBsCvAMdKGtfKymQTgKXAfaX0u/O6Yr6ZxQwRUQvUEwp52l2WmZl1gYauAUbEryTtBtwvaRYwr06etzSpTmOA+RGxtJQ+FxglaWRELMn5XlWPnG/MMJb1MkmHAocCjBvXjt8OZmbWqIYCoKTvA58HpjP0TjCVERHnAucC9PT0xADZzcysjRrtBfpJ4JiI+E4rK5PNBdaStGrpbGsMsKBwljUXWKfO9mPyuuEqy8zMukCj1wAXADNaWZGCmaR5RbcqpZev082kdO1N0mbAqEK+4SjLzMy6QKMB8HTgUEkaMOfQ3UwadjGpliBpFPBuYHIh32RgT0lrF9IOAhYCtWnchqMsMzPrAo02gW4AvBW4J8/cUu4wEhHx1UYKykFjn7y4KTBa0oF5+VcRsUDSScBxkubyyoDzVYAzCkWdDRwOXC7pZGBL4ATg1NpwhohYNAxlmZlZF2g0AB4IvESauWX3OusDaCgAku4jeEkprba8BTALOIkUWI4i3Y2+F9g9Ip56+QUj5kraFTiTNBZvHnAaKXAVtbUsMzPrDkoD7a3Venp6ore3d4W376v12cfPzFZmkmZERE8rym70GqCZmdlKpdFxgJ8eKE9E/HDo1TEzM2uPRq8BntnPulobnAOgmZl1jYaaQCNilfIDWA84GLgd2KaVlTQzM2u2Qd8RviYi5gE/k7QOcA6wc7MqZWZm1mrN6ATzENCSHjpmZmatMqQAKGlj4IukIGhmZtY1Gu0FOptXOrvUjATWBhYBBzS5XmZmZi3V6DXAs3h1AFwEPAZcExHPNLVWZmZmLdboDXFPaHE9zMzM2sozwZiZWSX1eQYo6deDKCciYtcm1MfMzKwt+msCbeS63sbAjrz6+qCZmVlH6zMARsSkvtZJGke6/dG+wBzSrYPMzMy6xqBmgpG0FeleeB8Gns5/nxMRC1tQNzMzs5ZpdBzgPwDHAJOAR4EjgPMiYkkL62ZmZtYy/fYClbS9pMuBO4A3A58Eto6Isx38zMysm/XXC3QysAfwZ+ADEXFJ22plZmbWYv01ge6Zn/8OOEvSWf0VFBEbNq1WZmZmLdZfAPxG22phZmbWZv0Ng3AANDOzlZanQjMzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0pyADQzs0rqyAAo6aOSos7jsEIeSTpa0qOSFkr6raTt6pS1jaQbJC2Q9FdJJ0patZSnaWWZmVl3GDHcFRjAvwALC8sPFv7+GnAc8GVgJnAkcL2kbSPiSQBJY4Drgb8A+wGvA04hBf5jW1SWmZl1gU4PgNMjYn45UdIapKD1nYg4M6f9HpgFfJZXAtJhwJrAARHxPHCdpNHACZK+GxHPN7Os5u++mZm1Skc2gTZgR2A08PNaQkT8DfglsHch397AlFJwupgUyHZqQVlmZtYlOj0APiDpJUn3SPpUIX0CsBS4r5T/7ryumG9mMUNEPAIsKORrZllmZtYlOrUJ9AnSNbk/AKsCHwDOljQqIk4DxgDzI2Jpabu5wChJIyNiSc43r075c/M6mlyWmZl1iY4MgBExBZhSSJqcr9UdK+n0YarWoEk6FDgUYNy4ccNcGzMzK+r0JtCiS4H1gPGks6616gxBGAMsyGds5Hzr1ClrTF5Xy9OsspYTEedGRE9E9IwdO7bPHTMzs/brpgAYheeZpKbRrUp5ytfpZlK6PidpM2BUIV8zyzIzsy7RTQHwQGAO8DBwM/A8MKm2UtIo4N3A5MI2k4E9Ja1dSDuINLbwN3m5mWWZmVmX6MhrgJIuI3WAuYN0dnZQfhweEcuARZJOAo6TNJdXBq+vApxRKOps4HDgckknA1sCJwCn1oYzRETTyjIzs+7RkQEQuAf4OLAZINLsK/8WEf9byHMSKUgdBawP9AK7R8RTtQwRMVfSrsCZpHF984DTSIGLFpVlZmZdQBG+AJdNAAASA0lEQVQxcC4bsp6enujt7V3h7SXVTffxM7OVmaQZEdHTirK76RqgmZlZ0zgAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJTkAmplZJY0Y7grY0Eiqmx4Rba6JmVl38RmgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlVkgOgmZlV0ojhroC1hqS66RHR5pqYmXUmnwGamVklOQAOkqRtJN0gaYGkv0o6UdKqw10vMzMbHDeBDoKkMcD1wF+A/YDXAaeQfkgcO4xVMzOzQXIAHJzDgDWBAyLieeA6SaOBEyR9N6eZmVkXcBPo4OwNTCkFuotJQXGn4anS4Eiq+zAzqxoHwMGZAMwsJkTEI8CCvK5rOTCaWdW4CXRwxgDz6qTPzetWOs0Kgh5+YWadxgGwhSQdChyaF+dLumcFi9oAmNOcWg2POoG06/epDu9Td/A+dYfaPm3eqhdwABycucA6ddLH5HXLiYhzgXOH+qKSeiOiZ6jldBLvU3fwPnUH79OK8TXAwZlJ6VqfpM2AUZSuDZqZWWdzABycycCektYupB0ELAR+MzxVMjOzFeEAODhnA4uByyXtlq/xnQCc2uIxgENuRu1A3qfu4H3qDt6nFSD3zhscSdsAZwITST1CfwScEBFLh7ViZmY2KA6AZmZWSW4C7WDdMvG2pEmSrpL0uKT5kmZIOriUZ6qkqPNYo5RvU0lXSHpB0hxJZ0oa1d49Akkf7aO+hxXySNLRkh6VtFDSbyVtV6esjjiO/RyDkDQx55lVZ92TnbJPkraSdI6kOyQtlTS1Tp6mHZdGy2rV/kjaWNL3JN2e/7celfQTSZuU8u3cx3E9qc5rHiLpPkmL8v/qrs3an0b2Kedp2udsKMfIwyA6lLpr4u0jgYeAL5DG7ewDXChpg4g4o5DvRuDo0raLa39IWg2YAiwBPgCsC5yanz/cstr3719InZxqHiz8/TXgOODLpF7ARwLXS9o2Ip6EjjuOnwZGl9JOBN4ETC+kXQgUj9uS4gbDvE//QPp83QKs1keeZh6XActq8f5sD7yXdKnlVmAjUr+Dm3Md5pfyf4jlP6OPF1cq/TA9O5fxO+BjwNWSdoiIO4e6M1kjxwia9zlb8WMUEX504AM4ijS2cHQh7SukaddGD1e9+qjrBnXSLgQeKixPBS4doJyDgaXAFoW09wPLgK3bvE8fBQJYq4/1awDPAV8vpL0GmA18qxuOIzASeBb4r0LaLOD7A2w3bPsErFL4+1JgaquOS6NltXh/1gVGlNJenz+bHymk7ZzTth3g9e4Bziu+PvBn4IJ2HaNmfs6GeozcBNq5umbi7YioNwPFbcAmddL7szcwPSIeKqRdSfpluNcKVq9VdiSdTf28lhARfwN+SdqPmk4+jnuRJnG4aJDbDds+RcSyAbI087g0WtYKG2h/ImJeRLxUSruXFAQG9f8laUtS8CzuzzLgEpq0P4Uym6Hlx8gBsHN1+8TbE4F7S2l75Lb8BZKmSPqn0vp6+7wEeIDh2+cHJL0k6R5JnyqkTyCdrd5Xyn83y9e1k4/jB4DHgGml9E9IWiLpOUmXSipPRdXJ+9TM49JoWW2V/29G8er/L4Bf5+tusyQdW7peVqtzedKOu4H1JI1tQXX704zP2ZCOka8Bdq6unXg7X1TfH/h4Ifk3wE+A+0lz+x0DTJP0xoiYlfN00j4/Qbqu8AdgVVKwOFvSqIg4Lddnfrx6+MtcYJSkkTl4d9I+vUypY9F7gHMitxtlvyBdu3kMeANwPOk4/WNEPJfzdOQ+Zc08Lo2W1TaSVgFOJ33hX1VY9RxwEunHzBJgX+AbwFjgiJyntl/l/Z5bWD+7+bWuq1mfsyEdIwdAaypJ40nX/34REefX0iPi+EK2aZKuJ/26+3x+dJSImELqkFMzWanH6rGSTh+majXTu0nXSpZr/oyIIwqL0yTdDPyJ1FniB+2rnvXhO6TWlZ0i4sVaYkTcRrrsUHO9pMXAkZK+2cdlimHTKZ8zN4F2rkFNvN0JJK1Hmi7uYVJvtD5F6p11E/DmQnKn7/OlwHrAeFJ91ip3ySbVdUHhV2en7tMHgPsjore/TJF6Bt5D9xynZh6XRstqC0mfJvV0/EhE3NrAJpeSTnJqlxpq+1Xe7zGl9W03hM/ZkI6RA2Dn6qqJt3OT2tWknoX7RsSCBjaL/Kipt88jgS3pjH2OwvNMUtPoVqU85esWHXccJa1D6iDQaOeXRo5Tp3w2m3lcGi2r5SS9jzRk4CsR8bMGN4vSc63O5WtjE4BnI6JdzZ99WZHP2ZCOkQNg5+qaibcljSD1JNsa2Csinm5gm9cCbwdmFJInAzuULoa/B1gduKZ5NV5hB5LGOT4M3Aw8D0yqrcw/At5N2o+aTjyO7yW9pwMGQEnbkr5Mysep0/apppnHpdGyWkrSzsBPgTMi4vuD2PRA4CXgDoCIeJDUcaa4P6vk5bbtTz1D+JwN7Rg1a+yHH819kE7hnwCuA3Yj3Vh3Pk0af9Tkup5L+uV2OPC20mN1UhPM/5HG1u0CfIT06+xZYFyhnNWAO/M/wT6kcYFP0sQxSoPYp8uAr5LOlPYF/jfv4+cKeY4i9Uj7DLBr3sc5wEadfBxJPyb+VCf9XaSg+KF8nP6dNJD6QZYfizVs+0T69X9gfvweuKuwPKrZx6WRslq5P6QOIvNI18cmlv63Xlco579Ikxq8G9iT1FFmKXBK6fVqY22Pzcf4fFJA6Xf8YJP3qamfs6Eco2H5B/Sj4Q/SNsCv8wf0CeCbwKrDXa869ZzFK80X5cd4YFPgV3kflgDPkALMhDpl/R1p7N/8nO+s2hdbm/fp26RrEgvy+z8D+NdSHpF6sz6W80wD3tTJx5F0l+0Xga/VWfdPwA2knoAvkn58nA9s0in7lD9PfX7Wmn1cGi2rVfvDKxMy1HucXyjncNKZ3guk2ZXuInUuU53XPITUG3sx8Edg13Yeo2Z/zoZyjDwZtpmZVZKvAZqZWSU5AJqZWSU5AJqZWSU5AJqZWSU5AJqZWSU5AJqZWSU5ANqwk3SCpMiPZZLmSpou6T/yjDHtrs+GuU7jS+k75zpu2+469UfSGyRNk/S3XL/xpfUfLby/fT1mDfAaJ0l6rIl1Hi9pfp7aaqhlNbVupbI3yZ+Fv2tB2bX3oOllW2M8DtCGnaQTSIN2aze9XYc0Ke6/k25+uVdEzKi/dUvqsy3pLtm7RMTUQvpo0sDc2yNiYbvqMxBJV5NuMfV54G/AbRGxuLB+LPC6wiYHAl8kzSxSszjSHQX6eo3NgA36yzPIOv8YICI+1oSymlq3Utk9wHRgYkTc0oLyfwq8EBGHNbtsG5hvh2Sd4qXSF8wUSf8F/Ba4WNKEePU9vxqWZ4tfNYYwg3+kO1M3/UuwCSYAV0XEDfVWRprk+OWJjvOXOo18oefJyF+KiEeBR5tRWUnrAx8kTW81ZM2s2zD4MXCVpK/E8nc+tzZwE6h1rIiYB3yFNNP77tB3M6SkqZIuLSyfL6lX0v6S7gIWAW+VtLGk8yQ9KGmhpHslfSt/0dfuZ/jnXMyNtSbCvl5b0ihJ/ynpSUmLctPtHvXqJumDku6X9LykyY00fUnaTtINkhbkpuGfStqoVtdct9cBX8h1mzqY97jO690i6QJJn5X0EGlqqfXLzYyS9sqvt4uka3L9Zkn6eN+lv+xg0jywvyuUNyGX9778+i9IekTSQXn9MZKekPS0pG9KUmHbvur2z5KuyE3DD0j6ZL19LaXVtt1K0gTS2R/A73P6okLesZL+J9dpoVIz9Pal8g6TdHdeP0fSjZJeX8hyY36PJ2Ft5wBonW4qaUb7t63AtuOB75JuIro38BBpLsxngSNJTa7fI92E84y8zRO8ci/Dz5CaCYtNhWX/nbf/D9JdFh4F/k/S20v53gp8ltT0eCipiffc/iqfmy6nkiYQ/iDwOWAn4LocsJ/IdXuSdBPiicCn+yuzQbsC/5bruh9pPtS+/AS4lbTvNwD/I2mgM7tdgVui/vWXU4AHgANIwecCSacC25LmxfwhaSLn/RrYj/Ny3fYnTcr835Le2MB2NbNIxxbgk6T3950AktYkBa93kj5LB5Dm4bxB0gY5zx7Af+Z67AV8Iu/T6NoL5FaNP9Cks2EbHDeBWkeLiEWS5gAbrcDm6wO7RcSfCmmPAV+qLUi6iXTd7DxJn4uIxZLuyKv/0l8zoaQ3kM5mPhYRP8lpU0iTEh9HmpW/ZjTwroiYm/O9FjhN0pr9XE/8Yn7es9Y8Juk+UjPs+yLiIuAWpTt/P9HEa1RrA3tHxDOFfe0r75URcXz+e4qkrUkB6vp+yt+eFDjrmVwrT9IfSYF1D+Afc8CcIumAnH7lAPvxk4g4KZc1jXRXj/cCtw+wHfDyZ+/OvHhX6f39OOnM+w0RMSu/xq9Jk0wfQTr+bwGmR8T3Ctv9os5L3Y7PAIeFzwCtG/T57TuAx0vBDyWfl/QXSQtJs9H/lHTbpnGDLH+HXLdLagkRsSwvl88Ap9eCX/aX/LxpP+W/Bbi2eG0o0p3AZ9Upv5luKQa/AVxRZ3mHAbZ5Lel2NfW8fB0z12EeMLV0tng//b9vNdcWylpEut1Os3pc7kY6u3xM0gile2IuJd2JoCfn+ROp2f37kt4uabU+yppDek+szRwAraNJWoN0JvfUCmxeb5vPA98nfVHvRwoyn8nr1hhk+RsD8yOi3ET4FDBK0uqFtHmlPLXOOP295sbU34engPUGU9FBGsx7Xb758dOkfV+nXmalzkirkW7FU0+996leWiPHakW3a8QGpOboF0uPg4HNACLiauAwUpPvNGC2pNNz82nRYtIPMGszN4Fap9uF9Dn9fV6udUIYWco3hlefVdS7xjQJuDQijqklSNpmBev2BLCWpFGlILgRsKA4FGEI5W9YJ30jlr9zdrMNZmzUhqR7zxWXF0TEc3ULjlgq6Tlg3SHUr1kWUf9z1IhngZtIP6jKXm7SjogfAT/KHZcOJF3jnAucUMi/bk6zNvMZoHUsSesCJ5OavGrXlGq9/d5QyLcZaShAI9bk1WcfHyotN3J2BqlDQ5C+2Gp1UV7+XV8bDcKtwJ6S1i6UvwOpc08zym+G99ZZnl4vY8E9wBatqc6gPEbhc5TtUVru67NwA/D3wIMR0Vt63FXKS0Q8FRFnkY5p+QfXeODeFdkBGxqfAVqnGCGp1tNzbVJHiX8n9YDcqzYGMCIek9QLfFPSAtKPuKNJv8gbcR1wuKRbSb0NP0QaZlH0COlX/Efy2cqLEdFbLigi7pZ0EXBmDlIPkO62PSHXfahOzeVMkXQysBZwEmmYxmVNKL8Z9pc0F7gZeD/wDpbv/FPPTeTelMPsCuBD+b29jjTUZpdSnodIQfBjubPR4oj4I/Aj0rGemnup1noYTwQeioizJH2HFDinAc+Qro1OJN29vagHmNKC/bMB+AzQOsU6pGbOm0mdSA4ELiD1/is39x1MClIXAN8GTiSdVTTiROAi4Fv5eQmlL6TcYeIQUhD+Df2f0RxC6tH4dVIPv82BfSNiyGdoeQD7LqSmuouAs0hfprsPZUB/k30U2JHUI3N34JCIuLbfLeBy4E25WXA4XQ4cT/oRdDmp+fZLxQwR8QLpOt4/kyZluDmnLyBdA5xGGgJzHfAD0vGvfV7+AGwHnANcQxpKcVREnF0rX9KmwD/SOT9oKsVToZnZoEnaC5gMbB0R96/A9jOBsyLijAEzr8QkHUH60dBR88tWhc8AzWw4/AfwWUmV/Q7KPWI/R2qNsGHga4BmNhx+ShousDHw+DDXZbhsQrqW+LPhrkhVuQnUzMwqqbLND2ZmVm0OgGZmVkkOgGZmVkkOgGZmVkkOgGZmVkkOgGZmVkn/H+KgJWQXNa6oAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as  pd\n",
    "\n",
    "bike_data = pd.read_csv(\"./data/indego-trips-2017-q3.csv\", parse_dates = [0])\n",
    "\n",
    "print(\"Number of trips: \", len(bike_data))\n",
    "\n",
    "fig = plt.figure(figsize = (6,6))\n",
    "\n",
    "_ = plt.hist(bike_data[\"duration\"], bins = 50, color = \"black\")\n",
    "\n",
    "_ = plt.ylabel(\"Number of Trips\", fontsize = 15)\n",
    "\n",
    "_ = plt.xlabel(\"Duration of Trip (minutes)\", fontsize = 15)\n",
    "\n",
    "_ = plt.title(\"Histogram  of Bike Trips\", fontsize = 15)\n",
    "\n",
    "plt.tick_params(labelsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, where's all the data? Even though there were nearly 300k bike trips, it's hard to see any variation. Just about all of the trips appear to be short, while a few must have been long—up to 1,500 minutes. But how many trips were this long relative to the number of short trips? This is where a logarithmic scale comes in super handy. A _logarithmic scale_ shows us logarithm-transformed data, which spaces wild, potentially exponential values out. Let's start by taking the logarithm of the y-axis variables. Matplotlib has a built-in function for this. Note: it's customary to view a logarithmic scale with base 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (6,6))\n",
    "\n",
    "_ = plt.hist(bike_data[\"duration\"], bins = 50, color = \"black\")\n",
    "\n",
    "_ = plt.ylabel(\"Number of Trips\", fontsize = 15)\n",
    "\n",
    "_ = plt.xlabel(\"Duration of Trip (minutes)\", fontsize = 15)\n",
    "\n",
    "_ = plt.title(\"Histogram  of Bike Trips\", fontsize = 15)\n",
    "\n",
    "plt.tick_params(labelsize = 15)\n",
    "\n",
    "_ = plt.yscale('log', nonposy = 'clip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see a little bit of what's going on. With the logarithm of the y-axis taken we can see things like a spike, right at the largest durations. What's this about? Why are their so many trips at the end with such a long duration? (A day is $1,440$ minutes long). To dig in a little further, let's see what happens when we take the logarithm of the x-axis, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAGTCAYAAAALNTQlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XucHFWZ//HPVwKYYBIiEGQxyCW6gHcdYFEUERCJZsEAsuqu642IruKNhUVRI7sqoIAXWCGiwu8nCy6IlygQIBJENJhBLgpEQYghXAOEAUzCJXn2j3MaO033TM1M1fT09Pf9evVrpk6dqn6muqefPpeqUkRgZmZWlme1OwAzMxtbnFjMzKxUTixmZlYqJxYzMyuVE4uZmZXKicXMzErlxGKFSJoj6YEW686S1Fu3/B5JIek5Bff9orz/TcuKd6yS9DlJd0laJ+msFnVqx7/2eFzSHyV9WtIGdfW2zevfWle2VNJXS455TkM8zR4LB9jHeZJ+VWZcVp1x7Q7AxqSfA7sDqwrWfxHweeAs4OGKYup4knqALwCfBhYC9w+wyRuB1cCzgdcB/5nLv5R/3kN6nZaUHWuDM4FL6pY/mmN7W13ZIwPs41hgo5Ljsoo4sVjpImIFsKLdcfRHkoCNI2JNu2MZhB3zz9MiYqAPYoDFEfFY/n2hpJcCB5ITS0Q8DiwqP8z1RcRyYHltWdLBwOMRMeBzSxofEasj4rYqY7RyuSvMStesK0zSMZJuk7RG0n2SLpH0PElvAOblanfk7ZbWbfcKSQskrZK0UtI5krZseL5tJF0sabWkO/LzX1DfvVLrypO0h6TFwBrgEEmbSDo1dxWtytufJmlSw3OEpE9IOknSg3lfR+Z1/yrpdkkPS/qupGcP4ZhtkGNclruubpL0zrr1ZwH/Py/25XjeMMineRTYsG6fz+gKaxLX1pKWSLpc0oRc9mxJJ0q6M8d6g6QZg4yl1fPdK+lLko6TdDe5VdbYFSbp8Bz7KyX9Or+vlkh6S8P+3pDXPyqpT9LvJB1QRqzWmlssNiiSmr1nNMA27yZ13xwN3ARsRuoK2QT4HXAk8FVgFql75vG83RakLp9bgHcCzwGOBy6T1BMRT+SWx0+BTYH3kRLGZ4EtgD83hDIBOBs4EfgTcHcu2wD4DKmVNS3/fj6wX8P2nyJ1870DeCvwFUlTgV2AI4BtgFPyvo/v75g0cRxwFKmrazFwEHCOpIiIc0ndWHeSuoRqXVw3D7DPDfLrtTHweuDtwFeKBiRpW2ABqavsoLrW3QXArqTuyz/n/f40vybXF91/P94LXA/MZuDPqPOB00jH50PAjyS9PCJukbQZ6UvLD4DPkV7nlwFTSojR+hMRfvgx4AOYA0Q/j966uu/JZc/Jy6cCP+xn32/N9bdtKD+eNOYyqa5st1z3HXn5LXl5l7o6WwNPAgubxH/AAH/nOOC1ue42deUBXFG3/CxSElzZEN//AtcM8tg+F/gr8PmG8ouAP7Y6rv3sr1av8fFDYFxdvW1z+VvrypaSkvx0YFneZqO69XvnbfZseM5fAucX/Hu/Cixtse7e/LwbNpSfB/yqbvnwHMcn68o2AO4AzsrLewDrSF2ebf8f6qaHu8JsMPpI384bHz8bYLvrgRmSviBp1/qZSQPYFbg06sYTIuIa0offHrloF+DeiFhcV+cu4Nom+wvg4sZCSf8i6TpJj5ESUq3L5UUNVRfUPcc60ofYtbH+eMdtpMQ2GC8htZzObyj/AfCi3HIbiteTjs/uwPtJSfnbBbb7e1Ki+BVwaEQ8UbduH9KH/9WSxtUepGPTM8Q4G10aEU8WrPuj2i8RsZbUet01F/2J1II9T9JMSZNLis8G4K4wG4ynIqK3sVDSg8BW/Wz3XWAiqWvjc8CDkk4nfUNf2892W5G6zhrdR/qWD/A8mk8UWJGfs97Khg9JJL0N+H/At0jddQ/l5/0RaTZVvcYZa0+0KBvsGEvt2N3XUF5bfi5DmwxxXfxt8H6RpIeBH0o6KSL+0M92r8nPeWZEPNWwbnPSMW/2wd/fazkYjcehP40z4+4nH8+IuF/SfqT33A8BJF0CfDQi/lJGoNacE4tVLn+7PwU4RdI04F3AF0kzhU7vZ9N7gKlNyrfkby2Se0njKY22IH1bXS+UJvUOIXVdfbhWIGnPfmKqwj3551Tgwbry2iSFh0p6nlvyz52A/hLL94BJwI8l7RMRv61b9xBwF2l2WVUGcy+PqaSWY/1y7XgSEVcB+0raBNiX9D48G3jD8MO0VtwVZiMqIu6MiONJXUY75+JaK6Lxm/41wH6Snm55SNqFNDZQ665aDDxP0q51dbYGXl0wpPHkyQJ13lVw27L8gXTOzyEN5W8H/hRp+nYZXpJ/3lmg7uGkLs6LlaYp1ywgtVgei4jexkdJcQ7G0+fC5C7WfwR+21gpIv4aET8mtU53blxv5XKLxSon6QzSN91FpHGavYAXkmaJAfwx//ygpPOAVRHxe+Bk0kyf+ZJO4G+zwn5P7togDXDfAPyvpGNIs6U+T+pOWVcgvMuA0yR9hpTIZpAGqEdMRDwk6WvAsZKeAnpJM+RmkGagDdUuklaT/s93Is04682PgWJal2fz/RC4VNLrIp1LchkwnzQz7wRSV+Uk4BXAsyPimGHEOxQflrSO9B46HHg+adYfkmYB/wT8hNQ6nkaaOfiLEY6x6zix2Ej4DXAY8EFSq+Q24LD8DZKI+Es+J+QI0lnZy0kzxFZI2gs4CTiX1LK5CPhEbawkIiKfl3AGqQvnPlI328EUO/P/DGB74GM5tstIU5srP3GwweeAp0iJdEvSMfrniDhvGPusfYCuJR3TecDnmoybNBURT0l6e95ugaQ9IuLO/IH9aeDjpCnWD5EmaHxzGLEO1aGk6cavAP5CmhZdm4b9J9Jn3AmkrtH7SYP7n25DnF1FEb41sY0tefbP7cCpEfH5dsdj5ZN0OGnCxYZFE6WNHLdYrOPlD5l1wK2kb6afJJ0U+N12xmXWrZxYbCxYQxqveQFpRtFvgX08pdSsPdwVZmZmpfJ0YzMzK5UTi5mZlaqrxlgkzQRmTpw48bAXvajxMlBmZtbKtdde+0BEFLpuXVeOsfT09ERvbztOEjYz60ySro2IQhcadVeYmZmVyonFzMxK5cRiZmalcmIxM7NSObGYmVmpnFjMzKxUXZVY8n2v5/b19bU7FDOzMaurEktEzIuI2ZMnT253KGZmY1ZXJRYzM6ueE4uZmZXKicXMzErlxGJmZqVyYjEzs1J11WXzyyCp3SFYB+rGq4hb93KLxczMSuXEYmZmpXJiMTOzUjmxmJlZqboqsfhaYWZm1euqxOJrhZmZVa+rEouZmVXPicXMzErlxGJmZqVyYjEzs1I5sZiZWamcWMzMrFROLGZmVionFjMzK5UTi5mZlcqJxczMSuXEYmZmper4O0hKWgqsAp7IRe+MiJvbF5GZWXfr+MSSzYiIpe0OwszM2tQVJmm6pDMk3ShpraSFLertLGmBpFWS7pZ0nKQNRjhcMzMbhHa1WF4MzAAWARs2qyBpCnA5cDNwALADcBIpGR7bUP3HkgT8DJgTEU9WFLeZmQ2gXYP38yJiWkQcAtzUos7hwHhgVkRcFhGnA18APilpUl29PSLiFcBrgZ2BI6sM3MzM+teWxBIR6wpU2x+YHxGP1JWdR0o2e9bta3n++RjwHeA1JYZqZmaDNJqnG+8ILKkviIhlpBlgOwJI2qTWepE0DjgIuHGE4zQzszqjObFMAR5uUr4yrwPYEvilpBuBG4C1wBeb7UzSbEm9knpXrFhRRbxmZkaHTzeOiNuBVxSsOxeYC9DT0xNVxmVm1s1Gc4tlJTC5SfmUvM7MzEah0ZxYlpDHUmokTQMm0DD2UpSkmZLm9vX1lRCemZk1M5oTy8XAfpIm1pUdCqwGrhzKDiNiXkTMnjy5WUPIzMzK0JYxFkkTSCdIAmwNTJJ0cF6+KCJWAacDRwAXSjoB2B6YA5zcMAXZzMxGkXYN3k8Fzm8oqy1vByyNiJWS9gZOBeaRZoidQkouQyJpJjBz+vTpQ92FmZkNQBHdN0Gqp6cnent7h7RtunKM2eB04/+ZjS2Sro2IniJ1R/MYi5mZdSAnFjMzK1VXJRZPNzYzq15XJRZPNzYzq15XJRYzM6ueE4uZmZXKicXMzErVVYnFg/dmZtXrqsTiwXszs+p1VWIxM7PqObGYmVmpnFjMzKxUXZVYPHhvZla9rkosHrw3M6teVyUWMzOrnhOLmZmVyonFzMxK5cRiZmal6qrE4llhZmbV66rE4llhZmbV66rEYmZm1XNiMTOzUjmxmJlZqZxYzMysVE4sZmZWKicWMzMrlROLmZmVqqsSi0+QNDOrXlclFp8gaWZWva5KLGZmVj0nFjMzK5UTi5mZlcqJxczMSjWu3QGYdQNJI/p8ETGiz2dWzy0WMzMrlROLmZmVyonFzMxK5cRiZmal6qrE4ku6mJlVr6sSiy/pYmZWva5KLGZmVj0nFjMzK5UTi5mZlcqJxczMSuXEYmZmpXJiMTOzUjmxmJlZqZxYzMysVE4sZmZWKicWMzMrlROLmZmVakwkFkmnSfIt88zMRoGOTyySXgc8p91xmJlZMuKJRdJ0SWdIulHSWkkLW9TbWdICSask3S3pOEkbNNTZGDgeOHIEQjczswLGteE5XwzMABYBGzarIGkKcDlwM3AAsANwEikRHltX9XPAdyJihaQqYzYzs4KGlVgkjY+I1YPcbF5E/CRvfwGweZM6hwPjgVkR8QhwmaRJwBxJJ0bEI5JeBuzG+onGzMzarFBXmKT3S/pk3fJLJN0OPCbpaklbFX3CiFhXoNr+wPycVGrOIyWbPfPya4GdgTskLc1xLZW0RdFYzMysfEXHWD4BrKlb/ibwMPB+YBLw5ZLj2hFYUl8QEcuAVXkdEfGtiPi7iNg2IrbNZdtGxIqSYzEzs0Eo2hX2AuAWAEmbA68D9ouIBZIeA75eclxTSImr0cq8btAkzQZmA2yzzTZDj8zMzPpVtMXyJLBR/v0NpNbLlXl5BUP8sC9TRPQ7eh8RcyOiJyJ6ttjCvWVmZlUpmlh6gdmSdgD+jTT+8VRetx1wT8lxrQQmNymfkteZmdkoVbQr7Ejg58CtpCSyb926twO/KTmuJeSxlBpJ04AJNIy9DIakmcDM6dOnDy86MzNrqVCLJSJuBLYBpgEviIib61Z/Hjiq5LguBvaTNLGu7FBgNX/rghu0iJgXEbMnT27WGDIzszIUPo8lIgK4C0DSxIh4NJcvHswTSppAOkESYGtgkqSD8/JFEbEKOB04ArhQ0gnA9sAc4OSGKchmZjbKFE4skt5IOhlxF2CCpFXAYuCLEbFgEM85FTi/oay2vB2wNCJWStobOBWYR5ohdgopuZiZ2ShWKLFIei9wJvAr4LPA/aQEMQu4VNIHIuJ7RfYVEUuBAa+/krvb3lhkn0V5jMXMrHpKPVwDVJLuAC6PiMOarPsOsHftJMVO0NPTE729vUPa1tcks05Q5P/abDAkXRsRPUXqFp1uPBX43xbrfgD4xBAzMwOKJ5ZfAq9pse41wNXlhFMtSTMlze3r62t3KGZmY1bRwfsTgLMkbQr8mL+NsbwNOBB4j6Tta5Uj4vayAy1DRMwD5vX09DyjS8/MzMpRNLH8Iv/8GGkacE1twGFB3XIA692Qy8zMukfRxLJ/pVGYmdmYUSixRMT8qgMxs/KUMXvRM8tsqEb8nvft5MF7M7PqtUwskpZJenn+/c683PIxciEPna8VZmZWvf66ws4BHqj73e1iMzMbUMvEEhHHACh11n4FWJ0vEGlmZtZSkTGWccC9pDtHmpmZ9WvAxBIRTwJ38rdbE3csD96bmVWv6Kywk4D/yGfedywP3puZVa/oCZL/QLqD5DJJi4D7WH8wPyLiX8sOzszMOk/RxDId+Ev+fWJ+1POMMTMzA4qfeb971YGYmdnY0N8JkrfXTpA0MzMrqr/B+22BjUcojhHhWWFmZtXrqmuFeVaYmVn1BkosHpQ3M7NBGWjwflHRy29HhG/uZWZmAyaWk4GlIxCHmZmNEQMllvMj4rcjEomZmY0JXTV4b2Zm1XNiMTOzUvWXWN4L/HmkAjEzs7GhZWKJiLMj4sGRDKZqPkHSzKx6XdUV5hMkzcyq11WJxczMqtffRSi3kbThSAZjZmadr78Wyx3AKwEk/ULSjiMTkpmZdbL+EstqYEL+/Q3ApMqjMTOzjtffmffXAV+XdFle/qike1rUjYg4utzQzKydil4nsCbC16y1pL/EchjwFeAA0lWO9wYeb1E3ACcWMzNrnVgiYgkwE0DSOuBAXzfMzMwGUuie98B2QKtuMDMzs6cVSiwR8RdJ4yQdCuwBPBd4CLgKuDAinqowRjMz6yCFTpCUNBXoBc4F3gJsn3+eByyWtEVlEZbIl3QxM6te0TPvTwY2A/4hIraPiN0jYntgt1x+clUBlsmXdDEzq17RxDIDOLpx8D4iFgPHkFovZmZmhRPLxsCjLdY9CmxUTjhmZtbpiiaWRcDRkjapL8zLR+f1ZmZmhacbfwq4ArhT0qXAfcBUYD9ApEu+mJmZFWuxRMT1wAuBucAWwL6kxHI68MKIuKGyCM3MrKMUbbEQEQ8A/1FhLGZmNgb4Rl9mZlYqJxYzMyuVE4uZmZXKicXMzEo1YGKRtLGkz0h6+UgEZGZmnW3AxBIRjwOfATatPpzBkXSlpBsk3SjpAkm+fbKZWZsV7Qq7BnhVlYEM0T9GxMsj4mXAMuDf2x2QmVm3K5pYjgI+LOkjkraXtImkCfWPok8oabqkM3IrY62khS3q7SxpgaRVku6WdJykDerrRERfrvssYBPSLZLNzKyNip4geU3++Q3g6y3qbNCivNGLSVdLXgRs2KyCpCnA5cDNwAHADsBJpER4bEPdi4BdgJtIl54xM7M2KppY3kd5rYF5EfETAEkXAJs3qXM4MB6YFRGPAJfl8ZM5kk7MZQBExIzckvky8GHgxJLiNDOzISh6a+KzynrCiFhXoNr+wPz6BEK6W+UJwJ7AvIZ9rpV0NvADnFjM2kLSgHUi3FvdDQZ1Hkse9/gXSZ+W9LxcNl3SxJLj2hFYUl8QEcuAVXkdkqZI2rKuykHAH0qOw8zMBqlQi0XSc4DvAgcDT+btLgHuBb5EmpF1ZIlxTQEeblK+Mq+r1fmBpI1Jl+6/Bfhoqx1Kmg3MBthmm21KDNXMzOoN5p73rwH2BiaSPshrLgLeXHJcA4qI2yNil4h4WUS8NCLeHhH39VN/bkT0RETPFltsMZKhmpl1laKD97OAj0XEFY1TfoG/AC8oNyxWApOblE/J68zMbJQq2mIZDzzYYt1EYG054TxtCXkspUbSNGACDWMvgyFppqS5fX19wwzPzMxaKZpYFgPvbrHuYODX5YTztIuB/RomBRwKrAauHOpOI2JeRMyePLlZY8jMzMpQtCvss6RzSS4Hzied0zJD0idIieX1RZ8wn6U/Iy9uDUySdHBevigiVpFueXwEcKGkE4DtgTnAyQ1TkM3MbJQpeh7LVZL2Bo4HTiUN3n+BdPb8PhGxeBDPOZWUnOrVlrcDlkbEyvx8p5LOWXkYOIWUXMzMbBQbzD3vrwZeJ2k8eTpwbl0MSkQsZf1ZZa3q3Qy8cbD774+kmcDM6dOnl7lbMytJq5MsfWJlZxnKjb7WkM5lWV1yLJXzGIuZWfUKJxZJMyT9mpRY7gXWSPq1pLdUFp2ZmXWcQolF0gdJYx2PAR8DDsk/HwN+mtebmZmhIn2Xkv4C/DwiPtxk3enAjIgY9ddJqRtjOezWW28d6j7KDcqsiwz0eeMxltFL0rUR0VOkbtGusM2AH7VY90PguQX301YeYzEzq17RxHIF6XL1zewJ/LKccMxsLJPU78PGhpbTjSXtXLf4DeBMSZsBPwbuJ52P8jbSvVM+UGWQZmbWOfo7j+UPrH/XSAEfzI9g/XNRLqH4rYnbxuexmJlVr7/EsteIRTFCImIeMK+np+ewdsdiZjZWtUwsETHkiz2amVn3KnxJlxpJ44CNGsuHcnkXMzMbe4qeIDlZ0n9Luod05v2jTR5mZmaFWyxnkaYVfxu4DXiiqoCq5MF7M7PqFT3z/hHggxFxbvUhVa+npyd6e3uHtK3n2puNPJ95335VnHm/DPAYipmZDahoYjkKOFbSqL8emJmZtVfRO0heJGkf4DZJS0l3dGyss2vJsZmZWQcqlFgkfRX4OLCYDh68NzOz6hWdFfYB4DMR8eUqgzEzs85XdIxlFXBtlYGMBEkzJc3t6+trdyhmZmNW0cTydWC2Onyure/HYmZWvaJdYZsDuwF/lLSQZw7eR0QcXWZgZmbWmYomloOBp4ANgX2brA/AicXMzApPN96u6kDMzGxsGPTVjc3MRlqr4V1f6mV0Knoey4cHqhMR/z38cMzMrNMVbbGc2s+62lcGJxYzMys23TgintX4AJ4LvAO4Adi5yiDL4vNYzMyqV/Q8lmeIiIcj4gfA6cAZ5YVUHZ/HYmZWvSEnljp3AIWu0W9mZmPfsBKLpK2AT5GSi5mZWeFZYSv42yB9zUbARGANMKvkuMzMrEMVnRV2Gs9MLGuA5cAlEfFgqVGZmVnHKnrm/ZyK4zAzszGijMF7MzOzp7VssUj6xSD2ExGxdwnxmJlZh+uvK6zIuMlWwGt45viLmZl1qZaJJSIOabVO0jaky+S/FXgAOKX80MzMrBMN6urGkqYDxwD/DNyffz8jIlZXEJuZmXWgQoP3kl4s6X+AW4C9gI8BO0TE1zopqfhaYWZm1es3sUh6taQLgRuBVwEfAF4YEadHxBMjEWCZfK0ws7FF0noPGx36mxV2MfAm4PfAP0XE+SMWlZmZdaz+xlj2yz+fD5wm6bT+dhQRU0uLyszMOlZ/ieULIxaFmZmNGf1NN3ZiMTOzQfMlXczMrFROLGZmVionFjOzLjCS07GdWMzMrFSDuqSLmdlo1uxbeYSvkTvS3GIxM7NSObGYmVmpOjqxSJomaYGkWyTdJOlE+YJBZmZt1dGJBXgKODoidgJeCewGzGpvSGZm3a0tiUXSdElnSLpR0lpJC1vU2zm3SFZJulvScZI2qK2PiHsiojf//gTpKszTRuSPMDOzpto1K+zFwAxgEbBhswqSpgCXAzcDBwA7ACeRkuGxTepvBhxIuiKzmRnwzJliniVWvXYllnkR8RMASRcAmzepczgwHpgVEY8Al0maBMyRdGIuI+9jY+AC4GsRcUv14ZuZWStt6QqLiHUFqu0PzK9PIMB5pGSzZ60gd42dA1wXESeVGqiZmQ3aaB683xFYUl8QEcuAVXldzRnAo8CnRi40MzNrZTQnlinAw03KV+Z1SHot8H6gB7hO0vWSjmi2M0mzJfVK6l2xYkVVMZuZdb2OvqRLRFwNFDpvJSLmAnMBenp6PHpn1qXqB/M9kF+N0dxiWQlMblI+Ja8zM7NRaDQnliWsP5aCpGnABBrGXoqSNFPS3L6+vhLCMzOzZkZzYrkY2E/SxLqyQ4HVwJVD2WFEzIuI2ZMnN2sImZlZGdoyxiJpAukESYCtgUmSDs7LF0XEKuB04AjgQkknANsDc4CTG6Ygm5nZKNKuwfupwPkNZbXl7YClEbFS0t7AqcA80gyxU0jJZUgkzQRmTp8+fai7MDOzAagbZ0X09PREb2/vkLb1xZPNxo5u+vyTNKy/V9K1EdFTpO5oHmMxM7MO5MRiZmal6qrE4unGZlZP0noPK0dXJRZPNzYzq15XJRYzM6ueE4uZmZWqqxKLx1jMzKrXVYnFYyxmZtXrqsRiZmbVc2IxM7NSdfSNvszMyuSbgJXDLRYzMytVVyUWzwozs6HwmfmD01WJxbPCzMyq11WJxczMqufBezOzJop2fXnA/5ncYjEzs1K5xWJmVlDR1kmtXre2YLqqxeJZYWZWFs8Ua62rEotnhZmZVa+rEouZmVXPicXMrCLd2l3mxGJmZqVyYjEzs1J5urGZWUmq6PbqxKnLbrGYmXWAThqr6arE4vNYzMyq11WJxeexmNlo09gSGQszyboqsZiZtVuzxFE0mXRKwnFiMTOr2FBaIZ2SRJpxYjEzs1J5urGZ2Qipb4V0cotkIG6xmJlZqZxYzMysVE4sZmajRLPZYp3IicXMzErlxGJmZqVyYjEzs1J1VWLxtcLMzKrXVYnF1wozs7Gg8Uz+VoP87Rr876rEYmZm1XNiMTOzUjmxmJlZqZxYzMw61Gg9gdKJxcxsDBkNycaJxczMSuXEYmbWQYpcT6zdrRYnFjMzK5UTi5nZGNDuVko9JxYzMyuVE4uZmZWq4xOLpG9JuktStDsWMzMbA4kFOBd4VbuDMDOzpC2JRdJ0SWdIulHSWkkLW9TbWdICSask3S3pOEkb1NeJiF9GxH0jEriZmQ1oXJue98XADGARsGGzCpKmAJcDNwMHADsAJ5GS4bEjE6aZmQ1WuxLLvIj4CYCkC4DNm9Q5HBgPzIqIR4DLJE0C5kg6MZeZmVkTRe7XUpW2dIVFxLoC1fYH5jckkPNIyWbPSgIzM7NhG82D9zsCS+oLImIZsCqvMzOzUWg0J5YpwMNNylfmdQBIOlPS8vz7cklnNtuZpNmSeiX1rlixopKAzcysfWMspYmIDxSsNxeYC9DT0+NzXszMKjKaWywrgclNyqfkdWZmNgqN5sSyhIaxFEnTgAk0jL0UJWmmpLl9fX0lhGdmNrqMlgtRjubEcjGwn6SJdWWHAquBK4eyw4iYFxGzJ09u1hAyM7MytGWMRdIE0gmSAFsDkyQdnJcviohVwOnAEcCFkk4AtgfmACf7HBYzs9GrXYP3U4HzG8pqy9sBSyNipaS9gVOBeaQZYqeQksuQSJoJzJw+ffpQd2Fm1lHa0T2miO6bINXT0xO9vb1D2na09GGamQ3WcD7vJV0bET1F6o7mMRYzM+tATixmZlaqrkosnm5sZla9rkosnm5sZla9rkosZmZWPScWMzMrlROLmZmVqqsSiwfvzcyq11WJxYP3ZmbV66rEYmZm1XNiMTOzUjmxmJlZqboqsXjw3sysel15dWNJfcCtDcWTgcaM06xsc+CBikJ244H5AAAMZElEQVRrpVkcVe+jSP2B6rRaP5jyxjIf/+J1+ltf9P0+Go5/szhGYvvhvgZDWddYPpo+gzaNiC0K1Y6IrnsAc4dR1jsa4q16H0XqD1Sn1frBlDeW+fgP//gXPdaj5fiX8RoMZfvhvgZDWdfkeHfkZ1BXdYXVmTeMsnYoI47B7qNI/YHqtFo/mPLR8BqMtePfat1oPf4w/DiGsv1wX4OhrGss78jj35VdYcMhqTcK3uzGyufj314+/u3XCa9Bt7ZYhmNuuwPocj7+7eXj336j/jVwi8XMzErlFouZmZXKicXMzErlxFIiSS+R9DtJt0r6qaSJ7Y6p20j6lqS7JLmPd4RJmiZpgaRbJN0k6URJandc3UTSlZJukHSjpAskTWpHHE4s5TodODYiXggsAY5qczzd6FzgVe0Ooks9BRwdETsBrwR2A2a1N6Su848R8fKIeBmwDPj3dgTR9YlF0nRJZ+QMv1bSwhb1ds7fxlZJulvScZI2qFu/JbBdRFyUi74DHFT9X9D5ynoNACLilxFx34gEPkaUdfwj4p6I6M2/PwHcCEwbkT+ig5X8/u/LdZ8FbAK0peU+rh1POsq8GJgBLAI2bFZB0hTgcuBm4ABgB+AkUmI+Nld7PrC8brNl+J+qqLJeAxua0o+/pM2AA4E3VRPymFLq8Zd0EbALcBPwqcqi7s9IXxpgtD2AZ9X9fgGwsEmdY4CVwKS6sqOAVbUyoAe4pm79eODRdv99nfAo6zVoqB/t/rs65VH28Qc2Bq4APtXuv60THhW9/zcATgSOasff1PVdYRGxrkC1/YH5EfFIXdl5pOSxZ15eTmq11GzD+i0Ya6HE18CGoMzjn7tmzgGui4iTSg10jKri/R8Ra4GzgXeXEuQgdX1iKWhH0mD80yJiGenbwo55+V5gqaQZucr7gQtHMsgxbsDXwCpV9PifATxKu7pgxq4Bj7+kKXmst+Yg4A8jFmEdJ5ZipgAPNylfmdfVfAj4oqRbgZ1JTVErR6HXQNKZkpbn35dLOnOE4hvrBjz+kl5L+kLVA1wn6XpJR4xciGNakff/FOBneRLA74GXAB8bofjW48H7EkXEjaRpltYmEfGBdsfQrSLiasDnrbRJRNxOGrRvO7dYillJutFNoyl5nVXPr0F7+fi3V0cdfyeWYpbQ0I8vaRowgYZ+T6uMX4P28vFvr446/k4sxVwM7NdwiZZDgdXAle0Jqev4NWgvH//26qjj3/VjLJImkE5OAtgamCTp4Lx8UUSsIl2q5QjgQkknANsDc4CTG6b/2RD4NWgvH//2GovHv+vvxyJpW+COFqu3i4ilud7OwKnA7qTZGWcCc/J8cRsGvwbt5ePfXmPx+Hd9YjEzs3J5jMXMzErlxGJmZqVyYjEzs1I5sZiZWamcWMzMrFROLGZmVionFjMzK5UTiz1N0hxJkR/rJK2UtFjSFyU9rw3xTM0xbdtQ/oYc40tGOqb+SNpJ0lWS/prj27Zh/Xvqjm+rx9IBnuP42m0BSop5W0mP5etODXdfpcbWsO+/y++F5w9ce9D7rh2D0vfdrXyCpD1N0hzg48Cbc9Fk4FWk+8yMB94cEdeOYDwvAX4P7BURC+vKJ5Hud3NDRKweqXgGIulnwAtIx/CvpLsoPl63fgvSvcprDibdEGv3urLHI+K6fp5jGrB5f3UGGfP3ACLivSXsq9TYGvbdAywGdo+IRRXs/xzSrcQPL3vf3ajrrxVmz/BUwz/ufEnfAn4JnCdpx+FcQiLfunaDiHhiqPvI10Yq/cOlBDsCP42IBc1WRsQKYEVtOX9YUuSDUtJGpNfmTuDOMoKVtBnwTmCfMvZXZmxt8D3gp5KOGo3X3uo07gqzAUXEw8BRwHRgX2jdHSVpoaQL6pbPktQr6UBJNwFrgN0kbSXpu5Jul7Ra0p8k/Vf+AK1dP+n3eTdX1LqKWj23pAmSviHpXklrchfem5rFJumdkm6T9Iiki4t0gUh6haQFklblLsJzlG8Dm7tSgtQa+USObeFgjnGT51sk6fuSPiLpDtJVbDdr7G6S9Ob8fHtJuiTHt1TS+wo8zTuAh4Bf1e1vx7y/g/LzPyppmaRD8/rPSLpH0v2S/lOS6rZtFdtrJf0odxH+WdJ6N2Or/a0NZbVtp0vakdRaAfhNLl9TV3cLSd/JMa1W6o58dcP+Dpd0S17/gKQrJL2orsoV+RgfUuC42QCcWKyohcBTwD8MYdttSbdp/jKwP+mCe5uTPtQ+Sep6+wrwXuCbeZt7gHfl3/+N1F1U32XU6Nt5+y8CbyN9c/65pD0a6u0GfITUBTWb1NU3t7/gcxfWQtK9L94JfBTYE7gsJ8J7cmz3Av+Tf/9wf/ssaG/g3TnWA0j3N2/lbOAa0t++APiOpIFaInsDi6J5f/hJwJ+BWaQP9e9LOpl0u9v3AP8NHJvjGsh3c2wHAr8Bvi3p5QW2q1lKem0BPkA6vq8HkDSelBReT3ovzQIeBRZI2jzXeRPwjRzHm0m3T14MTKo9QW6F/5aSWm/dzl1hVkhErJH0ALDlEDbfDNgnIq6vK1sOHFlbkHQ1aVziu5I+GhGPS7oxr765v+4iSTuRvn2/NyLOzmXzgRuBzwL71VWfBLwlIlbmes8DTpE0vp/xmk/ln/vVukkk3UrqjjsoIs4FFkl6HLinxDGAicD+EfFg3d/aqu6PI+Lz+ff5kl5I+uC/vJ/9v5qUkJq5uLY/Sb8jJaw3AS/NiWi+pFm5/McD/B1nR8TxeV9XAW/N290wwHbA0++9P+TFmxqO7/tILcWd6q4C/AvgNtL93j8L7Aosjoiv1G33kyZPdQNusZTCLRYbjKHez/yuhqSCko9LulnSauBJ4BxgY2CbQe5/lxzb+bWCiFiXlxtbLItrSSW7Of/cup/97wpcWt/3HhHXkL5JN+6/TIvqk8oAftRkeaD7nz8PeKDFuqfHiXIMDwMLG1o3t9H/cau5tG5fa4DbgbJmYO1Dag0tlzRO0jhgLXAV0JPrXE/qfv2qpD0kbdhiXw+QjokNkxOLFSLp2aSWx31D2LzZNh8Hvkr6ADyA9OH9b3ndswe5/62Ax/INkRqfd4KkjevKHm6oU5tE0N9zbkXzv+E+4LmDCXSQBnOs72+yPEFSs/uk1yZRbAg83mw9zY9Ts7Iir9VQtytic1K35JMNj3cA0wAi4mfA4aSuv6uAFZK+nrvR6j1O+mJjw+SuMCtqL9L75Td5uTZ4ulFDvSk881twsz78Q4ALIuIztQKlGxkNxT3AcyRNaEguWwKr6qf8DmP/U5uUbwlUOf16MOcCTAVualheFRF9TXccsVZSH7DpMOIryxqav4+KeAi4mvRFpdHTXZsRcSZwZp5wcTBpDGkl6S6MNZvmMhsmt1hsQJI2BU4gdX3U+uxrs392qqs3jTTltojxPPPb8rsalou0JiANxAbpA6MWi/Lyr1ptNAjX0HC/cUm7kCYllLH/MrytyfLiZhXr/BHYrppwBmU5de+j7E0Ny63eCwuAvwduj4jehsdNDXWJiPsi4jTSa9r4RWZb4E9D+QNsfW6xWKNxkmozvyaSBng/RJoR9ebaOSwRsVxSL/CfklaRvqR8mvQNsojLgCMkXUOaffQu0nTmestI3zr/NX+7fjIieht3FBG3SDoXODV/+P8ZOIyU5D5UMJ7+nJz3M1/pfuPPAY4nTYf+YQn7L8OBklYCvwbeDryO9SctNHM1eXZVm/0IeFc+tpeRprTv1VDnDlJyeW+eJPF4RPyOdHvew4CFedZabcbh7sAdEXGapC+TEtJVwIOksafdSfeQr9cDzK/g7+s6brFYo8mk7q5fkwa/Dwa+T5oN1Njt8w7Sh//3gS8Bx5G+BRdxHHAu8F/55xM0/KPngd7DSMntSvr/Bn4YaYbT50gzfl4AvDUiht2iyCc27kXqsjkXOI30IbXvcE70LNl7gNeQZmjtCxwWEZf2uwVcCLwydw+104XA50lfLi4kdeMdWV8hIh4ljZO8lnSy7q9z+SrSGMtVpKnmlwFfI73+tffLb4FXAGcAl5CmLB8TEafX9i9pa+CljJ4vCh3Nl3Qx62CS3gxcDLwwIm4bwvZLgNMi4psDVh7DJH2MlIxH1fXnOpVbLGbd7YvARyR17WdBniH3UVLr2UrgMRaz7nYOaVruVsBdbY6lXf6ONFbzg3YHMla4K8zMzErVtc1fMzOrhhOLmZmVyonFzMxK5cRiZmalcmIxM7NSObGYmVmp/g8NAvD7SGuDdQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize = (6,6))\n",
    "\n",
    "_ = plt.hist(bike_data[\"duration\"], bins = 50, color = \"black\")\n",
    "\n",
    "_ = plt.ylabel(\"Number of Trips\", fontsize = 15)\n",
    "\n",
    "_ = plt.xlabel(\"Duration of Trip (minutes)\", fontsize = 15)\n",
    "\n",
    "_ = plt.title(\"Histogram  of Bike Trips\", fontsize = 15)\n",
    "\n",
    "plt.tick_params(labelsize = 15)\n",
    "\n",
    "_ = plt.yscale('log', nonposy = 'clip')\n",
    "\n",
    "_ = plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram looks like a straight line! Transforming our data and getting back a straight line means something important. We won't get into the specifics of just what yet, but the important thing to understand is that there is now a very regular and identifiable paattern that we could not otherwise see. Basically, if our transformation of the data renders a straight line, we have effectively \"undone\" it's variation, and potentially uncovered a model for its formation. We'll come back to this another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Calculus\n",
    "So, what is calculus and how important is it for data science? There's no doubt for the scientific value that calculus has provided since its formalization. Many of society's advances in physics and engineering would have been impossible without it. Consequently, in U.S. primary education it's not uncommon to receive the perception that calculus is the fancy math following algebra that all scientifically-minded students positively _must_ have in order to succeed in their fields. Thus, college curricula commonly require sequences of several courses in calculus for degree completion. So, naturally we might ask if such coursework holds similar benefit in data science?\n",
    "\n",
    "### 5.1.1 Calculus is all about smoothness and analyzing theoretical models\n",
    "There have indeed been many circumstances in which smooth, theoretical models applied to data resulted in a good course of action for data science work. But this doesn't mean their applied work amounted to any deep procedural execution of calculus thinking. That work was often done earlier by researchers from more-theoretical fields like probability, statistics, mathematics, and machine learning. Thus, since data science is fundementally an empirical discipline, and so much of the work in simply managing, structuring, and handling data, it is quite rare that a data scientist will have to actually do any real calculus work. This being said, it _is_ important to have some basic understanding of when and why calculus is used.\n",
    "\n",
    "In general, there are two main flavors of calculus, focusing on two primary operations. Like many mathematical concepts, these are intrinsically complimentary, and in some sense \"undo\" each other. Let's explore these in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Derivatives: rates\n",
    "\n",
    "For a function $f(x)$, its _derivative_, $f'(x)$, is a function (itself) that describes the rate of change of the original function. This can be thought of as the slope of the (tangent) line touching $f$'s graph at $x$ without going through it. Since derivatives are zero when a function levels off, they're quite handy tools when you have to figure out the locations of maxima and minimia, i.e., in circumstances where optimization is required. So, the most common circumstance in which you'll see derivative calculus popping into data science is when someone is developing a machine learning algorithm. However, it's not an algorithm's definition that usually needs a derivative, but more commonly the development of a scheme for its optimizationm, i.e., finding its best version. To give you an intuition for what a derivative does, here's a good illustration of what the derivative of a function is:\n",
    "\n",
    "![deriv](img/Sec2tan.gif)\n",
    "\n",
    "Note: derivatives don't exist at any corners or sharp edges that a function might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.1 Calculating derivatives\n",
    "As reviewed in the next section, derivatives are calculated analytically for standard function types using formulae. But where do these come from? A standard course in pre-calculus will build up to derivatives through discussion of _limits_, which analyze the output of a function as an input approaches a specified value. For the derivative of a function, we're actually interested in the _limit of slopes of secant lines_ between two inputs as the inputs get closer together. The slope of such a secant line through a function, $f$ at two inputs, $x$ and $x+h$ is simply the rise, i.e., change in output: $f(x + h) - f(x)$ divided by the run, i.e., change in input: $x + h - x = h$, where we'll always assume $h$ is a small positive number. Thus, the derivative of $f$ is approximated by the value of the _difference quotient_ for small values of $h$: \n",
    "\n",
    "$$f'(x)\\approx\\frac{f(x + h) - f(x)}{h}$$\n",
    "\n",
    "As mentioned, some derivative rules are presented in the next section for completeness of review in this discussion. But as it turns out, the difference quotient approximation (above) is actually very important as it gives us a means to approximate derivatives for arbitrary functions! This is huge for our ability to optimize functions when derivatives are computationally expensive or analytically challenging to calculate. So, to round things out, let's write a `difference_quotient()` function that takes a function, `f`, point of evaluation `x`, and step size `h` as inputs to compute an approximate derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h): \n",
    "    return (f(x + h)-f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implementing gradient descent (below) we'll look at the resulting differences between analytic and approximate calculations of derivatives. To try out our derivative approximation, we can create and pass a function of one dimension. What do you think the derivative's true value is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.000999999999479"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x, c = 1):\n",
    "    return c*x**2\n",
    "\n",
    "difference_quotient(square, 3, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.2 Exercise: computing derivative approximations\n",
    "Compute derivative approximations using `difference_quotient` for the `square` and `linear` functions (assuming default coefficients) for a number of different values of `x` and `h`. Do these approximations appear to change as these values are adjusted? How does `h` appear to affect the approximations at a _given_ value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.3 Some derivative rules\n",
    "As mentioned above, derivatives are generally calculated from known folmulae of basic functions, such as those discussed above. For completeness, we'll review these here. However, this doesn't mean that _computation_ generally proceeds by scratching out derivatives by hand. Sometimes, functions are not smooth, or nicely composed of such atomic values, which is why the `difference_quotient()` function above (rather a generalized version, below) will serve as a nice 'swiss army knife'. This being said, what follows can be thought of as a cheet sheet for basic differential calculus.\n",
    "\n",
    "First an foremost, the derivative is a _linear operator_, meaning that the derivative of any _linear combination_ of functions can be computed as the _linear combination_ of the functions' derivatives:\n",
    "\n",
    "$$f(x) = c_gg(x) + c_hh(x) \\Longrightarrow f'(x) = c_gg'(x) + c_hh'(x)$$\n",
    "\n",
    "Some folks will describe this in terms of two separate rules:\n",
    "\n",
    "The __constant factor__ rule:\n",
    "$$(af)'(x)=af'(x)$$\n",
    "\n",
    "The __sum__ rule\n",
    "$$(f+g)'(x)=f'(x)+g'(x)$$\n",
    "\n",
    "Regardless, these are essential when calculating derivatives of more complex functions that are composed of other, simpler functions. Derivatives of the simpler, canoncial functions discussed at the beginning of this chapter are as follows:\n",
    "\n",
    "The __elementary power (polynomial) rule__ states that If $f(x) = x^n$ for any $n\\neq 0$, then\n",
    "\n",
    "$$f'(x)=nx^{n-1}$$\n",
    "\n",
    "As a special case, if $f(x) = x$, then $f′(x) = 1$. Combining this rule with the linearity of the derivative (its linear operator status) permits the computation of the derivative of any polynomial. This means our squaring function, $f(x) = x^2$ has derivative $f'(x) = 2x)$, making our approximation above by the difference quotient quite close to the true value of $f'(3) = 2\\cdot3 = 6$!\n",
    "\n",
    "The __product__ rule states that the derivative of the function $h(x) = f(x) g(x)$ with respect to $x$ is\n",
    "\n",
    "$$h'(x)=(f(x)g(x))'=f'(x)g(x)+f(x)g'(x)$$\n",
    "\n",
    "The __chain__ rule states that the derivatice of the function $h(x)=f(g(x))$ with respect to $x$ is\n",
    "\n",
    "$$h'(x)=(f(g(x)))'=f'(g(x))g'(x)$$\n",
    "\n",
    "This means that a __quotient__ rule for a derivative of a function $h(x) = f(x)/g(x)$ can be computed from a combination of the power, product, and chain rules, since:\n",
    "\n",
    "$$h(x) = \\frac{f(x)}{g(x)} = f(x)(g(x))^{-1}$$ \n",
    "\n",
    "In particular, one has:\n",
    "$$\n",
    "\\begin{align}\n",
    "h'(x) \n",
    "&= f'(x)g(x)^{-1} - f(x)g'(x){g(x)^-2}\\\\\\\\\n",
    "&= \\frac{f'(x)g(x) - f(x)g'(x)}{g(x)^2}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "For our transcendental function the derivatives get a little bit stranger!\n",
    "In particular, the __derivative of an exponential function__, $f(x) = c^{ax}$, with $\\hspace{10pt} c>0$ is given by:\n",
    "\n",
    "$$f'(x) = c^{ax}\\ln c\\cdot a$$\n",
    "\n",
    "and for the __derivative of a logarithmic function__, $f(x) = \\log _{c}x$, with $c>0,c\\neq 1$ is given by:\n",
    "\n",
    "$$f'(x) = \\frac{1}{x\\ln c}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.4 Exercise: Making sense of derivatives\n",
    "Considering the two applications of the difference quotient in __Secs. 5.1.2.1&ndash;5.1.2.3__, make sense of their ouput. Do the derivative rules (above) suggest these approximations were accurate? Discuss!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.5 Exercise: calculating logistic derivatives\n",
    "Using the above derivative rules, show that the derivative of the _standard_ logistic function (i.e., with the presets requested in __Sec. 5.0.3.3__) has the following special property: \n",
    "\n",
    "$$f'(x) = f(x)(1 - f(x))$$\n",
    "\n",
    "and use this property to define a (computing) function that calculates derivatives from the logistic function requested in __Sec. 5.0.3.3__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Integrals: accumulation\n",
    "The \"undo\" operation for a function's derivative is called an _integral_. Integrals describe the accumulated area between a function, $f(x)$'s graph and the $x$-axis. Integrals are taken over specified ranges of $x$, like starting from $x=a$ and ending at $x=b$, indicated by a somewhat more complicated notation:\n",
    "\n",
    "$$\\int_a^bf(x)dx$$\n",
    "\n",
    "This makes integrals handy tools in circumstances where you have to add things up. So, the common context in which integral calculus will pop into data science work is more commonly with probabilistic modeling. When one defines a (smooth) probability model, its total probability—the area under the model's curve—must add up to one. So, answering questions like \"Where does my model predict 95% of the outcomes will be?\" will often rely on the result of an integral. Mostly, we'll _use_ well-established distributions that have already had the integral calculus work done. Just like with derivatives, here's a good illustration of what the integral (denoted by $S$) of a function looks like:\n",
    "\n",
    "![int](img/integral.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.1 Calculating integrals\n",
    "Just like with derivatives, integrals are often defined and calculated from principles using limits. The easiest way to do this is to cover the region we'd like to measure the area of with rectangular tiles. This is called the _rectangle rule_. There're a few different rectangles we could choose, setting the tops at the left, right, or midpoint values of the function over the region where the rectangle is to be defined. Here's an excellent picture of these:\n",
    "\n",
    "![Rectangle rule](./img/322_left_right_midpoint_rules.gif)\n",
    "\n",
    "Here's a bit of code to approximate a function of one variable using the _left-endpoint_ rule with $n$ rectangles, testing on our `square()` function over a few different rectangle-wiidths. Can you see the approximations get more-refined with the rectangles?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0\n",
      "6.518518518518518\n",
      "5.551020408163264\n",
      "5.465020576131687\n",
      "5.360000000000002\n"
     ]
    }
   ],
   "source": [
    "def left_endpt_int(f, a, b, n):\n",
    "    h = (b - a)/n\n",
    "    lefts = np.array([a + h*k for k in range(n)])\n",
    "\n",
    "    return(sum(h*f(lefts)))\n",
    "\n",
    "print(left_endpt_int(square, -2, 2, 2))\n",
    "print(left_endpt_int(square, -2, 2, 3))\n",
    "print(left_endpt_int(square, -2, 2, 7))\n",
    "print(left_endpt_int(square, -2, 2, 9))\n",
    "print(left_endpt_int(square, -2, 2, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's worth noting that integral approximations can certainly get better than the above. For example, there's also the _trapezoid rule_. Intuitively, the top (or bottom, when $f(x) < 0$) will be a secant line through $f$'s values at each given trapezoid's left-right endpoints. Can you compute the area of each trapezoid to render integral approximations? It can be challenging! The picture looks like this:\n",
    "\n",
    "![Trapezoid rule](./img/322_trapezoidal_rule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.2 Analytic integrals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing integrals and derivatives are both challenging, but performing integral calculations in abstract can be much more difficult than derivatives! Integrals are linear operators:\n",
    "\n",
    "$$\\int cf(x) + dg(x)\\:dx = c\\int f(x)\\:dx \\: + \\: d\\int g(x)\\:dx,$$\n",
    "\n",
    "and (up to a constant), derivatives and integrals 'undo' each other, i.e.:\n",
    "\n",
    "$$\n",
    "\\int f'(x)\\:dx = f(x) + c\n",
    "\\hspace{10pt}\\text{and}\\hspace{10pt}\n",
    "f'\\left(\\int f(x)\\:dx\\right) = f(x)\n",
    "$$\n",
    "\n",
    "Such as with derivatives, properties like these are very important for determining integrals analytically. For example, the integral of a polynomial of one dimension is generally known via the power rule for integrals:\n",
    "\n",
    "$$\\int x^n dx = \\frac{x^{n+1}}{n+1} + c,$$\n",
    "\n",
    "where $c$ is a constant. But where does the integral power rule formula come from? One way to see it reasons in a straightforward manner from knowledge of the derivative (power) rule:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "& \\hspace{5pt} f(x) = x^m \\\\\n",
    "\\Longrightarrow & \\hspace{5pt} f'(x) = mx^{m-1} \\\\\n",
    "\\Longrightarrow & \\int mx^{m-1}\\: dx = f(x) + K = x^m + K \\\\\n",
    "\\Longrightarrow & \\hspace{5pt}m\\int x^{m-1}\\: dx = \\frac{x^{m}}{m} + \\frac{K}{m}\\\\\n",
    "\\Longrightarrow & \\int x^{n}\\: dx = \\frac{ x^{n+1} }{n+1} + c\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where in the last line this derivation defines $c = \\frac{K}{m}$ and $n = m - 1$.\n",
    "\n",
    "Integrals are like puzzles with often unclear pathways to resolving. Others require techniques that utilize some _very_ advanced mathematics. For those that are known analytically, there are generally large integral tables that record their complex, difficult to remember (or solve) formulae. Since we won't really need these too much in the course and those that are needed are commonly drawn from reference materials we won't go into depth here. However, the topic will come up again in __Chapter 6__, when we discuss probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Gradient descent and optimization\n",
    "We will often need to maximize/minimize&mdash;optimize&mdash;functions that take as input a vector of real numbers and simply outputs a single real number. So, here we'll need to find the specific input(s) that produces the largest/smallest possible output values. But how do we compute derivatives of functions of more than one dimension? One way is to compute the rate of change that a function takes along its different input dimensions, separately. This concept is called the _gradient_ and helps out immensely with optimizations, as it can determine information on the input direction along which the function will most likely increase or decrease the most. \n",
    "\n",
    "We've actually already covered the gradient in the case when our function is just of a single variable&mdash;it's simply the derivative! However, when our function is one of several variables, it possesses multiple _partial derivatives_. Each partial derivative depicts how the function changes when small changes are made in just a single input variable. This is calculated by treating the function as just a function of the variable in question, and treating all the other variables as constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.1 Calculating gradients\n",
    "The gradient of a function of multiple dimensions is _vector-valued_, which means that the output, i.e., gradient of a function, is a vector, itself, expressed in terms of the input variable dimensions. For a function of $n$ dimensions (i.e., columns of data), $f(x_1, \\cdots, x_n)$, expressing the gradient of $f$ requires the determination of $n$ _partial_ derivatives&mdash;one with respect to each input variable. For notation, we'll refer to the derivative of a function $f$ with respect to variable $x_i$ as $f_{x_i}'$, and calculate it as though all other variables are constant. Denoting the gradient of $f(x_1, \\cdots, x_n)$ by $\\nabla f(x_1, \\cdots, x_n)$, it may be calculated from partial derivatives as:\n",
    "\n",
    "$$\\nabla f(x_1, \\cdots, x_n) = \\left[\\hspace{2pt}f_{x_1}'(x_1, \\cdots, x_n),\\hspace{22pt} \\cdots,\\hspace{22pt} f_{x_n}'(x_1, \\cdots, x_n)\\right]$$\n",
    "\n",
    "that is, $\\nabla f(x_1, \\cdots, x_n)$ is itself a vector of functions of multiple dimensions.  \n",
    "\n",
    "So, taking our relatively simple polynomial example:\n",
    "\n",
    "$$ f(x, y, z) = x^2 + y^2 + z^2$$\n",
    "\n",
    "the subsequent gradient would be\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\nabla f(x, y, z)\n",
    "&= \\left[\\hspace{2pt}f_{x}'(x, y, z),\\hspace{22pt} f_{y}'(x, y, z),\\hspace{22pt} f_{z}'(x, y, z)\\right]\\\\\n",
    "&= \\left[2x,\\hspace{46pt} 2y,\\hspace{46pt} 2z\\right]\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.2 Gradient descent\n",
    "_Gradient descent_ is one particular (and relatively simple) algorithm often used for optimization. Well put in its [Wikipedia article](https://en.wikipedia.org/wiki/Gradient_descent):\n",
    "\n",
    "> Gradient descent is also known as [the approach of] steepest descent.\n",
    "\n",
    "As the Wikipedia article goes on:\n",
    "\n",
    "> To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. \n",
    "\n",
    "Of course, the same can technique can be used in maximizing a function, albeit with taking steps in the direction along that suggested by the gradient.\n",
    "\n",
    "So, to define gradient descent on a function $f$ we can:\n",
    "\n",
    "1. pick a random starting input, $x^{(0)} = \\left[x_1, \\cdots, x_n\\right]$; \n",
    "2. compute the gradient at $x$ and multiply by a small _negative_ constant: $\\gamma\\cdot\\nabla f(x)$;\n",
    "3. _pointwise_ add $\\gamma\\cdot\\nabla f(x)$ to $x^{(0)}$ to step to the next point: $x^{(1)} = x^{(0)} + \\gamma\\cdot\\nabla f(x)$\n",
    "4. repeat, using $x^{(n)}$ as the start for the next step $x^{(n+1)}$.\n",
    "\n",
    "Generally, the gradient descent algorithm is repeated until the values $x^{(n)}$ converge, i.e., stop changing appreciably. This is generally done by setting a _tolerance_. Once the change between points falls to within the desired tolerance, the algorithm will have _converged_. But, if the  $x^{(n)}$ are vectors, how do we compute the size of change between points? Well, from __Chapter 1__ we know that the Euclidean distance, i.e., the square root of the sum of squared errors (pointwise differences) does just this. \n",
    "\n",
    "Note that the small negative constant in this algorithm is called the _step size_, and if made positive changes te algorithm into _gradient ascent_, i.e., mountain climbing. The choice of framing is relatively arbitrary, as a function $f$ can be maximized by performing gradient descent on $-f(x)$. Additionally, the step size parameter can impact the ability for the algorithm to converge&mdash;as we're about tto discuss in __Sec. 5.1.3.4__, it can help to adjust this parameter dynamically.\n",
    "\n",
    "Importantly that gradient descent is only ever guarenteed to find a _local_ minimum, i.e., that a well that may or may not be the _deepest_ well. This is, in general, a challenge in the study of optimization algorithms and the reason for the development of many others. However, gradient descent is a clear, intuitive starting potin as the approach of steepest descent and very widely used for its scalability, namely in neural network applications. Here's a good picture to have in mind for gradient descent, taking the simplest non-trivial case\n",
    "\n",
    "![Gradient descent](./img/Gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.3 Implementing gradient descent\n",
    "There are many methods provided in the numerical modules of `Python` that allow for easy computation of gradients, so it isn't totally necessary to know how to calculate them by hand. What is important is understanding the general concept of what a gradient is. Let's use the gradient to minimize the our simple example function once again:\n",
    "\n",
    "$$f(x, y, z) = x^2 + y^2 + z^2$$\n",
    "\n",
    "Since the value of any squared real number is positive, this function is minimized when our input is the point $(0, 0, 0)$, but let's show this using what we've just learned! To get things started, let's define our sum of squares function, $f$, as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(x):\n",
    "    \"\"\"Our sum of squares function, f at point x\"\"\"\n",
    "    return sum([x_i**2 for x_i in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pick a random starting point, $x^{(0)}$ and set our `step_size` and `tolerance`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# pick our random starting point\n",
    "x = [random.randint(-10, 10) for i in range(3)]\n",
    "step_size = -0.01\n",
    "tolerance = 0.0000001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll need a (computing) function that returns the gradient of our (mathematical) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares_gradient(x):\n",
    "    \"\"\"The gradient of our sum of squares function, f at point x\"\"\"\n",
    "    return [2 * x_i for x_i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we'll need a function that computes our steps along the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(x, gradient, step_size):\n",
    "    \"\"\"move step_size in the direction of x\"\"\"\n",
    "    return [x_i + step_size * gradient_i for x_i, gradient_i in zip(x, gradient)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and take our small steps according to `step_size` in a direction opposite to where the gradient is pointing us, i.e., until we compute a Euclidean distance between subsequent points that is very close to 0 i.e., less than the `tolerance`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.416786050605469e-06, 3.416786050605464e-07, 3.416786050605469e-06]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    gradient = sum_of_squares_gradient(x) # compute the gradient at v\n",
    "    next_x = step(x, gradient, step_size) # take a negative gradient step\n",
    "    # stop when convergence occurs---euclidean distance! \n",
    "    if np.linalg.norm(np.array(next_x) - np.array(x)) < tolerance:\n",
    "        break\n",
    "    x = next_x # else, continue\n",
    "print(next_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.4 Approximating gradients\n",
    "While we've already created the `difference_quoteient()` to approximate single-dimensional derivatives, to approximate our gradients (multi-dimensional derivatives) we'll need to calculate _partial difference quotients_ as components of a _vector_ approximation. This is essentially the same&mdash;generalized&mdash;concept except. By treating our function as one of just the ith variable (holding the other variables fixed) we can compute estimates of the partial derivatives of a function $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, x, i, h):\n",
    "    \"\"\"approximate the ith partial difference quotient of f at x\"\"\"\n",
    "    y = [x_j + (h if j == i else 0) # add h to just the ith element of x\n",
    "         for j, x_j in enumerate(x)]\n",
    "\n",
    "    return (f(y) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f, x, h=0.00001):\n",
    "    return [partial_difference_quotient(f, x, i, h) for i, _ in enumerate(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, how close is our gradient at a given point? Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-12, 4, 4]\n",
      "[-11.999989999367243, 4.000010000027032, 4.000010000027032]\n"
     ]
    }
   ],
   "source": [
    "# set an initial value for the input\n",
    "x = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "print(sum_of_squares_gradient(x))\n",
    "print(estimate_gradient(sum_of_squares, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we approximate our gradient using a difference quotient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.056352461504272e-07, -2.441021069288023e-06, -4.999997441022726e-06]\n"
     ]
    }
   ],
   "source": [
    "# set an initial value for the input\n",
    "x = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "# run the program\n",
    "while True:\n",
    "    gradient = estimate_gradient(sum_of_squares, x) # compute the gradient at v\n",
    "    next_x = step(x, gradient, step_size) # take a negative gradient step\n",
    "    # stop when convergence occurs---euclidean distance! \n",
    "    if np.linalg.norm(np.array(next_x) - np.array(x)) < tolerance:\n",
    "        break\n",
    "    x = next_x  # else, continue\n",
    "print(next_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.5 Parameterizing gradient descent\n",
    "Unfortunately, there are no hard and fast rules for how large the step size should be. The three most popular options are:\n",
    "\n",
    "1. Fixed step size\n",
    "2. Shrinking the step size at each step\n",
    "3. Picking at each step the step size which minimizes the given function\n",
    "\n",
    "The final one sounds nice, but is clearly very computationally expensive&mdash;impossible in fact, given the infinite number of possible step sizes. So, to do something like (3) one might just use a few different step sizes and pick the one that results in the smallest value of $f$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [-100000, -10000, -1000, -100, -10, -1, -0.1, -0.01, -0.001, -0.0001, -0.00001, -0.000001]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.6 Safely parameterizing gradient descent\n",
    "One concept we've left out a bit in our discusion of functions is that of _domain_, which refers to the set of values on which a particular function is defined. For example, out logarithm function, $f(x) = \\log{x}$, will only produce real numbers as output when passed positive numbers (regardless of base). At zero, any logarithm function has a _singularity_, where no value exists. As it turns out, negative inputs to the logarithm result in complex number, i.e., numbers with non-zero 'imaginary' parts! While traversing these latter (complex) types of is within the realm of computational possibility, optimization over a function's complex output (if this is the case) may not be desired&mdash;would it make sense to have our algorithm select a point as optimal that has small, but complex/imaginary or non-existent error?\n",
    "\n",
    "All of this is to say that certain step sizes might result in invalid inputs for our function. So we also need to find a way to create a `safe()` wrapper to our function that, upon stepping out of domain (e.g., from a large step size) will sense the issue as an exception and returns infinity if something goes wrong&Mdash;this will then never be a minimizing value. Note that if we wished to modify our algorithm into _gradient ascent_, formally, we would not only have to modify our code (in __Sec. 4.1.3.6__, below) to select the largest-valued point at each step, but likewise we'd have to modify our `safe()` function to return `-float('inf')`, which will never be a _maximizer_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe(f):\n",
    "    \"\"\"return a new function that's the same as f, but it outputs infinity whenever something goes wrong\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf') # this is just infinity\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.7 Gradient descent, as used in data science and machine learning\n",
    "As discussed, our algorithm for gradient descent simply addresses points '$x$' that minimize the value of a target function '$f(x)$'. But for the purposes of data science and machine learning gradient descent is commonly, more specifically intersted in finding something else. In particular, one often asks: \"What parameters (e.g., coefficients) make a model (e.g., to a polynomial) most applicable to some data?\".\n",
    "\n",
    "Under this framing, our data would be expressed as vectors of points: $x = [x_1, \\cdots, x_m]$ and $y = [y_1, \\cdots, y_m]$. The goal, then, is to find some parameters, $c = \\{c_1, \\cdots, c_k\\}$, that produce the 'best' model, $f(x; c)$, i.e., that predicts reasonable values for $y$ from the inputs in $x$: $\\hspace{5pt}f(x_i; c)=\\hat{y}_i\\approx y_i\\hspace{5pt}\\text{ for all }\\hspace{5pt}i=1,\\cdots,m.$ Echoing the pervious paragraph's sentence, we may now articulate:\n",
    "\n",
    "$$\n",
    "\\text{Given }\\hspace{5pt} x \\hspace{5pt} %= [x_1, \\cdots, x_m]\n",
    "\\text{ and }\\hspace{5pt} y\\text{,}\\hspace{5pt} %= [y_1, \\cdots, y_m]\n",
    "\\text{ which }\\hspace{5pt} c \\hspace{5pt} %= \\{c_i\\}_{i=0}^k\n",
    "\\text{ optimze }\\hspace{5pt} \\hat{y} = f(x; c) = c_0 + c_1x + \\cdots + c_k{x}^k\\hspace{5pt}\n",
    "\\text{ into }\\hspace{5pt} y\\text{?}\n",
    "$$\n",
    "\n",
    "Note that it now becomes very helpful/important to consider $x$ and $y$ holistically, as vectors&mdash;for notation, intuition, _and_ ultimately computation, as when the '$m$' observations of data become large it becomes more important to process our data as vectors, [whole hog](https://en.wiktionary.org/wiki/whole_hog#English), into 'vectorized' computational functions, like those provided by `numpy`, as discussed in __Ch. 1__.\n",
    "\n",
    "> But, how does one determine which set of parameters is 'best'? \n",
    "\n",
    "___That_ question is why we are using gradient descent in data science and machine learning, and if this point is not yet clear, please take a moment to reflect and review.__ Subsequently, any optimization (and hence calculus) will be conducted with respect to the parameters $c$, and not $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.8 Setting up a gradient descent experiment\n",
    "Given our new framing, we're missing a few pieces to try gradient descent out in a meaningful way. In particular, we need data and a parameterized model. So, let's set up our model as a slight variation of our `linear()` function, now accepting a `list` of coefficients, `c`, instead of `m` and `b` as separate arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, c):\n",
    "    return c[1]*x + c[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to generate some data; let's use the first `100` `int`s for our input `x`. We'll use the model `linear` for our `y`, and have to choose some coefficients: `true_c = [1, 2]`. But to make the experiment a _little_ more genuine we'll want to add some noise to the model. In particular, we'll 'adjust' the `true_y` values by random numbers coming from the range $[0,0.1]$. Under this framing, we've chosen the 'right' model, but perhaps recorded our data imperfectly. The question will be, how well can we determine the parameters that were used to generate the data, given that the data are noisy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.03525574, 0.05713973, 0.09487138, 0.06682934, 0.06070163,\n",
       "        0.03489505, 0.04285179, 0.0409972 , 0.02935928, 0.01940323]),\n",
       " array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " array([ 1.03525574,  3.05713973,  5.09487138,  7.06682934,  9.06070163,\n",
       "        11.03489505, 13.04285179, 15.0409972 , 17.02935928, 19.01940323]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## set the number of data points\n",
    "m = 100\n",
    "## set the 'true' parameters that we'll try to estimate\n",
    "true_c = [1, 2]\n",
    "## set the experimental input\n",
    "x = np.array(range(m))\n",
    "## create the true values and noise\n",
    "true_y = linear(x, true_c)\n",
    "noise = np.random.uniform(high = 0.1, size = m)\n",
    "## combine the 'true' values with noise to obtain 'experimental' data\n",
    "y = true_y + noise\n",
    "\n",
    "noise[:10], x[:10], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.9 Objective functions\n",
    "Returning to our toy example of gradient descent in __Sec. 4.1.3.3__: we tested for convergence of points according to their Euclidean distance from one another. Recall from __Ch. 1__ that this is computed between two vectors as the square root of the sum of squared differences. This latter part of the description highlights a very common _objective function_, i.e., function that we're going to optimize now, called _sum of squared errors_ ($SSE$). But remember, we're no longer optimizing over $x$! So, under our new framing this would be:\n",
    "\n",
    "$$SSE(c_1, \\cdots, c_k) = \\sum_{i=1}^n \\left[f(x_i; c) -  y_i\\right]^2$$\n",
    "\n",
    "The gradient we're now looking for is a $k$-length vector of partial derivatives of $SSE(c_1, \\cdots, c_k)$:\n",
    "\n",
    "$$\\nabla\\cdot SSE = \\left[SSE_{c_1}'(c_1, \\cdots, c_k), \\cdots, SSE_{c_k}'(c_1, \\cdots, c_k)\\right]$$\n",
    "\n",
    "According to our derivative rules, for each $j$, these partial derivatives are calculated as:\n",
    "\n",
    "$$\n",
    "SSE_{c_j}'(c_1, \\cdots, c_k) = \n",
    "2\\sum_{i=1}^n f_{c_j}'(x_i; c)\\left[f(x_i; c) -  y_i\\right]\n",
    "$$\n",
    "\n",
    "which means that if the gradient function of $f$ (in $c$) is known, then so is the gradient of its $SSE$. So, let's define a computational function for this that utilizes `safe` versions of both the function `f` and _its_ `gradient` in the context of data `x` and `y` to return an `f`-contextualized version of $SSE$'s gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_SSE(f, gradient, x, y):\n",
    "    f = safe(f)\n",
    "    gradient = safe(gradient)\n",
    "    \n",
    "    def gradient_f_SSE(c):\n",
    "        # since the gradient and function evaluations are reused in nested comprehensions\n",
    "        # precompute the function and partial derivative evaluations to save computation!\n",
    "        fs = [f(xi, c) for xi in x]\n",
    "        grads = [gradient(xi, c) for xi in x]\n",
    "        \n",
    "        return [\n",
    "            2*np.sum([\n",
    "                grads[i][j] * (fs[i] - yi)\n",
    "                for i, yi in enumerate(y) ## note the sum must\n",
    "            ])\n",
    "            for j, _ in enumerate(c) ## the gradient is dimensional of the parameters!\n",
    "        ]\n",
    "    \n",
    "    return gradient_f_SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for our `linear()` function: $f(x; c) = c_1x + c_0$ the gradient is equal to $\\nabla\\cdot f = \\left[1, x\\right]$. According to our previous formula, this makes the corresponding $SSE$-relevant gradient equal to:\n",
    "\n",
    "$$\n",
    "\\nabla\\cdot SSE = \\left[\n",
    "2\\sum_{i=1}^n \\left[c_1x_i + c_0 -  y_i\\right]\n",
    ", \\hspace{5pt}\n",
    "2\\sum_{i=1}^n x_i\\left[c_1x_i + c_0 -  y_i\\right]\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "So, let's sanity check out work and code up both `linear`'s and the $SSE$-relevant gradient (above), separately. If we get the same answer from our $SSE$-relevant gradient as our hard-coded one (below), then we should feel a bit more confident in allowing users to simply provide the gradient of _their_ function, making our implementation a bit more user friendly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.763062815756356, -486.02455790203754]\n",
      "[-9.763062815756356, -486.02455790203754]\n"
     ]
    }
   ],
   "source": [
    "def gradient_linear(x, c):\n",
    "    return [1, x]\n",
    "\n",
    "def gradient_linear_SSE(x, y, c):\n",
    "    return([\n",
    "        2*sum(c[1]*x + c[0] - y),\n",
    "        2*x.dot(c[1]*x + c[0] - y)\n",
    "    ])\n",
    "\n",
    "print(gradient_linear_SSE(x, y, [1, 2]))\n",
    "\n",
    "implied_gradient_linear_SSE = gradient_SSE(linear, gradient_linear, x, y)\n",
    "\n",
    "print(implied_gradient_linear_SSE([1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.10 Computing gradient approximations with an objective function\n",
    "Now since $SSE$ is defined by $f$, its dependence on $c$ _depends_ on the formula for $f$&mdash;the model, whatever it is! This means that determining the partial derivatives requires the chain rule, with $f$ only part of the 'inside' function (a difference) to an 'outside' squaring function $(\\cdot)^2$ in the terms of $SSE$, as in the previous section. Doing this can constitute challenging and necessary differential calculus work for algorithms development, but we have our workarounds, too! In particular, since we can now compute gradient _approximations_ through our difference quotient (__Sec. 4.1.3.4__), we need only convert a given parameterized model $f(x,c)$ and data `x` and `y` into a (computational) function that computes $SSE$ as a function of the parameters, $c$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3199284276351949, 21911.082599311027)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def SSE(f, x, y):\n",
    "    def SSE_f(c):\n",
    "        return np.sum((f(x, c) - y) ** 2)\n",
    "    return SSE_f\n",
    "\n",
    "SSE_linear = SSE(linear, x, y)\n",
    "\n",
    "## sanity check some SSE values; it should be low at the 'true' parameters\n",
    "SSE_linear(true_c), SSE_linear([0.5,1.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're pretty sure our $SSE$ calculations are correct, let's sanity check its usage in gradient approximations; not bad!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-9.762062818297057, -482.7410579045066]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimate_gradient(SSE_linear, true_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.11 Pulling together the pieces\n",
    "Finally, regardless of the availability of the $SSE$-gradient that we'd have to calculate, we're now in a position to put together our first re-usable gradient descent routine. It will require a model function, `f`, starting parameters, `c0`, and optionally, a `gradient` function, and `tolerance`, to test for convergence.\n",
    "\n",
    "It's important to note here that this implementation is looking at the entire dataset at each gradient step. This is why we call this _batch_ gradient descent. This isn't a good solution when working with enormous datasets, so we'll look at a more efficient implementation next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_batch(f, x, y, c0, gradient_fn = False, tolerance = 0.00001):\n",
    "    \"\"\"perform gradient descent to find the input which minimizes the target function\"\"\"\n",
    "    \n",
    "    c = c0 # set theta to initial value\n",
    "    f = safe(f) # use the safe version of the target function\n",
    "    SSE_f = SSE(f,x,y) # convert our data and model into an SSE objective function of c\n",
    "    \n",
    "    print(\"Starting gradient descent at c0 = \", c0, \" with SSE = \", SSE_f(c))\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        ## create our gradient function for the SSE objective\n",
    "        grad_SSE = gradient_SSE(f, gradient_fn, x, y) if gradient_fn else False\n",
    "        \n",
    "        ## compute the gradient at this point: c\n",
    "        gradient = grad_SSE(c) if grad_SSE else estimate_gradient(SSE_f, c)\n",
    "        \n",
    "        ## determine the next candidate values, given several step sizes\n",
    "        next_cs = [step(c, gradient, step_size) for step_size in step_sizes]\n",
    "        \n",
    "        # choose the one that minimizes our objective function\n",
    "        next_c = min(next_cs, key = SSE_f)\n",
    "        iterations += 1\n",
    "        \n",
    "        # stop when convergence occurs---euclidean distance! \n",
    "        if np.linalg.norm(np.array(next_c) - np.array(c)) < tolerance:\n",
    "            print(\"Completed optimizing gradient descent after \", iterations, \"iterations.\")\n",
    "            print(\"The final value was c = \", next_c, \" with SSE = \", SSE_f(next_c))\n",
    "            return next_c\n",
    "        c = next_c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at c0 =  [8.34093772 4.13878793]  with SSE =  1661723.877079659\n",
      "Completed optimizing gradient descent after  16802 iterations.\n",
      "The final value was c =  [1.2109964031362142, 1.9975618049966546]  with SSE =  0.755842828207971\n"
     ]
    }
   ],
   "source": [
    "_ = minimize_batch(linear, x, y, np.random.uniform(low = -10, high = 10, size=2), gradient_fn = gradient_linear) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the approximation work as well here, even though we're not computing an exact derivative? Yes, not quite as well, but this could be because of the random initialization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at c0 =  [ 4.43526104 -7.8318662 ]  with SSE =  31411700.091439016\n",
      "Completed optimizing gradient descent after  13419 iterations.\n",
      "The final value was c =  [1.2194827153457026, 1.9974287060998053]  with SSE =  0.8278760950828415\n"
     ]
    }
   ],
   "source": [
    "_ = minimize_batch(linear, x, y, np.random.uniform(low = -10, high = 10, size=2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.12 Stochastic Gradient Descent\n",
    "Using the above (batch) approach, each gradient step involves computation for $SSE$ over the _entire_ dataset. _This takes a really long time._ With $SSE$, the error we calculate is _additive_, meaning that the predictive error on the whole dataset is just the sum of the predictive errors for each point in the set. When this is true we can use a far more potent technique for minimizing the error, called _stochastic gradient descent_. This process calculates steps by the gradient considering only each single point at a time. The process cycles along the data until it reaches a certain stopping point. Since this all depends on having a random order of data points from `x`, the first thing we'll need is a generator that returns the elements of our dataset randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(67, 135.04953796762706),\n",
       " (47, 95.00265405252127),\n",
       " (48, 97.05240038159458),\n",
       " (78, 157.00372459203453),\n",
       " (71, 143.04794067033797),\n",
       " (97, 195.08258515139755),\n",
       " (30, 61.08099079198932),\n",
       " (85, 171.00824056241984),\n",
       " (41, 83.01469927366286),\n",
       " (43, 87.01257037126858)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def in_random_order(data):\n",
    "    \"\"\"generator that returns the elements of data randomly\"\"\"\n",
    "    indices = [i for i, _ in enumerate(data)]\n",
    "    random.shuffle(indices)\n",
    "    for i in indices:\n",
    "        yield data[i]\n",
    "        \n",
    "list(in_random_order(list(zip(x, y))))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement our stochastic gradient descent we can now easily work directly off of our `minimize_batch()` routine. We need to put our gradient calculations into a loop over `in_random_order(data)`, but since our objective function requires our model, `f`, _and_ a specific data data point, `(xi, yi)`, `yield`ed by our randomizer, we'll have to bring our calculation of `SSE_f` _inside_ loop! Additionally, there can be some questions of where we put our test for convergence against the `tolerance`. In the following code we'll put this test for convergence _outside_ of the data randomization loop, allowing us to compare the number of 'passes' over the dataset required to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_stochastic(f, x, y, c0, gradient_fn = False, tolerance = 0.00001):\n",
    "    \"\"\"perform gradient descent to find the input which minimizes the target function\"\"\"\n",
    "    \n",
    "    c = c0 # set theta to initial value\n",
    "    f = safe(f) # use the safe version of the target function\n",
    "    data = list(zip(x, y)) # align input and output for randomization    \n",
    "    SSE_f = SSE(f,x,y) # convert our data and model into an SSE objective function of c\n",
    "\n",
    "    print(\"Starting stochastic gradient descent at c0 = \", c0, \" with SSE = \", SSE_f(c))\n",
    "    iterations = 0\n",
    "    \n",
    "    while True:\n",
    "        ## compute gradients and adjust parameters according to each individual point\n",
    "        for xi, yi in in_random_order(data):\n",
    "            # convert our data and model into an SSE objective function of c at this random point\n",
    "            SSE_f = SSE(f,xi,yi) \n",
    "\n",
    "            ## create our gradient function for the SSE objective\n",
    "            grad_SSE = gradient_SSE(f, gradient_fn, np.array([xi]), np.array([yi])) if gradient_fn else False\n",
    "\n",
    "            ## compute the gradient at this point: c\n",
    "            gradient = grad_SSE(c) if grad_SSE else estimate_gradient(SSE_f, c)\n",
    "\n",
    "            ## determine the next candidate values, given several step sizes\n",
    "            next_cs = [step(c, gradient, step_size) for step_size in step_sizes]\n",
    "\n",
    "            # choose the one that minimizes our objective function\n",
    "            next_c = min(next_cs, key = SSE_f)\n",
    "            \n",
    "        iterations += 1\n",
    "        \n",
    "        # stop when convergence occurs---euclidean distance! \n",
    "        if np.linalg.norm(np.array(next_c) - np.array(c)) < tolerance:\n",
    "            # convert our data and model into an SSE objective function of c\n",
    "            SSE_f = SSE(f,x,y)\n",
    "            print(\"Completed optimizing stochastic gradient descent after \", iterations, \"iterations.\")\n",
    "            print(\"The final value was c = \", next_c, \" with SSE = \", SSE_f(next_c))\n",
    "            return next_c\n",
    "        c = next_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stochastic gradient descent at c0 =  [-9.66982163 -5.3649861 ]  with SSE =  18603736.813692015\n",
      "Completed optimizing stochastic gradient descent after  534 iterations.\n",
      "The final value was c =  [0.7380283961968613, 2.0113544721721914]  with SSE =  17.1061035081686\n"
     ]
    }
   ],
   "source": [
    "_ = minimize_stochastic(linear, x, y, np.random.uniform(low = -10, high = 10, size=2), gradient_fn = gradient_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stochastic gradient descent at c0 =  [ 5.82288219 -2.00337489]  with SSE =  5075546.623122519\n",
      "Completed optimizing stochastic gradient descent after  467 iterations.\n",
      "The final value was c =  [1.2295414053596165, 1.996848765367233]  with SSE =  0.9789573283900209\n"
     ]
    }
   ],
   "source": [
    "_ = minimize_stochastic(linear, x, y, np.random.uniform(low = -10, high = 10, size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.13 Interpreting our implementation\n",
    "So, what did we learn? Regardless of analytic gradients vs. approximations, this stochastic implementation appears to 1) converge sooner (about ten times!), and 2) to a better (lower $SSE$) solution. These are exciting properties for an optimization algorithm, but there are certainly obstacles that _any_ optimization algorithm will be subject to, including gradient descent. In particular, there might be multiple 'good' solutions that the algorithm could find, and while they might all be _local optima_, and gradient descent may not find the one that is the _best_. This can be a tough pill to swallow, but it's just the way things are. \n",
    "\n",
    "As it turns out for our `linear` model, gradient descent _will_ always find the best solution, but this is only because `linear` models have a special property called _convexity_ that guarentees this. If it exists, convexity can be quite hard to prove for a model, but it may also not be the most important thing to have this guarentee in an application&mdash;it may just need to work 'good enough'! However, even for convex optimization scenarios like above, optimal convergence may be missed as a result of parameterization. For gradient descent, the most touchy parameter is probably the  `step_size`. When this parameter is too large, the steps between the model coefficients might change too wildly to actually get to better solutions, i.e., the algorithm might just bounce around and never converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3.14 Exercise: more gradient descent implementations\n",
    "Gradient descent and the logistic function go hand-in-hand in many application contexts. Set up an implementation of gradient descent (as in __Sec. 5.1.3.8__) setting 'true' parameters and 'noisy data' for the logistic function (as requested in __Sec. 5.0.3.3__). Run it using both of the batch and and stochastic implementations, but utilize the gradient approximations. Which implementation works better, faster? Did the same set of step sizes work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
