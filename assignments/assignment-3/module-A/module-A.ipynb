{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 3: Probabilistic modeling and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module A _(55 pts)_ Optimizing for predicted separation\n",
    "__Data.__ For this module we'll be working with the basic baseball player heights and weights from the lecture notes. These are packaged in the following location:\n",
    "\n",
    "- `\"./data/baseball_heightweight.csv\"`\n",
    "\n",
    "__Overview.__ The purpose of this module is to provide experience with optimization and to view/investigate a different optimizable functions (from error). In the __Chapter 5.__ lecture notes we investigated the sum of squared errors as a function of (linear) model parameters. This made the goal of optimization (executed through gradient descent) into a 'fitting' procedure, i.e., describing a pattern that data follow.\n",
    "\n",
    "Another way we can use a line is as a 'separator': supposing some data $x_1, \\cdots, x_n$ have labels $y_1, \\cdots, y_n$ falling into two classes (coded as $\\pm 1$), which set of line-defining $w$ and $b$ parameters separate the points the best. Check out the picture  below for an intution (note: their $b$ is our $-b$).\n",
    "\n",
    "Note: even though we're talking about 'lines', and actually working with true, 2-dimensional lines throughout this module, it's important to think/code throughout this module with the expectation that each data point $x_i$ will have $m$ (potentially greater than 2) dimensions. When $m>3$, a 'line' technically refers to a plane, but after we leave the comfort of 3-dimensionality our 'lines' are technically _hyperplanes_, for which we have little experiential intuition. Hence, we'll adopt the terminology 'line' throughout, since this provides the strongest grounding of intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM Margin](./images/SVM_margin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A1.__ _(7 pts_) To get things started: \n",
    "\n",
    "1. load the baseball player data with pandas into a dataframe called `bball`;\n",
    "2. filter `bball` to contain only those rows that correspond to shortstops and catchers;\n",
    "3. create an `.array()` called `y_bball` of labels, containing a `1` for each catcher and a `-1` for each shortstop;\n",
    "4. filter the remaining rows of `bball` to only the `Height` and `Weight` columns; and\n",
    "5. standardize the columns of `bball` and exhibit the first 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A2.__ _(5 pts)_ Since our goal with this problem will be to draw a line that separates the catchers from the shortstops, write and execute a function called \n",
    "\n",
    "- `plot_data_boundary(data, y)` \n",
    "\n",
    "that creates a scatter plot of the standardized heights and weights where the points are color coded with shortstops as black circles and catchers as red squares. In the response box below, disucss how well you think a line can separate these data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A3.__ _(5 pts)_ The way we'll be drawing lines (hyperplanes) in this problem uses what's called the 'standard form'. Supposing we have vectors (rows) of features (columns) $x = [x_1, \\cdots, x_m]$, a linear relationship is defined by a vector of weights $w = [w_1,\\cdots,w_m]$ and an intercept $b$ in the form:\n",
    "$$\n",
    "w\\cdot x + b = 0\n",
    "$$\n",
    "Since our data only has two columns, we can re-arrange into slope-intercept form:\n",
    "$$\n",
    "x_2 = -\\frac{w_1}{w_2} x_1 - \\frac{b}{w_2}\n",
    "$$\n",
    "to conveniently plot a line in standard form. \n",
    "\n",
    "So, modify your `plot_data_boundary()` function to accept a `w` (a vector) and `b` (a scalar) arguments and plot the corresponding line over the range of $x_1$ values using the slope-intercept form above. \n",
    "\n",
    "When complete, select you own $w$ and $b$ parameters and use your function to exhibit a corresponding line over the problem's data. Then, disucuss how well the line separates the two (catcher and shortstop) classes in the response box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A4.__ _(3 pts)_ Now, for any point $x$ on the line defined by $w$ and $b$, the following relationship holds:\n",
    "$$\n",
    "w\\cdot x + b = 0\n",
    "$$\n",
    "\n",
    "But our points probably don't lie exactly on a line. To tell which sides of the line they fall on, we can just look at the sign the left hand side of the equation above:\n",
    "\n",
    "- $x$ is above the line:\n",
    "    - $\\hat{y} = w\\cdot x_i + b > 0$ \n",
    "- $x$ is below the line:\n",
    "    - $\\hat{y} = w\\cdot x_i + b < 0$\n",
    "\n",
    "What we're hoping to do is have all $x$'s either have $y,\\hat{y} < 0$ or $y,\\hat{y} > 0$. One way to test if a point is 'correctly' separated is if $y\\hat{y} > 0$, so use this test ($y\\hat{y} > 0$ means 'separated') to write a function called \n",
    "- `separated(data, y, w, b)`\n",
    "\n",
    "that outputs a boolean mask of $n$ values where `True` corresponds to a point falling on the correct side of the line ($y\\hat{y} > 0$). When this is done, use your functon to calculate/report the percent of points your line correctly separated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A5.__ _(3 pts)_ Now that we have our `separated()` function, use it to modify your `plot_data_boundary()` function into a new one called:\n",
    "- plot_separated(data, y, w, b)\n",
    "\n",
    "which plots the same points and line as before, but now color codes them for _correctness of separation_ as well label. In particular, organize your points as follows:\n",
    "- indicate correctly separated negative points (shortstops) with black circles, \n",
    "- indicate incorrectly separated negative points (shortstops) with green triangles, \n",
    "- indicate correctly separated negative points (shortstops) with red squares, and\n",
    "- indicate incorrectly separated negative points (shortstops) with yellow diamonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ _(5 pts)_ Now, most of all we'll want to reduce the number of points $x$ for which our prediction $\\hat{y}$ has a different sign from the true label $y$. In these (bad) cases, one will have $y\\hat{y} < 0$.\n",
    "\n",
    "However, 'edge cases' are bad, too, and since we've provided our points with unit ($y = \\pm 1$) labels it's also important to pay attention to points whose prediction have correct sign ($y\\hat{y} > 0$), but are close to the boundary (here, within one unit):\n",
    "\n",
    "$$\n",
    "y(w\\cdot x + b) = y\\hat{y} < 1\n",
    "$$\n",
    "\n",
    "Thus, our objective function will seek to _maximize_ the distance between our $(w,b)$ separating line (plane) and 'bad' points holding the relationship $y(w\\cdot x + b) = y\\hat{y} < 1$. So, for a given plane $(w,b)$ and $n$ `data` points $x_i: i=1,\\cdots n$, we'll be working with a subset of $k$ 'bad' points: $x_{i_\\ell}:\\ell=1,\\cdots k$. To identify these, write a function:\n",
    "\n",
    "- `bad_points(data, y, w, b)`\n",
    "\n",
    "that takes `data` of $n$ rows and $m$ columns/features, line-defining parameters $w$ (a vector of $m$) and $b$ (a scalar), and outputs a boolean mask positively identifying the 'bad points' using a boolean mask, i.e., those for which:\n",
    "\n",
    "$$\n",
    "y(w\\cdot x + b) = y\\hat{y} < 1\n",
    "$$\n",
    "\n",
    "Note: this function is very similar to the requested `separated()` in __A4__.\n",
    "\n",
    "When this is complete, exhibit this function's output using the line you selected in __A3__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A6.__ _(4 pts)_ \"How bad\" a point is, is defined by distance&mdash;between our line (hyperplane) and the point. For a given point $x$ and line defined by $w$ and $b$, the distance is:\n",
    "$$\n",
    "d(x;w, b) = \\frac{|w\\cdot x + b|}{\\|w\\|}\n",
    "$$\n",
    "But since being on the wrong side (having the wrong sign) is extra bad, it's actually the signed distances we care about:\n",
    "$$\n",
    "d(x;w, b)_\\text{sgn} =\\frac{y(w\\cdot x + b)}{\\|w\\|}\n",
    "$$\n",
    "This makes the sum of signed distances for the 'bad points' we've found into our objective function, which we'll aim to maximize:\n",
    "$$\n",
    "\\begin{align*}\n",
    "   D (X; w, b) \n",
    "   = &\\sum_{\\ell=1}^k d_\\text{sgn}(x_{i_\\ell};w, b) \\\\\n",
    "   = &\\sum_{\\ell=1}^k \\frac{y_{i_\\ell}(w\\cdot x_{i_\\ell}+ b)}{\\|w\\|}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since this is our objective function, express it's gradient components with respect to the parameters $b$ and $w_j$ (for any $j$ of the $m$ features) in the response box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response_.\n",
    "$$\n",
    "\\nabla_b(D) = ?\n",
    "$$\n",
    "$$\n",
    "\\nabla_{w_j}(D) = ?\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A7.__ _(5 pts)_ Now that we have a formula for our gradient, it's time to write a function that computes it for an arbitrary set of points and plane. Use your above formulae to write a function:\n",
    "\n",
    "- `gradient(data, y, w , b)`\n",
    "\n",
    "that takes $w$, $b$, and a `data` matrix of $x_i$ rows, and computes the gradient for the $m + 1$ parameters.\n",
    "\n",
    "When complete, exhibit this function's output using the line you exhibited in __A3__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A8.__ _(10 pts)_ Finally, it's time to pull things together into a gradient descent application. Specifically, create a function called \n",
    "- `GD(data, y, w, b, eta=0.001, iter_num=1000, threshold=0.001)`\n",
    "that accepts the following arguments:\n",
    "\n",
    "- `data`: a matrix of $n$ row/data points by $m$ feature/columns\n",
    "- `y`:  an array of $n$ $\\pm 1$ valued 'labels'\n",
    "- `w`, `b`: an array of initial weights and an intercept (initial parameters) \n",
    "- `eta=0.001`: the 'learning rate' i.e., 'sensitivity' knob\n",
    "- `iter_num=1000`: the max number of gradient steps to take before termnating\n",
    "- `threshold=0.001`: a fixed, minimum distance that when steps fall below terminates the algorithm\n",
    "\n",
    "To correctly implement, your code must do the following:\n",
    "1. loop over `iter_num` steps\n",
    "2. compute $w$ and $b$ gradient components\n",
    "3. determinie $w$ and $b$ steps sizes\n",
    "4. update $w$ and $b$ by _adding_ gradient components (this is technically ascent)\n",
    "5. terminate if `iter_num` steps are complete or if the most recent step is smaller than `threshold`\n",
    "6. `return` a list of the history of $w$ and $b$ values\n",
    "\n",
    "When this is complete, apply your code to the standardized `bball` data and labels starting with your selected line from __A3__, and print the final parameters and whether or not your code converged, i.e., the threshold was met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A9.__ _(3 pts)_ Now, exhibit the final (best line) from your application in __A7__ using your `plot_separated()` function, report it's percent separated, and discuss if you think the line forms a better separation than the original line in the comment box below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A10.__ (5 pts) As a last step, modify `plot_data_boundary()` again, but now into a new function called `plot_convergence()` that accepts the full history of `ws` and `bs` that were output by you `GD` function in __A7.__ and exhibits all lines corresponding to the steps along the way to convergence. Use the output of this code to discuss in the comment box below if you believe your code is converging towards a good-separating solution.\n",
    "\n",
    "\\[Hint. Use changing alpha or a colormap to help illustrate thr transition between values of $w$ and $b$\\]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Response._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
